{"/home/travis/build/npmtest/node-npmtest-salient/test.js":"/* istanbul instrument in package npmtest_salient */\n/*jslint\n    bitwise: true,\n    browser: true,\n    maxerr: 8,\n    maxlen: 96,\n    node: true,\n    nomen: true,\n    regexp: true,\n    stupid: true\n*/\n(function () {\n    'use strict';\n    var local;\n\n\n\n    // run shared js-env code - init-before\n    (function () {\n        // init local\n        local = {};\n        // init modeJs\n        local.modeJs = (function () {\n            try {\n                return typeof navigator.userAgent === 'string' &&\n                    typeof document.querySelector('body') === 'object' &&\n                    typeof XMLHttpRequest.prototype.open === 'function' &&\n                    'browser';\n            } catch (errorCaughtBrowser) {\n                return module.exports &&\n                    typeof process.versions.node === 'string' &&\n                    typeof require('http').createServer === 'function' &&\n                    'node';\n            }\n        }());\n        // init global\n        local.global = local.modeJs === 'browser'\n            ? window\n            : global;\n        switch (local.modeJs) {\n        // re-init local from window.local\n        case 'browser':\n            local = local.global.utility2.objectSetDefault(\n                local.global.utility2_rollup || local.global.local,\n                local.global.utility2\n            );\n            break;\n        // re-init local from example.js\n        case 'node':\n            local = (local.global.utility2_rollup || require('utility2'))\n                .requireReadme();\n            break;\n        }\n        // export local\n        local.global.local = local;\n    }());\n\n\n\n    // run shared js-env code - function\n    (function () {\n        return;\n    }());\n    switch (local.modeJs) {\n\n\n\n    // run browser js-env code - function\n    case 'browser':\n        break;\n\n\n\n    // run node js-env code - function\n    case 'node':\n        break;\n    }\n\n\n\n    // run shared js-env code - init-after\n    (function () {\n        return;\n    }());\n    switch (local.modeJs) {\n\n\n\n    // run browser js-env code - init-after\n    case 'browser':\n        local.testCase_browser_nullCase = local.testCase_browser_nullCase || function (\n            options,\n            onError\n        ) {\n        /*\n         * this function will test browsers's null-case handling-behavior-behavior\n         */\n            onError(null, options);\n        };\n\n        // run tests\n        local.nop(local.modeTest &&\n            document.querySelector('#testRunButton1') &&\n            document.querySelector('#testRunButton1').click());\n        break;\n\n\n\n    // run node js-env code - init-after\n    /* istanbul ignore next */\n    case 'node':\n        local.testCase_buildApidoc_default = local.testCase_buildApidoc_default || function (\n            options,\n            onError\n        ) {\n        /*\n         * this function will test buildApidoc's default handling-behavior-behavior\n         */\n            options = { modulePathList: module.paths };\n            local.buildApidoc(options, onError);\n        };\n\n        local.testCase_buildApp_default = local.testCase_buildApp_default || function (\n            options,\n            onError\n        ) {\n        /*\n         * this function will test buildApp's default handling-behavior-behavior\n         */\n            local.testCase_buildReadme_default(options, local.onErrorThrow);\n            local.testCase_buildLib_default(options, local.onErrorThrow);\n            local.testCase_buildTest_default(options, local.onErrorThrow);\n            local.testCase_buildCustomOrg_default(options, local.onErrorThrow);\n            options = [];\n            local.buildApp(options, onError);\n        };\n\n        local.testCase_buildCustomOrg_default = local.testCase_buildCustomOrg_default ||\n            function (options, onError) {\n            /*\n             * this function will test buildCustomOrg's default handling-behavior\n             */\n                options = {};\n                local.buildCustomOrg(options, onError);\n            };\n\n        local.testCase_buildLib_default = local.testCase_buildLib_default || function (\n            options,\n            onError\n        ) {\n        /*\n         * this function will test buildLib's default handling-behavior\n         */\n            options = {};\n            local.buildLib(options, onError);\n        };\n\n        local.testCase_buildReadme_default = local.testCase_buildReadme_default || function (\n            options,\n            onError\n        ) {\n        /*\n         * this function will test buildReadme's default handling-behavior-behavior\n         */\n            options = {};\n            local.buildReadme(options, onError);\n        };\n\n        local.testCase_buildTest_default = local.testCase_buildTest_default || function (\n            options,\n            onError\n        ) {\n        /*\n         * this function will test buildTest's default handling-behavior\n         */\n            options = {};\n            local.buildTest(options, onError);\n        };\n\n        local.testCase_webpage_default = local.testCase_webpage_default || function (\n            options,\n            onError\n        ) {\n        /*\n         * this function will test webpage's default handling-behavior\n         */\n            options = { modeCoverageMerge: true, url: local.serverLocalHost + '?modeTest=1' };\n            local.browserTest(options, onError);\n        };\n\n        // run test-server\n        local.testRunServer(local);\n        break;\n    }\n}());\n","/home/travis/build/npmtest/node-npmtest-salient/lib.npmtest_salient.js":"/* istanbul instrument in package npmtest_salient */\n/*jslint\n    bitwise: true,\n    browser: true,\n    maxerr: 8,\n    maxlen: 96,\n    node: true,\n    nomen: true,\n    regexp: true,\n    stupid: true\n*/\n(function () {\n    'use strict';\n    var local;\n\n\n\n    // run shared js-env code - init-before\n    (function () {\n        // init local\n        local = {};\n        // init modeJs\n        local.modeJs = (function () {\n            try {\n                return typeof navigator.userAgent === 'string' &&\n                    typeof document.querySelector('body') === 'object' &&\n                    typeof XMLHttpRequest.prototype.open === 'function' &&\n                    'browser';\n            } catch (errorCaughtBrowser) {\n                return module.exports &&\n                    typeof process.versions.node === 'string' &&\n                    typeof require('http').createServer === 'function' &&\n                    'node';\n            }\n        }());\n        // init global\n        local.global = local.modeJs === 'browser'\n            ? window\n            : global;\n        // init utility2_rollup\n        local = local.global.utility2_rollup || local;\n        // init lib\n        local.local = local.npmtest_salient = local;\n        // init exports\n        if (local.modeJs === 'browser') {\n            local.global.utility2_npmtest_salient = local;\n        } else {\n            module.exports = local;\n            module.exports.__dirname = __dirname;\n            module.exports.module = module;\n        }\n    }());\n}());\n","/home/travis/build/npmtest/node-npmtest-salient/example.js":"/*\nexample.js\n\nquickstart example\n\ninstruction\n    1. save this script as example.js\n    2. run the shell command:\n        $ npm install npmtest-salient && PORT=8081 node example.js\n    3. play with the browser-demo on http://127.0.0.1:8081\n*/\n\n\n\n/* istanbul instrument in package npmtest_salient */\n/*jslint\n    bitwise: true,\n    browser: true,\n    maxerr: 8,\n    maxlen: 96,\n    node: true,\n    nomen: true,\n    regexp: true,\n    stupid: true\n*/\n(function () {\n    'use strict';\n    var local;\n\n\n\n    // run shared js-env code - init-before\n    (function () {\n        // init local\n        local = {};\n        // init modeJs\n        local.modeJs = (function () {\n            try {\n                return typeof navigator.userAgent === 'string' &&\n                    typeof document.querySelector('body') === 'object' &&\n                    typeof XMLHttpRequest.prototype.open === 'function' &&\n                    'browser';\n            } catch (errorCaughtBrowser) {\n                return module.exports &&\n                    typeof process.versions.node === 'string' &&\n                    typeof require('http').createServer === 'function' &&\n                    'node';\n            }\n        }());\n        // init global\n        local.global = local.modeJs === 'browser'\n            ? window\n            : global;\n        // init utility2_rollup\n        local = local.global.utility2_rollup || (local.modeJs === 'browser'\n            ? local.global.utility2_npmtest_salient\n            : global.utility2_moduleExports);\n        // export local\n        local.global.local = local;\n    }());\n    switch (local.modeJs) {\n\n\n\n    // init-after\n    // run browser js-env code - init-after\n    /* istanbul ignore next */\n    case 'browser':\n        local.testRunBrowser = function (event) {\n            if (!event || (event &&\n                    event.currentTarget &&\n                    event.currentTarget.className &&\n                    event.currentTarget.className.includes &&\n                    event.currentTarget.className.includes('onreset'))) {\n                // reset output\n                Array.from(\n                    document.querySelectorAll('body > .resettable')\n                ).forEach(function (element) {\n                    switch (element.tagName) {\n                    case 'INPUT':\n                    case 'TEXTAREA':\n                        element.value = '';\n                        break;\n                    default:\n                        element.textContent = '';\n                    }\n                });\n            }\n            switch (event && event.currentTarget && event.currentTarget.id) {\n            case 'testRunButton1':\n                // show tests\n                if (document.querySelector('#testReportDiv1').style.display === 'none') {\n                    document.querySelector('#testReportDiv1').style.display = 'block';\n                    document.querySelector('#testRunButton1').textContent =\n                        'hide internal test';\n                    local.modeTest = true;\n                    local.testRunDefault(local);\n                // hide tests\n                } else {\n                    document.querySelector('#testReportDiv1').style.display = 'none';\n                    document.querySelector('#testRunButton1').textContent = 'run internal test';\n                }\n                break;\n            // custom-case\n            default:\n                break;\n            }\n            if (document.querySelector('#inputTextareaEval1') && (!event || (event &&\n                    event.currentTarget &&\n                    event.currentTarget.className &&\n                    event.currentTarget.className.includes &&\n                    event.currentTarget.className.includes('oneval')))) {\n                // try to eval input-code\n                try {\n                    /*jslint evil: true*/\n                    eval(document.querySelector('#inputTextareaEval1').value);\n                } catch (errorCaught) {\n                    console.error(errorCaught);\n                }\n            }\n        };\n        // log stderr and stdout to #outputTextareaStdout1\n        ['error', 'log'].forEach(function (key) {\n            console[key + '_original'] = console[key];\n            console[key] = function () {\n                var element;\n                console[key + '_original'].apply(console, arguments);\n                element = document.querySelector('#outputTextareaStdout1');\n                if (!element) {\n                    return;\n                }\n                // append text to #outputTextareaStdout1\n                element.value += Array.from(arguments).map(function (arg) {\n                    return typeof arg === 'string'\n                        ? arg\n                        : JSON.stringify(arg, null, 4);\n                }).join(' ') + '\\n';\n                // scroll textarea to bottom\n                element.scrollTop = element.scrollHeight;\n            };\n        });\n        // init event-handling\n        ['change', 'click', 'keyup'].forEach(function (event) {\n            Array.from(document.querySelectorAll('.on' + event)).forEach(function (element) {\n                element.addEventListener(event, local.testRunBrowser);\n            });\n        });\n        // run tests\n        local.testRunBrowser();\n        break;\n\n\n\n    // run node js-env code - init-after\n    /* istanbul ignore next */\n    case 'node':\n        // export local\n        module.exports = local;\n        // require modules\n        local.fs = require('fs');\n        local.http = require('http');\n        local.url = require('url');\n        // init assets\n        local.assetsDict = local.assetsDict || {};\n        /* jslint-ignore-begin */\n        local.assetsDict['/assets.index.template.html'] = '\\\n<!doctype html>\\n\\\n<html lang=\"en\">\\n\\\n<head>\\n\\\n<meta charset=\"UTF-8\">\\n\\\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\\n\\\n<title>{{env.npm_package_name}} (v{{env.npm_package_version}})</title>\\n\\\n<style>\\n\\\n/*csslint\\n\\\n    box-sizing: false,\\n\\\n    universal-selector: false\\n\\\n*/\\n\\\n* {\\n\\\n    box-sizing: border-box;\\n\\\n}\\n\\\nbody {\\n\\\n    background: #dde;\\n\\\n    font-family: Arial, Helvetica, sans-serif;\\n\\\n    margin: 2rem;\\n\\\n}\\n\\\nbody > * {\\n\\\n    margin-bottom: 1rem;\\n\\\n}\\n\\\n.utility2FooterDiv {\\n\\\n    margin-top: 20px;\\n\\\n    text-align: center;\\n\\\n}\\n\\\n</style>\\n\\\n<style>\\n\\\n/*csslint\\n\\\n*/\\n\\\ntextarea {\\n\\\n    font-family: monospace;\\n\\\n    height: 10rem;\\n\\\n    width: 100%;\\n\\\n}\\n\\\ntextarea[readonly] {\\n\\\n    background: #ddd;\\n\\\n}\\n\\\n</style>\\n\\\n</head>\\n\\\n<body>\\n\\\n<!-- utility2-comment\\n\\\n<div id=\"ajaxProgressDiv1\" style=\"background: #d00; height: 2px; left: 0; margin: 0; padding: 0; position: fixed; top: 0; transition: background 0.5s, width 1.5s; width: 25%;\"></div>\\n\\\nutility2-comment -->\\n\\\n<h1>\\n\\\n<!-- utility2-comment\\n\\\n    <a\\n\\\n        {{#if env.npm_package_homepage}}\\n\\\n        href=\"{{env.npm_package_homepage}}\"\\n\\\n        {{/if env.npm_package_homepage}}\\n\\\n        target=\"_blank\"\\n\\\n    >\\n\\\nutility2-comment -->\\n\\\n        {{env.npm_package_name}} (v{{env.npm_package_version}})\\n\\\n<!-- utility2-comment\\n\\\n    </a>\\n\\\nutility2-comment -->\\n\\\n</h1>\\n\\\n<h3>{{env.npm_package_description}}</h3>\\n\\\n<!-- utility2-comment\\n\\\n<h4><a download href=\"assets.app.js\">download standalone app</a></h4>\\n\\\n<button class=\"onclick onreset\" id=\"testRunButton1\">run internal test</button><br>\\n\\\n<div id=\"testReportDiv1\" style=\"display: none;\"></div>\\n\\\nutility2-comment -->\\n\\\n\\n\\\n\\n\\\n\\n\\\n<label>stderr and stdout</label>\\n\\\n<textarea class=\"resettable\" id=\"outputTextareaStdout1\" readonly></textarea>\\n\\\n<!-- utility2-comment\\n\\\n{{#if isRollup}}\\n\\\n<script src=\"assets.app.js\"></script>\\n\\\n{{#unless isRollup}}\\n\\\nutility2-comment -->\\n\\\n<script src=\"assets.utility2.rollup.js\"></script>\\n\\\n<script src=\"jsonp.utility2._stateInit?callback=window.utility2._stateInit\"></script>\\n\\\n<script src=\"assets.npmtest_salient.rollup.js\"></script>\\n\\\n<script src=\"assets.example.js\"></script>\\n\\\n<script src=\"assets.test.js\"></script>\\n\\\n<!-- utility2-comment\\n\\\n{{/if isRollup}}\\n\\\nutility2-comment -->\\n\\\n<div class=\"utility2FooterDiv\">\\n\\\n    [ this app was created with\\n\\\n    <a href=\"https://github.com/kaizhu256/node-utility2\" target=\"_blank\">utility2</a>\\n\\\n    ]\\n\\\n</div>\\n\\\n</body>\\n\\\n</html>\\n\\\n';\n        /* jslint-ignore-end */\n        if (local.templateRender) {\n            local.assetsDict['/'] = local.templateRender(\n                local.assetsDict['/assets.index.template.html'],\n                {\n                    env: local.objectSetDefault(local.env, {\n                        npm_package_description: 'the greatest app in the world!',\n                        npm_package_name: 'my-app',\n                        npm_package_nameAlias: 'my_app',\n                        npm_package_version: '0.0.1'\n                    })\n                }\n            );\n        } else {\n            local.assetsDict['/'] = local.assetsDict['/assets.index.template.html']\n                .replace((/\\{\\{env\\.(\\w+?)\\}\\}/g), function (match0, match1) {\n                    // jslint-hack\n                    String(match0);\n                    switch (match1) {\n                    case 'npm_package_description':\n                        return 'the greatest app in the world!';\n                    case 'npm_package_name':\n                        return 'my-app';\n                    case 'npm_package_nameAlias':\n                        return 'my_app';\n                    case 'npm_package_version':\n                        return '0.0.1';\n                    }\n                });\n        }\n        // run the cli\n        if (local.global.utility2_rollup || module !== require.main) {\n            break;\n        }\n        local.assetsDict['/assets.example.js'] =\n            local.assetsDict['/assets.example.js'] ||\n            local.fs.readFileSync(__filename, 'utf8');\n        // bug-workaround - long $npm_package_buildCustomOrg\n        /* jslint-ignore-begin */\n        local.assetsDict['/assets.npmtest_salient.rollup.js'] =\n            local.assetsDict['/assets.npmtest_salient.rollup.js'] ||\n            local.fs.readFileSync(\n                local.npmtest_salient.__dirname + '/lib.npmtest_salient.js',\n                'utf8'\n            ).replace((/^#!/), '//');\n        /* jslint-ignore-end */\n        local.assetsDict['/favicon.ico'] = local.assetsDict['/favicon.ico'] || '';\n        // if $npm_config_timeout_exit exists,\n        // then exit this process after $npm_config_timeout_exit ms\n        if (Number(process.env.npm_config_timeout_exit)) {\n            setTimeout(process.exit, Number(process.env.npm_config_timeout_exit));\n        }\n        // start server\n        if (local.global.utility2_serverHttp1) {\n            break;\n        }\n        process.env.PORT = process.env.PORT || '8081';\n        console.error('server starting on port ' + process.env.PORT);\n        local.http.createServer(function (request, response) {\n            request.urlParsed = local.url.parse(request.url);\n            if (local.assetsDict[request.urlParsed.pathname] !== undefined) {\n                response.end(local.assetsDict[request.urlParsed.pathname]);\n                return;\n            }\n            response.statusCode = 404;\n            response.end();\n        }).listen(process.env.PORT);\n        break;\n    }\n}());\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/index.js":"\nmodule.exports = require('./lib/salient');\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/index.js":"\n/**\n * Module imports for tokenizers\n */\n\nmodule.exports.tokenizers = {};\nmodule.exports.tokenizers.ArticleTokenizer = require('./tokenizers/article_tokenizer');\nmodule.exports.tokenizers.TweetTokenizer = require('./tokenizers/tweet_tokenizer');\nmodule.exports.tokenizers.UrlTokenizer = require('./tokenizers/url_tokenizer');\nmodule.exports.tokenizers.EmoticonTokenizer = require('./tokenizers/emoticon_tokenizer');\nmodule.exports.tokenizers.RegExpTokenizer = require('./tokenizers/regexp_tokenizer');\nmodule.exports.tokenizers.WordPunctTokenizer = require('./tokenizers/wordpunct_tokenizer');\nmodule.exports.tokenizers.Tokenizer = require('./tokenizers/tokenizer');\nmodule.exports.classifiers = {};\nmodule.exports.classifiers.BayesClassifier = require('./classifiers/bayes_classifier');\nmodule.exports.classifiers.LogisticRegression = require('./classifiers/logistic_regression');\nmodule.exports.regression = {};\nmodule.exports.regression.LinearRegression = require('./regression/linear_regression');\nmodule.exports.math = {};\nmodule.exports.math.Matrix = require('./math/matrix');\nmodule.exports.math.Vector = require('./math/vector');\nmodule.exports.normalizers = {};\nmodule.exports.normalizers.MeanNormalizer = require('./normalizers/mean_normalizer');\nmodule.exports.neuralnetworks = {};\nmodule.exports.neuralnetworks.NeuralNetwork = require('./neuralnetworks/neuralnetwork');\nmodule.exports.crossvalidation = {};\nmodule.exports.crossvalidation.CrossValidate = require('./crossvalidation/crossvalidate');\nmodule.exports.wiktionary = {};\nmodule.exports.wiktionary.WiktionaryParser = require('./wiktionary/wikparser');\nmodule.exports.corpus = {};\nmodule.exports.corpus.BrownCorpus = require('./corpus/brown');\nmodule.exports.corpus.PennTreeBank = require('./corpus/penn');\nmodule.exports.corpus.TwitterTreeBank = require('./corpus/twitter');\nmodule.exports.corpus.IULATreeBank = require('./corpus/iula');\nmodule.exports.language = {};\nmodule.exports.language.HiddenMarkovModel = require('./language/hmm');\nmodule.exports.tagging = {}\nmodule.exports.tagging.HmmTagger = require('./tagging/hmm_tagger');\nmodule.exports.tagging.TreeTagger = require('./tagging/treetagger');\nmodule.exports.glossary = {};\nmodule.exports.glossary.Glossary = require('./glossary/glossary');\nmodule.exports.sentiment = {};\nmodule.exports.sentiment.BayesSentimentAnalyser = require('./sentiment/bayesanalyser');\nmodule.exports.sentiment.SentiwordnetAnalyser = require('./sentiment/sentiwordnet_analyser');\nmodule.exports.crawlers = {};\nmodule.exports.crawlers.ContentCrawler = require('./crawlers/content_crawler');\nmodule.exports.crawlers.SitemapCrawler = require('./crawlers/sitemap_crawler');\nmodule.exports.graph = {};\nmodule.exports.graph.DocumentGraph = require('./graph/document');\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/tokenizers/article_tokenizer.js":"\nvar Tokenizer = require('./tokenizer');\nvar RegExpTokenizer = require('./regexp_tokenizer');\nvar WordPunctTokenizer = require('./wordpunct_tokenizer');\nvar UrlTokenizer = require('./url_tokenizer');\nvar he = require('he');\nvar util = require('util');\nvar _ = require('underscore')._;\n\nvar ArticleTokenizer = function (options) {\n    var options = options || {};\n    this.cleanWikiMarkup = options.cleanWikiMarkup;\n    this.cleanXHTML = options.cleanHTML;\n    this.decodeURLEncoded = options.decodeURLEncoded;\n    this.cleanNonAlphaNumeric = options.cleanNonAlphaNumeric || false;\n    this.compressWhitespace = options.compressWhitespace || false;\n    this.preserveEmoticons = options.preserveEmoticons || this.preserveEmoticons;\n    if (typeof this.cleanWikiMarkup === 'undefined') {\n        this.cleanWikiMarkup = true;\n    }\n    if (typeof this.cleanXHTML === 'undefined') {\n        this.cleanXHTML = true;\n    }\n    if (typeof this.decodeURLEncoded === 'undefined') {\n        this.decodeURLEncoded = true;\n    }\n    if (typeof this.cleanNonAlphaNumeric === 'undefined') {\n        this.cleanNonAlphaNumeric = true;\n    }\n};\n\n// Inherit from the base tokenizer\nutil.inherits(ArticleTokenizer, Tokenizer);\n\nArticleTokenizer.prototype._initializeTokenizers = function () {\n    this.tokenizer = new WordPunctTokenizer({ includeIndices: true, preserveEmoticons : this.preserveEmoticons });\n    this.urlTokenizer = new UrlTokenizer({ includeIndices: true });\n    this.cleanTokenizers = [];\n    if (this.decodeURLEncoded) {\n        // decode URL encoded characters\n        this.cleanTokenizers.push(new RegExpTokenizer({\n            cleanPattern: /&amp;/ig,\n            replaceWith: '&'\n        }));\n        this.cleanTokenizers.push(new RegExpTokenizer({\n            cleanPattern: /&lt;/ig,\n            replaceWith: '<'\n        }));\n        this.cleanTokenizers.push(new RegExpTokenizer({\n            cleanPattern: /&gt;/ig,\n            replaceWith: '>'\n        }));\n    }\n    if (this.cleanWikiMarkup) {\n        this.cleanTokenizers.push(new RegExpTokenizer({\n            cleanPattern: /<ref[^<]*<\\/ref>/ig,\n            cleanTriggerChar: '<'\n        }));\n    }\n    if (this.cleanXHTML) {\n        // removes xhtml tags\n        this.cleanTokenizers.push(new RegExpTokenizer({\n            cleanPattern: /(<([^>]+)>)/ig,\n            cleanTriggerChar: '<'\n        }));\n    }\n    if (this.cleanWikiMarkup) {\n        this.cleanTokenizers.push(new RegExpTokenizer({\n            cleanPattern: /\\[http:[^] ]*/ig,\n            replaceWith: '['\n        }));\n        this.cleanTokenizers.push(new RegExpTokenizer({\n            cleanPattern: /\\|thumb/ig\n        }));\n        this.cleanTokenizers.push(new RegExpTokenizer({\n            cleanPattern: /\\|left/ig\n        }));\n        this.cleanTokenizers.push(new RegExpTokenizer({\n            cleanPattern: /\\|right/ig\n        }));\n        this.cleanTokenizers.push(new RegExpTokenizer({\n            cleanPattern: /\\|\\d+px/ig\n        }));\n        this.cleanTokenizers.push(new RegExpTokenizer({\n            cleanPattern: /\\[\\[image:[^\\[\\]]*\\|/ig\n        }));\n        this.cleanTokenizers.push(new RegExpTokenizer({\n            cleanPattern: /\\[\\[category:([^|\\]]*)[^]]*\\]\\]/ig,\n        }));\n        this.cleanTokenizers.push(new RegExpTokenizer({\n            cleanPattern: /\\[\\[[a-z\\-]*:[^\\]]*\\]\\]/ig\n        }));\n        this.cleanTokenizers.push(new RegExpTokenizer({\n            cleanPattern: /\\[\\[[^\\|\\]]*\\|/ig,\n            replaceWith: '[['\n        }));\n        this.cleanTokenizers.push(new RegExpTokenizer({\n            cleanPattern: /{{[^}]*}}/ig\n        }));\n        this.cleanTokenizers.push(new RegExpTokenizer({\n            cleanPattern: /{[^}]*}/ig\n        }));\n        this.cleanTokenizers.push(new RegExpTokenizer({\n            cleanPattern: /\\[/ig\n        }));\n        this.cleanTokenizers.push(new RegExpTokenizer({\n            cleanPattern: /\\]/ig\n        }));\n        this.cleanTokenizers.push(new RegExpTokenizer({\n            cleanPattern: /&[^\\s]*;/ig,\n            replaceWith: ''\n        }));\n    }\n    if (this.cleanNonAlphaNumeric) {\n        this.cleanTokenizers.push(new RegExpTokenizer({\n            cleanPattern: /[^a-zA-Z0-9]/ig,\n            replaceWith: ' '\n        }));\n    }\n};\n\n/**\n * Tokenizes the given input\n */\nArticleTokenizer.prototype.tokenize = function (s) {\n    if (!s || s.length == 0)\n        return [];\n\n    if (!this.urlTokenizer || !this.tokenizer) {\n        this._initializeTokenizers();\n    }\n\n    var urls = this.urlTokenizer.tokenize(s);\n    var tokens = [];\n    var index = 0;\n    if (urls.length > 0) {\n        for (var i = 0; i < urls.length; i++) {\n            var urlToken = urls[i];\n            var indices = urlToken.indices;\n            if (index < indices[0] && index < s.length) {\n                var t = s.substring(index, indices[0]);\n                var results = this.tokenizer.tokenize(t);\n                tokens = tokens.concat(results);\n            }\n            tokens.push(urlToken.url);\n            index = indices[1];\n        }\n        if (index < s.length) {\n            var t = s.substring(index);\n            var results = this.tokenizer.tokenize(t);\n            tokens = tokens.concat(results);\n        }\n        return tokens;\n    }\n    else {\n        return this.tokenizer.tokenize(s);\n    }\n};\n\n/*\n * Cleans the given input\n */\nArticleTokenizer.prototype.clean = function (s) {\n    if (!this.cleanTokenizers) {\n        this._initializeTokenizers();\n    }\n    s = he.decode(s);\n    for (var i = 0; i < this.cleanTokenizers.length; i++) {\n        var cleanTokenizer = this.cleanTokenizers[i];\n        s = cleanTokenizer.clean(s);\n    }\n    if (this.compressWhitespace) {\n        s = this.chop(s);\n    }\n    return s;\n};\n\nmodule.exports = ArticleTokenizer;\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/tokenizers/tokenizer.js":"\nvar Tokenizer = function() {\n};\n\n/**\n * Default trim operation for an array of tokens.\n */\nTokenizer.prototype.trim = function (array) {\n    if (array[array.length - 1] == '')\n        array.pop();\n\n    if (array[0] == '')\n        array.shift();\n\n    return array;\n};\n\n/**\n * Default chop operation to replace multiple spaces with 1 space.\n */\nTokenizer.prototype.chop = function (s) {\n    if (!s || s.length == 0)\n        return s;\n\n    return s.replace(/\\s+/ig, ' ').trim();\n};\n\n/**\n * Exposes an attached function that patches String with the \n * appropriate tokenize and possibly other functions.\n */\nTokenizer.prototype.attach = function () {\n    var self = this;\n\n    String.prototype.tokenize = function () {\n        return self.tokenize(this);\n    };\n};\n\n/**\n * Tokenizes the given string into the appropriate form.\n */\nTokenizer.prototype.tokenize = function() {};\n\n/*\n * Cleans the given string instead of tokenizing it by replacing \n * certain values and scrubbing of certain invalid characters.\n */\nTokenizer.prototype.clean = function() {};\n\nmodule.exports = Tokenizer;\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/tokenizers/regexp_tokenizer.js":"\nvar Tokenizer = require('./tokenizer');\nvar util = require('util');\nvar _ = require('underscore')._;\n\nvar RegExpTokenizer = function (options) {\n    var options = options || {};\n    this.pattern = options.pattern || (typeof this.pattern !== 'undefined' ?  this.pattern : \"\");\n    this.discardEmpty = options.discardEmpty || (typeof this.discardEmpty !== 'undefined' ? this.discardEmpty : true);\n    this.includeIndices = options.includeIndices || this.includeIndices;\n\n    // Match and split on gaps in the text\n    this.gaps = options.gaps || (typeof this.gaps !== 'undefined' ? this.gaps : true);\n    this.triggerChar = options.triggerChar ? options.triggerChar.charCodeAt(0) : \n        (typeof this.triggerChar !== 'undefined' ? this.triggerChar : 0);\n    this.matchExp = undefined;\n\n    // Clean pattern, triggering character and regexp\n    this.cleanPattern = options.cleanPattern || (typeof this.cleanPattern !== 'undefined' ? this.cleanPattern : \"\");\n    this.cleanTriggerChar = options.cleanTriggerChar ? options.cleanTriggerChar.charCodeAt(0) :\n        (typeof this.triggerChar !== 'undefined' ? this.triggerChar : 0);\n    this.replaceWith = options.replaceWith || (typeof this.replaceWith !== 'undefined' ? this.replaceWith : \"\");\n    this.cleanMatchExp = undefined;\n};\n\n// Inherit from the base tokenizer\nutil.inherits(RegExpTokenizer, Tokenizer);\n\nRegExpTokenizer.prototype._findTriggerChar = function (triggerChar, s) {\n    var foundTriggerChar = false;\n    var l = s.length;\n    for (var i = 0; i < l; ++i) {\n        if (triggerChar == s.charCodeAt(i)) {\n            foundTriggerChar = true;\n            break;\n        }\n    }\n\n    return foundTriggerChar;\n};\n\n/**\n * Resets the input based in a quick measurement \n * if there is such a thing as a triggering character.\n */\nRegExpTokenizer.prototype.reset = function (s) {\n    if (!s || s == \"\")\n        return;\n\n    var match = true;\n    var cleanMatch = true;\n    if (this.triggerChar && this.triggerChar > 0) {\n        if (!this._findTriggerChar(this.triggerChar, s)) {\n            this.matchExp = undefined;\n            match = false;\n        }\n    }\n    if (this.cleanTriggerChar && this.cleanTriggerChar > 0) {\n        if (!this._findTriggerChar(this.cleanTriggerChar, s)) {\n            this.cleanMatchExp = undefined;\n            cleanMatch = false;\n        }\n    }\n\n    if (match && this.pattern) {\n        this.matchExp = new RegExp(this.pattern);\n    }\n    if (cleanMatch && this.cleanPattern) {\n        this.cleanMatchExp = new RegExp(this.cleanPattern);\n    }\n\n    s = s.replace('’', '\\'');\n\n};\n\n/**\n * Tokenizes the given input \n */\nRegExpTokenizer.prototype.tokenize = function (s) {\n    this.reset(s);\n    if (this.gaps) {\n        if (this.matchExp) {\n            var results = s.split(this.matchExp);\n            return (this.discardEmpty) ? _.without(results, '', ' ') : results;\n        }\n    }\n    else if (this.matchExp) {\n        if (!this.includeIndices) {\n            return s.match(this.matchExp);\n        }\n        else {\n            var m, results  = [];\n            while (m = this.matchExp.exec(s)) {\n                var value = m[0];\n                results.push({ v: value, indices: [m.index, m.index + value.length]});\n            }\n            return results;\n        }\n    }\n\n    return [];\n};\n\n/*\n * Cleans the given input\n */\nRegExpTokenizer.prototype.clean = function (s) {\n    this.reset(s);\n    if (this.cleanMatchExp) {\n        return s.replace(this.cleanMatchExp, this.replaceWith);\n    }\n    return s;\n};\n\nmodule.exports = RegExpTokenizer;\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/tokenizers/wordpunct_tokenizer.js":"\nvar RegExpTokenizer = require('./regexp_tokenizer');\nvar util = require('util');\n\nvar WordPunctTokenizer = function (options) {\n    var options = options || {};\n    this.preserveEmoticons = options.preserveEmoticons || this.preserveEmoticons;\n    var pattern = \"(?:[0-9]|0[0-9]|1[0-9]|2[0-3]):[0-5][0-9]\\\\s?(?:[PApa]\\\\.?[Mm]\\\\.?)?\" + // time\n                   \"|\\\\d+(?:st|nd|rd|th)\" + // numeric 1st, 2nd..etc\n                   \"|\\\\$?\\\\d+(?:[\\\\d,]?\\\\d)*(?:\\\\.\\\\d+)?\\\\%?\" + // numerics (with and w/o commmas and decimals) (with and w/o percentage and or $)\n                   \"|(?:[a-zA-ZÁÂàáâÈÉÊèéê]+)-(?:[a-zA-ZÁÂàáâÈÉÊèéê]+)\" + // word with hyphen\n                   \"|(?:[a-zA-ZÁÂàáâÈÉÊèéê]+\\')?[a-zA-ZÁÂàáâÈÉÊèéê]+\" + // word with w/o accents w/o apos\n                   \"|\\\\%|\\\\!|\\\\.|;|,|:|\\\\\\'|\\\\\\\"|-|\\\\?|\\\\&|\\\\*|\\\\(|\\\\)\"; // punctuations\n                   \"|\\\\S\";\n    if (this.preserveEmoticons) {\n        pattern = \"(?:[<>]?[:;=8][\\\\-o\\\\*\\\\\\']?[\\\\)\\\\]\\\\(\\\\[dDpP/\\\\:\\\\}\\\\{@\\\\|\\\\\\\\]|[\\\\)\\\\]\\\\(\\\\[dDpP/\\\\:\\\\}\\\\{@\\\\|\\\\\\\\][\\\\-o\\\\*\\\\\\']?[:;=8][<>]?)\" + // emoticons\n                  \"|\" + pattern;\n    }\n    this.pattern = new RegExp(\"(\" + pattern + \")\", \"i\");\n    RegExpTokenizer.call(this, options);\n};\n\n// Inherit from the base tokenizer\nutil.inherits(WordPunctTokenizer, RegExpTokenizer);\nmodule.exports = WordPunctTokenizer;\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/tokenizers/url_tokenizer.js":"\nvar Tokenizer = require('./tokenizer');\nvar util = require('util');\nvar twitter = require('twitter-text');\n\nvar UrlTokenizer = function (options) {\n    var options = options || {};\n    this.includeIndices = typeof options.includeIndices !== 'undefined' ? options.includeIndices : this.includeIndices;\n};\n\n// Inherit from the base tokenizer\nutil.inherits(UrlTokenizer, Tokenizer);\n\nUrlTokenizer.prototype.tokenize = function (s) {\n    if (!s || s.length == 0)\n        return [];\n\n    if (this.includeIndices) {\n        return twitter.extractUrlsWithIndices(s);\n    }\n    else {\n        return twitter.extractUrls(s);\n    }\n};\n\nmodule.exports = UrlTokenizer;\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/tokenizers/tweet_tokenizer.js":"\nvar ArticleTokenizer = require('./article_tokenizer');\nvar util = require('util');\nvar _ = require('underscore')._;\nvar twitter = require('twitter-text');\n\nvar TweetTokenizer = function (options) {\n    var options = options || {};\n    this.preserveEmoticons = true;\n    ArticleTokenizer.call(this, options);\n};\n\n// Inherit from the base tokenizer\nutil.inherits(TweetTokenizer, ArticleTokenizer);\n\n/**\n * Tokenizes the given input \n */\nTweetTokenizer.prototype.tokenize = function (s) {\n    if (!s || s.length == 0)\n        return [];\n\n    if (!this.tokenizer) {\n        this._initializeTokenizers();\n    }\n\n    s = s.replace('’', '\\'');\n\n    var entities = twitter.extractEntitiesWithIndices(s);\n    var tokens = [];\n    var index = 0;\n    if (entities.length > 0) {\n        for (var i = 0; i < entities.length; i++) {\n            var entity = entities[i];\n            var indices = entity.indices;\n            if (index < indices[0] && index < s.length) {\n                var t = s.substring(index, indices[0]);\n                var results = this.tokenizer.tokenize(t);\n                tokens = tokens.concat(results);\n            }\n            if (entity.screenName) {\n                tokens.push('@' + entity.screenName);\n            }\n            else if (entity.hashtag) {\n                tokens.push('#' + entity.hashtag);\n            }\n            else if (entity.url) {\n                tokens.push(entity.url);\n            }\n            else if (entity.cashtag) {\n                tokens.push('$' + entity.cashtag);\n            }\n            else if (entity.v) {\n                tokens.push(entity.v);\n            }\n            index = indices[1];\n        }\n        if (index < s.length) {\n            var t = s.substring(index);\n            var results = this.tokenizer.tokenize(t);\n            tokens = tokens.concat(results);\n        }\n        return tokens;\n    }\n    else {\n        return this.tokenizer.tokenize(s);\n    }\n};\n\nmodule.exports = TweetTokenizer;\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/tokenizers/emoticon_tokenizer.js":"\nvar RegExpTokenizer = require('./regexp_tokenizer');\nvar util = require('util');\nvar _ = require('underscore')._;\n\nvar EmoticonTokenizer = function (options) {\n    var options = options || {};\n    this.gaps = false;\n    this.pattern = /[<>]?[:;=8][\\-o\\*\\']?[\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\]|[\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\][\\-o\\*\\']?[:;=8][<>]?/ig;\n    RegExpTokenizer.call(this, options);\n};\n\nutil.inherits(EmoticonTokenizer, RegExpTokenizer);\nmodule.exports = EmoticonTokenizer;\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/classifiers/bayes_classifier.js":"\nvar util = require('util'),\n    Classifier = require('./classifier');\n\nfunction BayesClassifier(smoothing) {\n    Classifier.call(this);\n    this.features = {};\n    this.classTotals = {};\n    this.totalExamples = 1;\n    this.smoothing = smoothing === undefined ? 1.0 : smoothing;\n};\n\nutil.inherits(BayesClassifier, Classifier);\n\n/**\n * Trains a particular observation to be classified as the given label.\n *\n * @param {Object} observation: the observation found\n * @param {String} classification: the association classification label.\n */\nBayesClassifier.prototype.addExample = function (observation, label) {\n    if (!this.classTotals[label]) {\n        this.classTotals[label] = 1;\n    }\n\n    this.totalExamples++;\n    this.classTotals[label]++;\n    if (observation instanceof Array) {\n        var i = observation.length;\n        while(i--) {\n            var feature = observation[i];\n            if (feature) {\n                // Ensure the default values are set for a given feature\n                if (!this.features[feature]) {\n                    this.features[feature] = {};\n                }\n\n                if (this.features[feature][label]) {\n                    this.features[feature][label]++;\n                }\n                else {\n                    this.features[feature][label] = 1 + this.smoothing;\n                }\n            }\n        }\n    }\n    else {\n        for (var key in observation) {\n            var feature = observation[key];\n            // Ensure default values are set for a given feature\n            if (!this.features[feature]) {\n                this.features[feature] = {};\n            }\n\n            if (this.features[feature][label]) {\n                this.features[feature][label]++;\n            }\n            else {\n                this.features[feature][label] = 1 + this.smoothing;\n            }\n        }\n    }\n};\n\n/**\n * p(C|D): Probability of class given document/observation. Takes the unseen \n * features and computes a probability that the given document/observation belongs \n * to the supplied class.\n *\n * @param {Object} observation: the observation to classify\n * @param {String} className: the classification label name\n */\nBayesClassifier.prototype.probabilityOfClass = function (observation, className) {\n    var probability = 1;\n    if (observation instanceof Array) {\n        var i = observation.length;\n        while (i--) {\n            var feature = observation[i];\n            probability += this.probabilityOfFeatureGivenClass(feature, className);\n        }\n    }\n    else {\n        for (var key in observation) {\n            var feature = observation[key];\n            probability += this.probabilityOfFeatureGivenClass(feature, className);\n        }\n    }\n\n    // p(C) * unlogging above calculation p(X|C)\n    probability = (this.classTotals[className] / this.totalExamples) * Math.exp(probability);\n    return probability;\n};\n\n/**\n * p(F,C). Computes the probability of observing a feature given a class name.\n *\n * @param {Object} feature: the feature to find the probability of \n * @param {String} className: the classification label name\n */\nBayesClassifier.prototype.probabilityOfFeatureGivenClass = function (feature, className) {\n    var probability = 0;\n    var count = this.smoothing;\n    if (feature in this.features) {\n        var fClasses = this.features[feature];\n        count = fClasses[className] || this.smoothing;\n    }\n    return Math.log(count / this.classTotals[className]);\n};\n\n/**\n * Returns the set of labels and association probability for a given \n * observation being made.\n *\n * @param {Object} observation: the observation to classify\n */\nBayesClassifier.prototype.getClassifications = function (observation) {\n    var classifier = this;\n    var labels = [];\n    for (var className in this.classTotals) {\n        labels.push({label: className, value: this.probabilityOfClass(observation, className) });\n    }\n\n    return labels.sort(function (x, y) { return y.value - x.value });\n};\n\nBayesClassifier.restore = function (classifier) {\n    classifier = Classifier.restore(classifier);\n    classifier.__proto__ = BayesClassifier.prototype;\n    return classifier;\n};\n\nmodule.exports = BayesClassifier;\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/classifiers/classifier.js":"\nfunction Classifier() {\n};\n\n/**\n * Restores the classifier from a json saved model.\n *\n * @param {String} classifier saved json model.\n */\nClassifier.restore = function (classifier) {\n    classifier = typeof classifier == 'string' ? JSON.parse(classifier) : classifier;\n    return classifier;\n};\n\n/**\n * Saves the given classifier to a json model including the type information.\n */\nClassifier.prototype.save = function () {\n    var json = JSON.stringify(this);\n    return json;\n};\n\n/**\n * Trains a particular observation to be classified as the given label.\n *\n * @param {Object} observation: the observation found\n * @param {String} classification: the association classification label.\n */\nClassifier.prototype.addExample = function (observation, classification) {\n};\n\n/**\n * Trains a particular set of observations to be classified as the given labels.\n * \n * @param {Object} observation: the observation found\n * @param {String} classification: the association classification label.\n */\nClassifier.prototype.addExamples = function (observations, classifications) {\n    if (observations.elements) {\n        var m = observations.rows();\n        for (var i = 1; i <= m; i++) {\n            var outputi = classifications.row(i).e(1,1);\n            var inputi = observations.row(i).elements;\n            this.addExample(inputi, outputi);\n        }\n    }\n    else {\n        var m = observations.length;\n        for (var i = 0; i < m; i++) {\n            this.addExample(observations[i], classifications[i]);\n        }\n    }\n};\n\n/**\n * Prototype function for loading in a particular database or training set \n * data in order to add many examples to the classifier.\n */\nClassifier.prototype.train = function () {};\n\n/**\n * Returns the appropriate set of probabilities that the given \n * observation is found according to the nearest labels.\n *\n * @param {Object} observation: the observation to classify\n */\nClassifier.prototype.classify = function (observation) {\n    var labels = this.getClassifications(observation);\n    if (labels && labels.length > 0 && labels[0].label) {\n        return labels[0].label;\n    }\n\n    return undefined;\n};\n\n/**\n * Returns the set of labels and association probability for a given \n * observation being made.\n *\n * @param {Object} observation: the observation to classify\n */\nClassifier.prototype.getClassifications = function (observation) {\n};\n\nmodule.exports = Classifier;\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/classifiers/logistic_regression.js":"\nvar Classifier = require('./classifier');\nvar util = require('util');\nvar Matrix = require('./../math/matrix');\nvar Vector = require('./../math/vector');\nvar MeanNormalizer = require('./../normalizers/mean_normalizer');\n\nvar LogisticRegression = function (options) {\n    var options = options || {};\n    this.examples = [];\n    this.outputs = [];\n    this.labels = [];\n    this.features = null;\n    this.outputVector = null;\n    this.iterations = options.iterations || 500;\n    this.alpha = options.alpha || 0.03;\n    this.regularization = true;\n    this.lambda = (options.lambda || 0.3);\n    this.normalizer = options.normalizer || new MeanNormalizer();\n};\n\nutil.inherits(LogisticRegression, Classifier);\n\n/**\n * Adds an example to the items to train against.\n */\nLogisticRegression.prototype.addExample = function (input, label) {\n    this.examples.push(input);\n    if (this.labels.indexOf(label) < 0) {\n        this.labels.push(label);\n    }\n\n    this.outputs.push([this.labels.indexOf(label) + 1]);\n};\n\n/**\n * Normalizes all examples by running them through mean-normalization.\n * This will calculate each element in the matrix using the ith element \n * in each column minus the average of the column divided by the standard \n * deviation of the column (or mean normalization).\n */\nLogisticRegression.prototype.normalize = function () {\n    this.features = this.normalizer.normalize(this.examples);\n    this.outputVector = $M(this.outputs);\n    return this.features;\n};\n\n/**\n * Normalizes the given input list according to the already trained \n * and normalized data using mean normalization.\n *\n * @param [Array] input: Input features of n-dimensions\n */\nLogisticRegression.prototype.normalizeInput = function (input) {\n    return this.normalizer.normalizeInput(input);\n};\n\n/**\n *  Runs gradient descent for each of the labeled input using the normalized \n *  design matrix, output y vector (using one vs all), alpha learning rate, \n *  number of iterations to run gradint descent and the initial theta parameters.\n *  \n *  @returns The proposed theta for each of the labels after running gradient descent.\n */\nLogisticRegression.prototype.train = function () {\n    if (!this.features) {\n        this.normalize();\n    }\n\n    var allTheta = [];\n    for (var i = 0; i < this.labels.length; i++) {\n        var result = this.trainLabel(this.labels[i]);\n        result.label = this.labels[i];\n        result.labelIndex = i;\n        allTheta.push(result);\n    }\n    this.trainedTheta = allTheta;\n    return allTheta;\n};\n\n/**\n * Tests the given samples and output according to the already trained theta.\n *\n * @returns The total error cost over the all-vs-one computed cost function.\n */\nLogisticRegression.prototype.test = function (samples, output) {\n    var features = this.normalizer.normalize(samples);\n    var outputVector = $M(output);\n    var cost = [];\n    for (var i = 0; i < this.trainedTheta.length; i++) {\n        var value = this.trainedTheta[i];\n        var labelY = outputVector.map(function (x, i) { return x == value.labelIndex ? 1 : 0; });\n        cost.push(this.computeCost(value.theta, features, labelY));\n    }\n    return cost;\n};\n\n/**\n * Runs gradient descent for the given label using the normalized design matrix and a\n * conditional y-vector for the given label using the one-vs-all approach to logistic \n * regression. This leverages the alpha learning rate, lambda regularization and \n * gradient descent for the given number of iterations to produce a proper theta output\n * parameters optimized for the given label over the training set.\n *\n * @returns The proposed theta vector parameters for the given label.\n */\nLogisticRegression.prototype.trainLabel = function (label) {\n    var dim = this.features.dimensions();\n    var n = dim.cols;\n    var theta = Matrix.Zeros(n, 1);\n    var lambdaV = undefined;\n    if (this.lambda) {\n        var l = [];\n        l.push([0]);\n        for (var i = 1; i < n; i++) {\n            l.push([this.lambda]);\n        }\n\n        lambdaV = $M(l);\n    }\n    var X = this.features;\n    var index = this.labels.indexOf(label) + 1;\n    var labelY = this.outputVector.map(function (x, i) { return x == index ? 1 : 0; });\n    var alpha = this.alpha;\n    var iterations = this.iterations;\n    var result = this.gradientDescent(theta, X, labelY, alpha, iterations, lambdaV);\n    return result;\n};\n\n/**\n * Standard implementation of the Multivariant Square-Error Cost function \n * for reguarlized and non-regularized logistic regression:\n * using the implementation of J(Θ) = -1/m * sum [ y(i) * log(h(x(i))) + (1 - y(i) * log(1 - h(x(i))) ] + regularization_parameter\n * where regularization parameter: (diag(Θ) * λ)'Θ/2m \n * and h(x(i)) is the sigmoid function: σ (z) = 1 / (1 + e^-z) and \n * h(x) = σ(Θ'x) or more appropriately simplified:\n * h(x(i)) = 1 / (1 + e^-Θ'x). Using vectorization, this is further simplified to:\n *\n * J(Θ) = -1/m * [ log(1/(1+e^-XΘ))'y + log(1-(1/1+e^-XΘ))'(1-y) ] + (diag(Θ)λ)'Θ/2m\n * J(Θ) = -1/m * [ log(sigmoid)'y + log(1-sigmoid)'(1-y) ] + (diag(Θ)λ)'Θ/2m\n *\n * @param {Vector} theta: n+1-dimensional vector of parameters used to represent the Θ.\n * @param {Matrix} X: m x (n+1) matrix of m-samples of n-features.\n * @param {Vector} y: m x 1-dimensional vector of output values\n * @param {Vector} lambdaVec: n+1-dimensional vector of regularization parameter.\n */\nLogisticRegression.prototype.computeCost = function (theta, X, y, lambdaVec) {\n    var sigmoid = this.sigmoid(X.multiply(theta));\n    var m = X.rows();\n    var prob1 = sigmoid.log().transpose().multiply(y).e(1,1);\n    var prob0 = sigmoid.multiply(-1).add(1).log().transpose()\n                       .multiply((y.multiply(-1).add(1))).e(1,1);\n    var cost = (-1.0 * (prob1 + prob0)) / m;\n    if (lambdaVec) {\n        cost += Matrix.Diagonal(theta.elements)\n                .multiply(lambdaVec).transpose()\n                .multiply(theta).div(2 * m).e(1,1);\n    }\n    return cost;\n};\n\n/**\n * Gradient Descent regression algorithm to compute the minimal cost \n * of a hypothesis h(x) that best fits the output data. This is done \n * using the implementation: Θ = Θ - α ((XΘ-y)'X/m)'\n *\n * Θ := diag(Θ) * (1 - α λ/m) - (α / m) * sum [ (h(x(i)) - y(i)) x(i) ]\n *\n * where h(x) is the sigmoid function computed upon: XΘ\n * or in vectorized form:\n *\n * Θ := diag(Θ) * (1 - α λ/m) - (α / m) * sum [ (h(x(i)) - y(i)) x(i) ]\n * Θ := diag(Θ) * (1 - α λ/m) - α ((sigmoid(XΘ) - y)'X/m)' ]\n *\n * @param {Vector} theta: n+1-dimensional vector of parameters used to represent the Θ.\n * @param {Matrix} X: m x (n+1) matrix of m-samples of n-features.\n * @param {Vector} y: m x 1-dimensional vector of output values.\n * @param {Number} alpha: alpha-learning rate used in the gradient descent algorithm.\n * @param {Number} iterations: # of iterations to run gradient descent.\n * @param {Vector} lambdaVec: n+1-dimensional vector of regularization parameter.\n */\nLogisticRegression.prototype.gradientDescent = function (theta, X, y, alpha, iterations, lambdaVec) {\n    var m = X.rows();\n    var history = [];\n\n    for (var i = 0; i < iterations; i++) {\n        var result = this.sigmoid(X.multiply(theta)).subtract(y)\n                      .transpose().multiply(X).div(m);\n        if (lambdaVec) {\n            var lambdaAdjust = lambdaVec.multiply(-1 * alpha).div(m).add(1);\n            var thetaAdjust = Matrix.Diagonal(theta.elements).multiply(lambdaAdjust);\n            theta = thetaAdjust.subtract(result.multiply(alpha).transpose());\n        }\n        else {\n            theta = theta.subtract(result.multiply(alpha).transpose());\n        }\n        history.push(this.computeCost(theta, X, y, lambdaVec));\n    }\n\n    var error  = this.computeCost(theta, X, y);\n    return { 'theta': theta, 'cost': history, 'error': error };\n};\n\n/**\n * Sigmoid or logistic function represents the bounded differentiable \n * real function for all real input values showing the cumulative distribution \n * between 0 and 1 given the input value Z.\n *\n * @param {Vector|Matrix|Number} z: The item to calculate sigmoid.\n */\nLogisticRegression.prototype.sigmoid = function (z) {\n    return Matrix.sigmoid(z);\n};\n\n/**\n * Predicts the value probability output of the given input feature vector \n * against the class-based theta parameters.\n *\n * @param {Vector} theta: n+1-dimensional vector of parameters associated with a given class.\n * @param {Vector} input: n+1-dimensional vctor of input features to calculate the probability y-output.\n */\nLogisticRegression.prototype.calculate = function (theta, input) {\n    return this.sigmoid(theta.transpose().multiply(input));\n};\n\n/**\n * Returns the set of labels and association probability for a given \n * observation input vector being made.\n */\nLogisticRegression.prototype.getClassifications = function (input) {\n    if (!this.trainedTheta) {\n        throw new Error('must train the classifier first');\n        return;\n    }\n\n    var classifier = this;\n    var labels = [];\n    for (var i = 0; i < this.trainedTheta.length; i++) {\n        var thetaInput = this.trainedTheta[i];\n        var result = this.calculate(thetaInput.theta, input);\n        labels.push({ label: thetaInput.label, value: result.max() });\n    }\n\n    return labels.sort(function (x, y) { return y.value > x.value });\n};\n\nmodule.exports = LogisticRegression;\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/math/matrix.js":"\nvar Matrix = require('sylvester').Matrix;\nvar fs = require('fs');\n\n/**\n * Returns a vector of each column's standard deviation.\n */\nMatrix.prototype.std = function () {\n    var dim = this.dimensions();\n    var mMean = this.mean();\n    var r = [];\n    for (var i = 1; i <= dim.cols; i++) {\n        var meanDiff = this.col(i).subtract(mMean.e(i));\n        meanDiff = meanDiff.elementMultiply(meanDiff);\n        r.push(Math.sqrt(meanDiff.sum() / dim.rows));\n    }\n    return $V(r);\n};\n\nMatrix.prototype.maxIndex = function () {\n    var m = 0,\n    i = this.elements.length,\n    nj = this.elements[0].length,\n    j;\n\n    var maxI = -1;\n    var maxJ = -1;\n    while (i--) {\n        j = nj;\n        while (j--) {\n            if (Math.abs(this.elements[i][j]) > Math.abs(m)) {\n                m = this.elements[i][j];\n                maxI = i + 1;\n                maxJ = j + 1;\n            }\n        }\n    }\n\n    return { value: m, row: maxI, col: maxJ };\n};\n\n/**\n * Reshapes the given vector or array of elements into the given \n * matrix dimensions of rows and columns.\n *\n * @param {Vector|Array} vector: The array to reshape into a matrix.\n * @param {Number} rows: Number of rows in the matrix\n * @param {Number} cols: Number of columns in the matrix.\n */\nMatrix.reshape = function (vector, rows, cols) {\n    var elements = null;\n    if (vector.elements) {\n        elements = vector.elements;\n    }\n    else {\n        elements = vector;\n    }\n\n    var matrix = Matrix.Zeros(rows, cols); \n    for (var r = 0; r < rows; r++) {\n        for (var c = 0; c < cols; c++) {\n            var index = (c * rows) + r;\n            matrix.elements[r][c] = elements[index];\n        }\n    }\n\n    return matrix;\n};\n\n/**\n * Derivative of sigmoid or logistic function.\n *\n * @param {Vector|Matrix|Number} z: The item to calculate sigmoid.\n */\nMatrix.sigmoidGradient = function (z) {\n    var sig = Matrix.sigmoid(z);\n    if (sig.elements) {\n        return sig.elementMultiply(sig.multiply(-1).add(1));\n    }\n    else {\n        return sig * (1 - sig);\n    }\n};\n\n/**\n * Derivative of sigmoid or logistic function.\n */\nMatrix.prototype.sigmoidGradient = function () {\n    return Matrix.sigmoidGradient(this);\n};\n\n/**\n * Sigmoid or logistic function represents the bounded differentiable \n * real function for all real input values showing the cumulative distribution \n * between 0 and 1 given the input value Z.\n *\n * @param {Vector|Matrix|Number} z: The item to calculate sigmoid.\n */\nMatrix.sigmoid = function (z) {\n    if (!z.elements) {\n        return (1.0 / (1 + Math.exp(-1.0 * z)));\n    }\n    else {\n        var rows = z.rows();\n        var cols = z.cols();\n        var result = Matrix.Zeros(rows,cols).elements;\n        for (var i = 1; i <= rows; i++) {\n            for (var j = 1; j <= cols; j++) {\n                var e = z.e(i,j);\n                result[i-1][j-1] = Matrix.sigmoid(e);\n            }\n        }\n        return $M(result);\n    }\n};\n\n/**\n * Sigmoid or logistic function represents the bounded differentiable \n * real function for all real input values showing the cumulative distribution \n * between 0 and 1 given the input value Z.\n */\nMatrix.prototype.sigmoid = function () {\n    return Matrix.sigmoid(this);\n};\n\nMatrix.tanhGradient = function (z) {\n    var v = Matrix.tanh(z);\n    if (v.elements) {\n        return v.elementMultiply(v).multiply(-1).add(1);\n    }\n    else {\n        return 1 - (v * v);\n    }\n};\n\nMatrix.prototype.tanhGradient = function () {\n    return Matrix.tanhGradient(this);\n};\n\n/**\n * Hyperbolic tangent as a symmetric alternative to the logistic function.\n *\n * @param {Vector|Matrix|Number} z: The item to calculate sigmoid.\n */\nMatrix.tanh = function (z) { \n    if (!z.elements) {\n        return (Math.exp(z) - Math.exp(-z)) / (Math.exp(z) + Math.exp(-z));\n    }\n    else {\n        var rows = z.rows();\n        var cols = z.cols();\n        var result = Matrix.Zeros(rows,cols).elements;\n        for (var i = 1; i <= rows; i++) {\n            for (var j = 1; j <= cols; j++) {\n                var e = z.e(i,j);\n                result[i-1][j-1] = Matrix.tanh(e);\n            }\n        }\n        return $M(result);\n    }\n};\n\n/**\n * Hyperbolic tangent as a symmetric alternative to the logistic function.\n */\nMatrix.prototype.tanh = function () {\n    return Matrix.tanh(this);\n};\n\n/**\n * Loads a .dat matlab file that consists of matrix data.\n *\n * @param {String} dat: location to the data file containing the matrix data.\n */\nMatrix.load = function (dat) {\n    var lines = fs.readFileSync(dat).toString().split('\\n').map(function (x, i) { return x.trim() });\n    lines = lines.filter(function (x, i) { return x.length > 0 && x.indexOf('#') < 0 });\n    var elements = [];\n    for (var l = 0; l < lines.length; l++) {\n        var items = lines[l].split(' ').map(function (x) { return parseFloat(x); });\n        elements.push(items);\n    }\n    return $M(elements);\n};\n\nmodule.exports = Matrix;\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/math/vector.js":"\nvar Vector = require('sylvester').Vector;\nvar Matrix = require('./matrix');\n\nVector.prototype.div = function (item) {\n    if (typeof item == 'number') {\n        return this.map(function (v, i) {\n            return v / item;\n        });\n    }\n    else {\n        return this.elementDivide(item);\n    }\n};\n\nVector.prototype.reshape = function (r, c) {\n    return Matrix.reshape(this.elements, r, c);\n};\n\nVector.sigmoid = function () {\n    return Matrix.sigmoid(this);\n};\n\nmodule.exports = Vector;\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/normalizers/mean_normalizer.js":"\nvar Matrix = require('./../math/matrix');\nvar Vector = require('./../math/vector');\n\nvar MeanNormalizer = function (options) {\n    this.normalizeFeature0 = true;\n};\n\n/**\n * Normalizes all examples by running them through mean-normalization.\n * This will calculate each element in the matrix using the ith element\n * in each column minus the average of the column divided by the standard\n * deviation of the column (or mean normalization).\n */\nMeanNormalizer.prototype.normalize = function (observations) {\n    var features = $M(observations);\n\n    var dim = features.dimensions();\n    var m = dim.rows;\n    var n = dim.cols;\n    var mu = Matrix.Zeros(1, n);\n    var sigma = Matrix.Zeros(1, n);\n\n    this.mu = features.mean();\n    this.sigma = features.std();\n    var elements = [];\n    if (this.normalizeFeature0) {\n        var feature0 = Vector.One(m);\n        elements.push(feature0.elements);\n    }\n\n    for (var i = 1; i <= n; i++) {\n        var r = features.col(i).subtract(this.mu.e(i)).div(this.sigma.e(i));\n        elements.push(r.elements);\n    }\n\n    return $M(elements).transpose();\n};\n\n/**\n * Normalizes the given input list according to the already trained\n * and normalized data using mean normalization.\n *\n * @param [Array] input: Input features of n-dimensions\n */\nMeanNormalizer.prototype.normalizeInput = function (observation) {\n    var inputNorm = [];\n    if (this.normalizeFeature0) {\n        inputNorm.push(1);\n    }\n\n    for (var i = 1; i <= observation.length; i++) {\n        inputNorm.push((observation[i-1] - this.mu.e(i)) / this.sigma.e(i));\n    }\n    \n    return $V(inputNorm);\n};\n\nmodule.exports = MeanNormalizer;\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/regression/linear_regression.js":"\nvar Regression = require('./regression');\nvar util = require('util');\nvar Matrix = require('./../math/matrix');\nvar Vector = require('./../math/vector');\nvar MeanNormalizer = require('./../normalizers/mean_normalizer');\n\nvar LinearRegression = function (options) {\n    var options = options || {};\n    this.examples = [];\n    this.outputs = [];\n    this.features = null;\n    this.outputVector = null;\n    this.iterations = options.iterations || 1500;\n    this.alpha = options.alpha || 0.03;\n    this.regularization = options.regularization || false;\n    this.lambda = (options.lambda || 1) * (this.regularization ? 1 : 0);\n    this.normalizer = options.normalizer || new MeanNormalizer();\n};\n\nutil.inherits(LinearRegression, Regression);\n\n/**\n * Adds an example to the items to train against.\n */\nLinearRegression.prototype.addExample = function (input, output) {\n    this.examples.push(input);\n    this.outputs.push([output]);\n};\n\n/**\n * Normalizes all examples by running them through mean-normalization.\n * This will calculate each element in the matrix using the ith element \n * in each column minus the average of the column divided by the standard \n * deviation of the column (or mean normalization).\n */\nLinearRegression.prototype.normalize = function () {\n    this.features = this.normalizer.normalize(this.examples);\n    this.outputVector = $M(this.outputs);\n    return this.features;\n};\n\n/**\n * Normalizes the given input list according to the already trained \n * and normalized data using mean normalization.\n *\n * @param [Array] input: Input features of n-dimensions\n */\nLinearRegression.prototype.normalizeInput = function (input) {\n    return this.normalizer.normalizeInput(input);\n};\n\n/**\n * Runs gradient descent using the normalized design matrix, output y vector \n * alpha learning rate, number of iterations to run gradient descent and the \n * initial theta parameters for the hypothesis h(x).\n *\n * @returns The proposed theta after running gradient descent and the cost history.\n */\nLinearRegression.prototype.train = function () {\n    if (!this.features) {\n        this.normalize();\n    }\n\n    var dim = this.features.dimensions();\n    var n = dim.cols;\n\n    var theta = Matrix.Zeros(n, 1);\n    var l = [];\n    l.push([0]);\n    for (var i = 1; i < n; i++) {\n        l.push([this.lambda]);\n    }\n    var lambdaV = $M(l);\n    var X = this.features;\n    var y = this.outputVector;\n    var alpha = this.alpha;\n    var iterations = this.iterations;\n    var result = this.gradientDescent(theta, X, y, alpha, iterations, lambdaV);\n    this.trainedTheta = result.theta;\n    return result;\n};\n\n/**\n * Tests the given samples and output according to the already trained theta.\n *\n * @returns The total error cost over the all-vs-one computed cost function.\n */\nLinearRegression.prototype.test = function (samples, output) {\n    var features = this.normalizer.normalize(samples);\n    var outputVector = $M(output);\n    var cost = this.computeCost(this.trainedTheta, features, outputVector);\n    return cost;\n};\n\n/**\n * Standard implementation of the Multivariant Square-Error Cost function \n * using the implementation of J(Θ) = (XΘ-y)'(XΘ-y) / 2m  + regularization_parameter\n * where regularization parameter: (diag(Θ) * λ)'Θ/2m \n *\n * @param {Vector} theta: n+1-dimensional vector of parameters used to represent the Θ.\n * @param {Matrix} X: m x (n+1) matrix of m-samples of n-features.\n * @param {Vector} y: m x 1-dimensional vector of output values\n * @param {Vector} lambda: n+1-dimensional vector of regularization parameter.\n */\nLinearRegression.prototype.computeCost = function (theta, X, y, lambda) {\n    var m = X.rows();\n    var n = X.cols();\n    var result = X.multiply(theta).subtract(y);\n    var cost = result.transpose().multiply(result).div(2 * m).e(1, 1);\n    if (lambda) {\n        cost += Matrix.Diagonal(theta.elements)\n                .multiply(lambda).transpose()\n                .multiply(theta).div(2 * m).e(1,1);\n    }\n    return cost;\n};\n\n/**\n * Gradient Descent regression algorithm to compute the minimal cost \n * of a hypothesis h(x) that best fits the output data. This is done \n * using the implementation: Θ = Θ - α ((XΘ-y)'X/m)'\n *\n * @param {Vector} theta: n+1-dimensional vector of parameters used to represent the Θ.\n * @param {Matrix} X: m x (n+1) matrix of m-samples of n-features.\n * @param {Vector} y: m x 1-dimensional vector of output values.\n * @param {Number} alpha: alpha-learning rate used in the gradient descent algorithm.\n * @param {Number} iterations: # of iterations to run gradient descent.\n * @param {Vector} lambda: n+1-dimensional vector of regularization parameter.\n */\nLinearRegression.prototype.gradientDescent = function (theta, X, y, alpha, iterations, lambda) {\n    var m = X.rows();\n    var history = [];\n\n    for (var i = 0; i < iterations; i++) {\n        var result = X.multiply(theta).subtract(y)\n                      .transpose().multiply(X).div(m);\n        if (lambda) {\n            var lambdaAdjust = lambda.multiply(-1 * alpha).div(m).add(1);\n            var thetaAdjust = Matrix.Diagonal(theta.elements).multiply(lambdaAdjust);\n            theta = thetaAdjust.subtract(result.multiply(alpha).transpose());\n        }\n        else {\n            theta = theta.subtract(result.multiply(alpha).transpose());\n        }\n        history.push(this.computeCost(theta, X, y, lambda));\n    }\n\n    var error = this.computeCost(theta, X, y);\n    return { 'theta': theta, 'cost': history, 'error': error };\n};\n\n/**\n * Predicts the value output of the given input feature vector.\n *\n * @param {Vector} theta: n+1-dimensional vector of parameters used to represent the Θ.\n * @param {Vector} input: n+1-dimensional vector of input features to calculate y-output.\n */\nLinearRegression.prototype.calculate = function (theta, input) {\n    return theta.transpose().multiply(input);\n};\n\nmodule.exports = LinearRegression;\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/regression/regression.js":"\nvar fs = require('fs'),\n    readline = require('readline');\n\nvar Regression = function () {\n};\n\nRegression.prototype.train = function () {\n};\n\nRegression.prototype.addExample = function (input, output) {\n};\n\nRegression.prototype.load = function (input, callback) {\n    var lines = fs.readFileSync(input).toString().split('\\n');\n    for (var l = 0; l < lines.length; l++) {\n        var items = lines[l].split(',');\n        var features = [];\n        var output = 0;\n        for (var i = 0; i < items.length; i++) {\n            if (i < items.length - 1) {\n                features.push(parseFloat(items[i]));\n            } \n            else {\n                output = parseFloat(items[i]);\n            }\n        }\n        if (features.length > 0) {\n            this.addExample(features, output);\n        }\n    }\n};\n\nmodule.exports = Regression;\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/neuralnetworks/neuralnetwork.js":"\nvar util = require('util');\nvar Classifier = require('./../classifiers/classifier');\nvar SimpleNormalizer = require('./../normalizers/simple_normalizer');\nvar Matrix = require('./../math/matrix');\nvar Vector = require('./../math/vector');\n\nvar NeuralNetwork = function (options) {\n    var options = options || {};\n    this.examples = [];\n    this.outputs = [];\n    this.labels = [];\n    this.features = null;\n    this.outputVector = null;\n    this.iterations = options.iterations || 100;\n    this.alpha = options.alpha || 0.03;\n    this.lambda = (options.lambda || 0.3);\n    this.normalizer = options.normalizer || new SimpleNormalizer();\n    this.layers = options.layers || [ 25 ]; // array of units per layer\n    this.epsilon = options.epsilon || 0.13;\n};\n\nutil.inherits(NeuralNetwork, Classifier);\n\n/**\n * Adds an example to the items to train against.\n */\nNeuralNetwork.prototype.addExample = function (input, label) {\n    this.examples.push(input);\n    if (this.labels.indexOf(label) < 0) {\n        this.labels.push(label);\n    }\n\n    this.outputs.push([this.labels.indexOf(label) + 1]);\n};\n\n/**\n * Normalizes all examples by running them through mean-normalization.\n * This will calculate each element in the matrix using the ith element \n * in each column minus the average of the column divided by the standard \n * deviation of the column (or mean normalization).\n */\nNeuralNetwork.prototype.normalize = function () {\n    this.features = this.normalizer.normalize(this.examples);\n    this.outputVector = $M(this.outputs);\n    this.identityLabels = Matrix.I(this.labels.length);\n    var m = this.outputs.length;\n    var mappedOutput = [];\n    for (var i = 0; i < m; i++) {\n        var ithActualValue = this.outputs[i][0];\n        var ithOutputVector = this.identityLabels.row(ithActualValue);\n        mappedOutput.push(ithOutputVector.elements);\n    }\n\n    this.mappedOutputMatrix = $M(mappedOutput);\n    return this.features;\n};\n\n/**\n * Normalizes the given input list according to the already trained \n * and normalized data using mean normalization.\n *\n * @param [Array] input: Input features of n-dimensions\n */\nNeuralNetwork.prototype.normalizeInput = function (input) {\n    return this.normalizer.normalizeInput(input);\n};\n\n/**\n * Trains the neural network using the standard backpropagation algorithm with \n * gradient descent as the cost optimization method.\n */\nNeuralNetwork.prototype.train = function () {\n    if (!this.features) {\n        this.normalize();\n    }\n\n    // Initialize theta to a random set of weights to start from\n    var dim = this.features.dimensions();\n    var inputConnections = dim.cols;\n    var thetaLD = [];\n    for (var l = 0; l < this.layers.length; l++) {\n        var outputConnections = this.layers[l];\n        thetaLD.push(this.randomInitialize(outputConnections, inputConnections));\n        inputConnections = outputConnections + 1;\n    }\n    thetaLD.push(this.randomInitialize(this.labels.length, inputConnections));\n\n    // Setup the gradient descent algorithm to run\n    var result = this.gradientDescent(thetaLD, this.features, this.outputVector, \n                    this.alpha, this.iterations, this.lambda);\n    this.trainedTheta = result.theta;\n    return result;\n};\n\n/**\n * Tests the given samples and output according to the already trained theta.\n *\n * @returns The total error cost over the neural network.\n */\nNeuralNetwork.prototype.test = function (samples, output) {\n    var features = this.normalizer.normalize(samples);\n    var outputVector = $M(output);\n    var cost = this.computeCost(this.trainedTheta, features, outputVector);\n    return cost;\n};\n\n/**\n * Initializes a random matrix set of weights using the preconfigured \n * epsilon unit and random initialization algorithm for weights.\n *\n * @param {Number} r: Number of rows in the matrix\n * @param {Number} c: Number of columns in the matrix\n */\nNeuralNetwork.prototype.randomInitialize = function (r, c) {\n    return Matrix.Random(r, c).multiply(2 * this.epsilon).subtract(this.epsilon);\n};\n\n/**\n * Error-Cost function of the an L-dimensional neural network. This will perform \n * the regularized or non-regularized neural network using a modified logistic regression cost function:\n * J(Θ) = -1/m * sum [ y(i) * log(h(x(i))) + (1 - y(i) * log(1 - h(x(i))) ] + regularization_parameter\n * with the exception of including the summation of each label in the last layer as an inner summation.\n * where regularization parameter: (diag(Θ) * λ)'Θ/2m\n * and h(x(i)) is the sigmoid function: σ (z) = 1 / (1 + e^-z) and\n * h(x) = σ(Θ'x) or more appropriately simplified:\n * h(x(i)) = 1 / (1 + e^-Θ'x). Using vectorization, this is further simplified to:\n *\n * J(Θ) = -1/m * [ log(1/(1+e^-XΘ))'y + log(1-(1/1+e^-XΘ))'(1-y) ] + (diag(Θ)λ)'Θ/2m\n * J(Θ) = -1/m * [ log(sigmoid)'y + log(1-sigmoid)'(1-y) ] + (diag(Θ)λ)'Θ/2m\n *\n * To compute the cost function for the last layer of K-labels, we must perform forward propogation first of \n * the input values against the given ThetaLD parameters until we can compute the cost in layer L (or output layer).\n *\n * J(Θ) = sum k-labels [ log(sigmoid)'yk + log(1-sigmoid)'(1-yk) ] + (diag(Θ)'Θ)λ/2m\n * where yk is the binary vector for label k in the K-number of labels provided by the output layer y and \n * sigmoid in this case is the activation output of the final hidden layer propogated to units in the output layer.\n *\n * @param {NDMatrix} theta: NxL dimensional matrix of parameters used to represent the big-Θ.\n * @param {Matrix} X: m x (n+1) matrix of m-samples of n-features\n * @param {Vector} y: m x 1-dimensional vector of output values.\n * @param {Vector} lambda: n+1-dimensional vector of regularization parameter.\n */\nNeuralNetwork.prototype.computeCost = function (ThetaLD, X, y, lambda) {\n    var activatedOutput = this.feedForward(ThetaLD, X);\n    var cost = 0;\n    var m = X.dimensions().rows;\n    for (var k = 1; k <= this.labels.length; k++) {\n        var labelKActivation = $M(activatedOutput.col(k));\n        var yk = y.map(function (x, i) { return x == k ? 1 : 0; });\n        var prob1 = labelKActivation.log().transpose().multiply(yk).e(1,1);\n        var prob0 = labelKActivation.multiply(-1).add(1).log().transpose()\n                                    .multiply((yk.multiply(-1).add(1))).e(1,1);\n        cost += (-1.0 * (prob1 + prob0)) / m;\n    }\n    if (lambda) {\n        var lambdaCost = 0;\n        for (var l = 0; l < ThetaLD.length; l++) {\n            var thetaL = ThetaLD[l];\n            lambdaCost += thetaL.map(function (x) { return x * x }).sum();\n        }\n        cost += (lambda / (2 * m)) * lambdaCost;\n    }\n    return cost;\n};\n\n/**\n * Propogates the input of X through the various layers with weights provided \n * by the multi-dimensional matrix ThetaLD, which derives of weights in each \n * layer of the neural network for applied weights of input to the first hidden layer \n * and all hidden layers until we reach the output layer. This will return the \n * activated result of the output layer which can be used to determine probability of \n * labels, compute the cost function or perform backpropagation.\n *\n * @param {NDMatrix} ThetaLD: NxL dimensional matrix of parameters used to represent big-Θ.\n * @param {Matrix} X: m x (n+1) matrix of m-samples of n-features.\n */\nNeuralNetwork.prototype.feedForward = function (ThetaLD, X, includePropagation) {\n    var layers = ThetaLD.length;\n    var input = X;\n    var output = [];\n    for (var l = 0; l < layers; l++) {\n        // obtain the weights in layer l\n        var layerThetaM = ThetaLD[l];\n        var weightedInputLayerL = input.multiply(layerThetaM.transpose());\n        input = Matrix.sigmoid(weightedInputLayerL);\n\n        // Modify the input to include the bias unit on each hidden layer\n        if (l < layers - 1) {\n            var dimensions = input.dimensions();\n            var m = dimensions.rows;\n            input = Matrix.Ones(m, 1).augment(input);\n        }\n\n        // Push the propagated values into the output if specified\n        if (includePropagation) {\n            output.push({ 'z': weightedInputLayerL, 'a': input });\n        }\n    }\n\n    if (includePropagation) {\n        return output;\n    }\n    else {\n        return input;\n    }\n};\n\n/**\n * Performs the standard neural network implementation of gradient descent using \n * back propagation using the given alpha learning rate for the number of iterations. This is \n * done by computing the gradient via backpropagation and applying the difference between \n * the old weights and the partial derivative gradient times the learning rate alpha.\n *\n * W = W - alpha * partial-derivative-gradient[via back-propagation]\n *\n * @param {NDMatrix} ThetaLD: the NxL dimensional matrix of parameters for each layer.\n * @param {Matrix} X: m x (n+1) matrix of m-samples of n-features.\n * @param {Vector} y: m x 1-dimensional vector of output values.\n * @param {Number} alpha: alpha learning rate used to optimized the weights\n * @param {Number} iterations: the number of iterations to run gradient descent\n * @param {Vector} lambda: n+1-dimensional vector of regularization parameter.\n */\nNeuralNetwork.prototype.gradientDescent = function (ThetaLD, X, y, alpha, iterations, lambda) {\n    var history = [];\n    var m = X.rows();\n\n    for (var i = 0; i < iterations; i++) {\n        var thetaLDResult = this.backpropagate(ThetaLD, X, y, lambda);\n        for (var l = 0; l < thetaLDResult.length; l++) {\n            thetaLDResult[l] = ThetaLD[l].add(thetaLDResult[l].multiply(-1 * alpha));\n        }\n        ThetaLD = thetaLDResult;\n        history.push(this.computeCost(ThetaLD, X, y, lambda));\n    }\n\n    var error = this.computeCost(ThetaLD, X, y);\n    return { 'theta': ThetaLD, 'cost': history, 'error': error };\n};\n\n/**\n * Standard implementation of the backpropagation algorithm by using \n * the given set of weights, input and output - this function will perform \n * the feed forward and back propagation of the given weights in order to \n * propagate the aggregated error of the network. Moreover, this will need \n * to perform feed forward, followed by:\n *\n * d3 = delta of proposed predicted values in the output layer - actual output.\n * theta2_gradient = theta in the final layer * d3;\n *\n * sum of weighted delta error (of the given hidden layer) * sigmoid gradient of \n *  the weighted input mapping from the current hidden layer - 1 (or input layer if \n *  there is only 1 hidden layer).\n * \n * Finally, this process leads us to the backpropagation of each layer's theta gradient\n * which can be used to parameterize for gradient descent.\n *\n * @param {NDMatrix} ThetaLD: NxL dimensional matrix of parameters used to represent big-Θ.\n * @param {Matrix} X: m x (n+1) matrix of m-samples of n-features.\n * @param {Vector} y: m x 1-dimensional vector of output values.\n * @param {Vector} lambda: n+1-dimensional vector of regularization parameter.\n */\nNeuralNetwork.prototype.backpropagate = function (ThetaLD, X, y, lambda) {\n    var output = this.feedForward(ThetaLD, X, true);\n    var layers = ThetaLD.length;\n    var ThetaGrad = [];\n    for (var l = 0; l < layers; l++) {\n        var thetaLDimensions = ThetaLD[l].dimensions();\n        ThetaGrad.push(Matrix.Zeros(thetaLDimensions.rows, thetaLDimensions.cols));\n    }\n\n    var prediction = output[output.length - 1].a;\n    var deltaOutput = prediction.subtract(this.mappedOutputMatrix);\n    for (var l = layers - 1; l >= 0; l--) {\n        var thetaL = ThetaLD[l];\n        var thetaGrad = ThetaGrad[l];\n        var activationMinus1 = l == 0 ? X : output[l-1].a;\n\n        // gradient function in the next few lines\n        var m = deltaOutput.rows();\n        for (var i = 1; i <= m; i++) {\n            var activeDelta = $M(deltaOutput.row(i)).multiply($M(activationMinus1.row(i)).transpose());\n            thetaGrad = thetaGrad.add(activeDelta);\n        }\n        //var activatedDelta = deltaOutput.transpose().multiply(activationMinus1);\n        //thetaGrad = thetaGrad.add(activatedDelta);\n        ThetaGrad[l] = thetaGrad;\n\n        // Calculate the propagated weight using the previous delta output\n        var weightedErrorL = deltaOutput.multiply(thetaL);\n\n        // Using the current layer - 1 weighted input values, \n        // we can calculate the distributed error weight using the sigmoid \n        if (l == 0)\n            break;\n\n        var weightedInputMinus1 = output[l-1].z;\n        var augmentWeightedInputMinus1 = Matrix.Ones(weightedInputMinus1.rows(), 1).augment(weightedInputMinus1.sigmoidGradient());\n        deltaOutput = weightedErrorL.elementMultiply(augmentWeightedInputMinus1);\n        deltaOutput = deltaOutput.slice(1, deltaOutput.rows(), 2, deltaOutput.cols());\n    }\n\n    var m = X.rows();\n    for (var i = 0; i < ThetaGrad.length; i++) {\n        var grad = ThetaGrad[i];\n        grad = grad.div(m);\n        var dim = grad.dimensions();\n        var gradAugment = Matrix.Zeros(dim.rows, 1).augment(grad.slice(1, dim.rows, 2, dim.cols));\n        var adjust = lambda / m;\n        grad = grad.add(gradAugment.multiply(adjust));\n        ThetaGrad[i] = grad;\n    }\n\n    return ThetaGrad;\n};\n\n/**\n * Predicts the value probability output of the given input feature vector \n * against the parameterized theta values calculated for each of the layers in the \n * neural network.\n *\n * @param {NDMatrix} ThetaLD: NxL dimensional matrix of parameters used to represent big-Θ.\n * @param {Vector} input: n+1-dimensional vector of input features to calculate the output.\n */\nNeuralNetwork.prototype.calculate = function (ThetaLD, input) {\n    // Determine if the input is a matrix, an array or a vector\n    if (input.length > 0) {\n        return this.feedForward(ThetaLD, $M(input).transpose());\n    }\n    else if (input.elements) {\n        if (input.elements[0][0]) {\n            return this.feedForward(ThetaLD, input);\n        }\n        else {\n            return this.feedForward(ThetaLD, $M(input.elements).transpose());\n        }\n    }\n    else {\n        throw new Error('invalid input parameter, must provide element array, matrix or vector');\n        return;\n    }\n};\n\n/**\n * Returns the set of labels and association probability for a given observation input.\n */\nNeuralNetwork.prototype.getClassifications = function (input) {\n    if (!this.trainedTheta) {\n        throw new Error('must train the classifier first');\n        return;\n    }\n\n    var result = this.calculate(this.trainedTheta, input);\n    var labels = [];\n    for (var k = 0; k < this.labels.length; k++) {\n        labels.push({ label: this.labels[k], value: result.e(1, k+1) });\n    }\n\n    return labels.sort(function (x, y) { return y.value > x.value });\n};\n\nmodule.exports = NeuralNetwork;\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/normalizers/simple_normalizer.js":"\nvar Matrix = require('./../math/matrix');\nvar Vector = require('./../math/vector');\n\nvar SimpleNormalizer = function (options) {\n    this.normalizeFeature0 = true;\n};\n\n/**\n * Normalizes the feature 0 (typically used in bias or to handle additional theta-0 weight).\n */\nSimpleNormalizer.prototype.normalize = function (observations) {\n    var features = $M(observations);\n    var dim = features.dimensions();\n    var m = dim.rows;\n\n    if (this.normalizeFeature0) {\n        return Matrix.Ones(m, 1).augment(features);\n    }\n    \n    return features;\n};\n\n/**\n * Normalizes the given input list according to the already trained\n * and normalized data using a simple feature 0 addition.\n *\n * @param [Array] input: Input features of n-dimensions\n */\nSimpleNormalizer.prototype.normalizeInput = function (observation) {\n    var inputNorm = [];\n    if (this.normalizeFeature0) {\n        inputNorm.push(1);\n    }\n\n    for (var i = 1; i <= observation.length; i++) {\n        inputNorm.push(observation[i-1]);\n    }\n    \n    return $V(inputNorm);\n};\n\nmodule.exports = SimpleNormalizer;\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/crossvalidation/crossvalidate.js":"\nvar CrossValidate = function (analyzerPrototype, analyzerOptions) {\n    this.analyzerPrototype = analyzerPrototype;\n    this.analyzerOptions = analyzerOptions;\n};\n\n/**\n * Obtains the analyzer using the configured prototype.\n *\n * @param {Number} lamba: Configured lamba value for the analyzer to regularize with.\n * @returns A new instance of the configured classifier with preconfigured options.\n */\nCrossValidate.prototype.getAnalyzer = function (lambda) {\n    var options = this.analyzerOptions || {};\n    options.lambda = lambda;\n    var analyzer = new this.analyzerPrototype(options);\n    return analyzer;\n};\n\nCrossValidate.prototype.computeError = function (analyzer, inputValidationSamples, outputValidation) {\n    var result = analyzer.train();\n    var trainError = [0];\n    if (result.length > 0) {\n        for (var e = 0; e < result.length; e++) {\n            trainError[e] = result[e].error;\n        }\n    }\n    else {\n        trainError[0] = result.error;\n    }\n\n    var validationError = analyzer.test(inputValidationSamples, outputValidation);\n    return { 'trainError': trainError, 'validationError': validationError };\n};\n\n/**\n * Computes the learning curve by training the analyzer on a subset of the training examples to \n * determine how well the learning algorithm is working as more data is added. The cross validation \n * curve is computed in order to determine how well the learned parameters upon each iteration does \n * on the cross validation data. All of which is computed based on a given regularlization parameter \n * which can be determined by plotting the validation curve below for various regularization values.\n *\n * @param {Matrix|Array} inputSamples: The training samples to train the given classifier.\n * @param {Matrix|Array} output: The expected output values for each training sample.\n * @param {Matrix|Array} inputValidationSamples: The cross-validation samples to test the classifier against.\n * @param {Matrix|Array} outputValidation: The cross-validation expected output values for each cross-validation sample.\n *\n * @returns The learning curve used to plot whether the learning algorithm suffers from high bias or high variance.\n */\nCrossValidate.prototype.learningCurve = function (inputSamples, output, inputValidationSamples, outputValidation, lambda) {\n    var trainingErrorCurve = [];\n    var validationErrorCurve = [];\n    var m = 0;\n    var cols = 1;\n    if (inputSamples.elements) {\n        m = inputSamples.rows();\n        cols = inputSamples.cols();\n    }\n    else {\n        m = inputSamples.length;\n    }\n\n    for (var i = 1; i <= m; i++) {\n        var analyzer = this.getAnalyzer(lambda);\n        if (!inputSamples.elements)\n            analyzer.addExamples(inputSamples.slice(0, i), output.slice(0, i));\n        else\n            analyzer.addExamples(inputSamples.slice(1, i, 1, cols), output.slice(1, i, 1, 1));\n\n        var result = this.computeError(analyzer, inputValidationSamples, outputValidation);\n        trainingErrorCurve.push(result.trainError);\n        validationErrorCurve.push(result.validationError);\n    }\n\n    return { 'trainingError': trainingErrorCurve, 'validationError': validationErrorCurve, 'samples': m };\n};\n\n/**\n * Calculates the cross-validation error curve in order to calculate the best fit \n * regularization parameter provided from a list of lambda values. This is done by \n * first training on the given training set for each lambda and using the trained theta \n * without regularization tested on the given validation samples.\n *\n * @param {Number[]} lambdaVals: An array of regularization parameters to train against.\n * @param {Matrix|Array} inputSamples: The training samples to train the given classifier.\n * @param {Matrix|Array} output: The expected output values for each training sample.\n * @param {Matrix|Array} inputValidationSamples: The cross-validation samples to test the classifier against.\n * @param {Matrix|Array} outputValidation: The cross-validation expected output values for each cross-validation sample.\n *\n * @returns The cross validation curve used to find the best fit regularization parameter.\n */\nCrossValidate.prototype.validationCurve = function (lambdaVals, inputSamples, output, inputValidationSamples, outputValidation) {\n    var trainingErrorCurve = [];\n    var validationErrorCurve = [];\n    for (var i = 0; i < lambdaVals.length; i++) {\n        var analyzer = this.getAnalyzer(lambdaVals[i]);\n        analyzer.addExamples(inputSamples, output);\n\n        var result = this.computeError(analyzer, inputValidationSamples, outputValidation);\n        trainingErrorCurve.push(result.trainError);\n        validationErrorCurve.push(result.validationError);\n    }\n\n    return { 'trainingError': trainingErrorCurve, 'validationError': validationErrorCurve, 'lambda': lambdaVals };\n};\n\nmodule.exports = CrossValidate;\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/wiktionary/wikparser.js":"\nvar fs = require('fs'),\n    path = require('path'),\n    XmlStream = require('./xml-stream'),\n    util = require('util'),\n    events = require('events');\n\nfunction WiktionaryParser(file, options) {\n    events.EventEmitter.call(this);\n    this.options = options || {};\n    this.file = file;\n    if (!file) {\n        throw Error(\"Must provide a valid wiktionary corpus to derive a vocabulary from\");\n    }\n\n    // Expand the home directory appropriately if necessary\n    if (this.file.indexOf('~') == 0) {\n        var p = process.env['HOME'];\n        this.file = path.join(p, this.file.substring(1));\n    }\n    var name = path.basename(this.file);\n    this.lang = name.substring(0, 2);\n    this.languageMapping = { 'en': 'english' };\n    this.language = this.languageMapping[this.lang];\n    this.mapFile = path.join(__dirname, this.lang + '.wik.map');\n    this.verbose = this.options.verbose || false;\n    this.output = this.options.output || path.join(__dirname, util.format('%s.wik.vocab', this.lang));\n    this.outputDist = this.options.outputDist || path.join(__dirname, util.format('%s.wik.dist', this.lang));\n    this.writtenTo = 0;\n    this.distribution = {};\n    this.states =\n    {\n        'NOUN': { c: 0, i: 0, s: 'NOUN' },\n        'VERB': { c: 0, i: 1, s: 'VERB' },\n        'ADJ': { c: 0, i: 2, s: 'ADJ' },\n        'ADV': { c: 0, i: 3, s: 'ADV' },\n        'X': { c: 0, i: 4, s: 'X' },\n        'NUM': { c: 0, i: 5, s: 'NUM' },\n        'PRON': { c: 0, i: 6, s: 'PRON' },\n        'CONJ': { c: 0, i: 7, s: 'CONJ' },\n        'DET': { c: 0, i: 8, s: 'DET' },\n        'PRT': { c: 0, i: 9, s: 'PRT' },\n        'ADP': { c: 0, i: 10, s: 'ADP' },\n        '.': { c: 0, i: 11, s: '.' },\n        '*': { c: 0, i: 12, s: '*' }\n    };\n};\n\nutil.inherits(WiktionaryParser, events.EventEmitter);\n\nWiktionaryParser.prototype.buildMap = function () {\n    if (this.mapping) {\n        return this.mapping;\n    }\n\n    this.mapping = {};\n    if (fs.existsSync(this.mapFile)) {\n        var lines = fs.readFileSync(this.mapFile).toString().split('\\n');\n        for (var i = 0; i < lines.length; i++) {\n            var lineItems = lines[i].split('->');\n            if (lineItems.length != 2) {\n                continue;\n            }\n            var pattern = lineItems[0].trim();\n            var mapTo = lineItems[1].trim();\n            if (typeof this.mapping[pattern] == 'undefined') {\n                this.mapping[pattern] = [mapTo];\n            }\n            else {\n                var existingMap = this.mapping[pattern];\n                existingMap.push(mapTo);\n                this.mapping[pattern] = existingMap;\n            }\n        }\n    }\n    else {\n        this.printHeadings = true;\n    }\n\n    return this.mapping;\n};\n\nWiktionaryParser.prototype.parse = function () {\n    var self = this;\n    var mappings = this.buildMap();\n    var stream = fs.createReadStream(this.file);\n    var vocab = {};\n    self.items = 0;\n    self.vocabCount = 0;\n    var xml = new XmlStream(stream);\n    var pattern = /^=+\\s*\\{*([^=\\[\\]\\{\\}]*)\\}*\\s*=+/;\n    var categoryPattern = /^\\[+Category:.*\\]+/;\n    var wait = 0;\n    var language = this.languageMapping[this.lang];\n    xml.on('updateElement: page', function (item) {\n        if (item.ns == \"0\") { // main namespace\n            var title = item.title; // word token\n            var content = item.revision.text['$text'];\n            var id = item.id;\n            var headers = [];\n            var lines = content.split('\\n');\n            var breakSegment = false;\n            var foundLanguage = false || !this.language;\n            for (var i = 0; i < lines.length; i++) {\n                if (breakSegment) {\n                    break;\n                }\n                var line = lines[i];\n                var match = line.match(pattern);\n                if (match && match.length > 1) {\n                    var currentState = match[1].toLowerCase().trim();\n                    var substates = currentState.split(':');\n                    if (substates.length > 1) {\n                        currentState = substates[0];\n                    }\n                    substates = currentState.split('・');\n                    if (substates.length > 1) {\n                        currentState = substates[0];\n                    }\n                    substates = currentState.split('|');\n                    if (substates.length > 1) {\n                        if (substates[0] == self.lang) {\n                            currentState = substates[1];\n                        }\n                        else {\n                            currentState = substates[0];\n                        }\n                    }\n                    if (currentState == self.language) {\n                        foundLanguage = true;\n                    }\n                    if (!self.printHeadings && foundLanguage && typeof mappings[currentState] != 'undefined') {\n                        var stateMap = mappings[currentState];\n                        for (var s = 0; s < stateMap.length; s++) {\n                            var state = stateMap[s];\n                            if (headers.indexOf(self.states[state]) < 0)\n                                headers.push(self.states[state]);\n                        }\n                    }\n                    else if (self.printHeadings) {\n                        if (!self.mapping.hasOwnProperty(currentState)) {\n                            self.mapping[currentState] = 0;\n                        }\n\n                        self.mapping[currentState]++;\n                    }\n                }\n                else {\n                    match = line.match(categoryPattern);\n                    if (match && foundLanguage) { // break out of page\n                        breakSegment = true;\n                    }\n                }\n                line = null;\n            }\n            if (foundLanguage && headers.length > 0) {\n                var h = [];\n                for (var hIndex = 0; hIndex < headers.length; hIndex++) {\n                    h.push(headers[hIndex].i);\n                    self.states[headers[hIndex].s].c++;\n                }\n                var v = { i: self.vocabCount++, w: title, h: h };\n                vocab[v.i] = v;\n                self.updateDist(v);\n                self.emit('vocab', v, content);\n            }\n            else if (!foundLanguage) {\n                console.log('\\t', title);\n            }\n            content = null;\n            title = null;\n            item = null;\n            self.items++;\n            process.stdout.clearLine();\n            process.stdout.cursorTo(0);\n            process.stdout.write('Processed ' + self.items + ' total pages, found ' + self.vocabCount + ' entries.');\n            if (self.items % 2000 == 0) {\n                var dist = self.dist(self.states, vocab);\n                if (self.verbose) {\n                    self.printStates(dist);\n                }\n                vocab = {};\n            }\n        }\n    });\n    xml.on('end', function () {\n        process.stdout.clearLine();\n        process.stdout.cursorTo(0);\n        var dist = self.dist(self.states, vocab);\n        if (self.verbose) {\n            self.printStates(dist);\n        }\n        if (self.printHeadings) {\n            console.log(self.mapping);\n        }\n    });\n};\n\nWiktionaryParser.prototype.updateDist = function (v) {\n    var self = this;\n    var dist = self.distribution;\n    if (!dist.uniState) {\n        dist.uniState = 0;\n        dist.biState = 0;\n        dist.triState = 0;\n        dist.quadState = 0;\n        dist.plusState = 0;\n    }\n\n    dist.vocabCount = self.vocabCount;\n\n    if (v.h.length == 1) {\n        dist.uniState++;\n    }\n    else if (v.h.length == 2) {\n        dist.biState++;\n    }\n    else if (v.h.length == 3) {\n        dist.triState++;\n    }\n    else if (v.h.length == 4) {\n        dist.quadState++;\n    }\n    else if (v.h.length > 4) {\n        dist.plusState++;\n    }\n\n    dist.uniStatePercent = Math.round(100 * (dist.uniState / self.vocabCount));\n    dist.biStatePercent = Math.round(100 * (dist.biState / self.vocabCount));\n    dist.triStatePercent = Math.round(100 * (dist.triState / self.vocabCount));\n    dist.quadStatePercent = Math.round(100 * (dist.quadState / self.vocabCount));\n    dist.plusStatePercent = Math.round(100 * (dist.plusState / self.vocabCount));\n};\n\nWiktionaryParser.prototype.dist = function (states, vocab) {\n    var self = this;\n\n    var i = self.writtenTo;\n    for (; i < self.vocabCount; i++) {\n        if (typeof vocab[i] != 'undefined') {\n            var v = vocab[i];\n            var outputLine = util.format('%s\\t%s\\t%s\\n', i, v.w, v.h.join(','));\n            fs.appendFileSync(self.output, outputLine);\n            self.writtenTo++;\n        }\n    }\n\n    var sortedStates = [];\n    for (var s in states) {\n        var item = states[s];\n        item.s = s;\n        item.p = Math.round(100 * (item.c / self.vocabCount));\n        sortedStates.push(item);\n    }\n    sortedStates = sortedStates.sort(function (a, b) {\n        return b.c - a.c;\n    });\n\n    self.distribution.sortedStates = sortedStates;\n\n    if (self.outputDist) {\n        var output = \"\";\n        output += util.format('Total Vocabulary: %s\\n', self.distribution.vocabCount);\n        output += util.format('Unambiguous: %s %s%\\n', self.distribution.uniState, self.distribution.uniStatePercent);\n        output += util.format('Dual: %s %s%\\n', self.distribution.biState, self.distribution.biStatePercent);\n        output += util.format('Tri: %s %s%\\n', self.distribution.triState, self.distribution.triStatePercent);\n        output += util.format('Quad: %s %s%\\n', self.distribution.quadState, self.distribution.quadStatePercent);\n        output += util.format('+4: %s %s%\\n\\n', self.distribution.plusState, self.distribution.plusStatePercent);\n        output += \"# The following displays the order of frequency and distribution\\n\";\n        output += \"# for all the mapped states for the given language {\" + self.lang + \"}\\n\\n\";\n        for (var i = 0; i < sortedStates.length; i++) {\n            var state = sortedStates[i];\n            output += util.format('%s\\t%s\\t%s\\t%s%\\n', state.i, state.s, state.c, state.p);\n        }\n        output += util.format(\"%s\\t%s\\t%s\\t%s\\n\", 11, \".\", \"X\", \"X%\");\n        fs.writeFileSync(self.outputDist, output);\n    }\n\n\n    return self.distribution;\n};\n\nWiktionaryParser.prototype.printStates = function (dist) {\n    console.log(distribution.sortedStates);\n    var format = 'Total: %s, 1: %s (%s%), 2: %s (%s%), 3: %s (%s%), 4: %s (%s%), +4: %s (%s%)';\n    var log = util.format(format, dist.vocabCount,\n            dist.uniState, dist.uniStatePercent,\n            dist.biState, dist.biStatePercent,\n            dist.triState, dist.triStatePercent,\n            dist.quadState, dist.quadStatePercent,\n            dist.plusState, dist.plusStatePercent);\n    console.log(log);\n};\n\nmodule.exports = WiktionaryParser;\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/wiktionary/xml-stream.js":"var events         = require('events')\n  , expat          = require('node-expat')\n  , FiniteAutomata = require('./finite-automata')\n  , Iconv          = require('iconv').Iconv\n  ;\n\n// Retains link to hasOwnProperty.\nvar __own = Object.prototype.hasOwnProperty;\n\n// Tests if object is empty (has no own properties).\nfunction isEmpty(obj) {\n  for (var key in obj) if (__own.call(obj, key)) {\n    return false;\n  }\n  return true;\n}\n\n// XML entities.\nvar entities = {\n  '\"': '&quot;',\n  '&': '&amp;',\n  '\\'': '&apos;',\n  '<': '&lt;',\n  '>': '&gt;'\n};\n\n// Escapes text for XML.\nfunction escape(value) {\n  return value.replace(/\"|&|'|<|>/g, function(entity) {\n    return entities[entity];\n  });\n}\n\n// Parser events to finite automata events mapping.\nvar faModes = {\n  'startElement': 'enter',\n  'endElement': 'leave',\n  'text': 'state'\n};\n\n// I accidentally the whole class.\nmodule.exports = XmlStream;\n\n// **XmlStream** is an XML stream filter based on Expat.\n// It traverses a given stream and emits events for predefined selectors.\n// Event listeners receive selected elements, context, and trace from root.\nfunction XmlStream(stream, encoding) {\n  events.EventEmitter.call(this);\n  this._stream = stream;\n  this._fa = new FiniteAutomata();\n  this._lastState = 0;\n  this._startState = {};\n  this._finalStates = {};\n  this._emitData = false;\n  this._bufferLevel = 0;\n  this._preserveLevel = 0;\n  this._preserveWhitespace = 0;\n  this._collect = false;\n  this._parser = undefined;\n\n  // Set input stream encoding and create an iconv instance,\n  // if conversion is required. Default working encoding is UTF-8,\n  // so iconv is used when input is anything else, but UTF-8.\n  this._encoding = encoding || null;\n  this._encoder = makeEncoder(this._encoding);\n\n  // Start parsing.\n  parse.call(this);\n}\n\n// Either make an iconv instance, or not.\nfunction makeEncoder(encoding) {\n  if (encoding && !/^utf-?8$/i.test(encoding)) {\n    return new Iconv(encoding, 'utf8');\n  }\n  return null;\n}\n\n// Inherit events.EventEmitter.\nXmlStream.super_ = events.EventEmitter;\nXmlStream.prototype = Object.create(events.EventEmitter.prototype, {\n  constructor: {\n    value: XmlStream,\n    enumerable: false\n  }\n});\n\n// Adds a listener for the specified event.\n//\n// Supported events:\n//\n// * `data` on outgoing data chunk,\n// * `end` when parsing has ended,\n// * `startElement[: selector]` on opening tag for selector match,\n// * `updateElement[: selector]` on finished node for selector match\n//   with its contents buffered,\n// * `endElement[: selector]` on closing tag for selector match,\n// * `text[: selector]` on tag text for selector match.\n//\n// When adding listeners for `startElement`, `updateElement`, and `text` the\n// callback can modify the provided node, before it is sent to the consumer.\n//\n// Selector syntax is CSS-like and currently supports:\n//\n// * `ancestor descendant`\n// * `parent > child`\nXmlStream.prototype.on = function(eventName, listener) {\n  var event = parseEvent(eventName);\n  if (event !== null) {\n    // If we're dealing with a selector event,\n    // continue with selector-specific processing logic.\n    XmlStream.super_.prototype.on.call(this, event.name, listener);\n    var finalState = getFinalState.call(this, event.selector);\n    var self = this;\n    if (event.type === 'updateElement') {\n      this._fa.on('enter', finalState, function() {\n        self._bufferLevel++;\n      });\n      this._fa.on('leave', finalState, function(element, context, trace) {\n        self.emit(event.name, element, context, trace);\n        if (!--self._bufferLevel && self._emitData) {\n           emitElement.call(self, element, self._name, true);\n        }\n      });\n    } else {\n      var fn = function(element, context, trace) {\n        self.emit(event.name, element, context, trace);\n      };\n      this._fa.on(faModes[event.type], finalState, fn);\n    }\n  } else {\n    // Otherwise, we're dealing with a non-selector event.\n    if (eventName === 'data') {\n      this._emitData = true;\n    }\n    XmlStream.super_.prototype.on.call(this, eventName, listener);\n  }\n};\n\n// Collects elements with identical names, specified by a selector.\n// They will reside in the parent element as an array.\nXmlStream.prototype.collect = function(selector) {\n  selector = normalizeSelector(selector);\n  var finalState = getFinalState.call(this, selector);\n  var self = this;\n  this._fa.on('flag', finalState, function() {\n    self._collect = true;\n  });\n};\n\n// Preserves the order of element and text nodes inside elements\n// that match the selector. Optionally, preserves whitespace.\nXmlStream.prototype.preserve = function(selector, whitespace) {\n  selector = normalizeSelector(selector);\n  var finalState = getFinalState.call(this, selector);\n  var self = this;\n  this._fa.on('enter', finalState, function() {\n    self._preserveLevel++;\n    if (whitespace) {\n      self._preserveWhitespace++;\n    }\n  });\n  this._fa.on('leave', finalState, function() {\n    self._preserveLevel--;\n    if (whitespace) {\n      self._preserveWhitespace--;\n    }\n  });\n};\n\n// pause expat\nXmlStream.prototype.pause = function() {\n  this._stream.pause();\n  this._suspended = true;\n  if( !this._parser.pause() ) {\n      throw(new Error(\"Cannot pause parser: \"+this._parser.getError()));\n  }\n}\n\nXmlStream.prototype.close = function() {\n    this.pause();\n    this._stream.close();\n    this._parser.stop();\n    this.emit('end');\n};\n\n// resume expat\nXmlStream.prototype.resume = function() {\n  this._suspended = false;\n\n  if( !this._parser.resume() ) {\n    throw(new Error(\"Cannot resume parser: \"+this._parser.getError()));\n  }\n\n  // resume stream only if parser hasn't been paused again\n  if( !this._suspended ) {\n    this._stream.resume();\n  }\n}\n\n// Normalizes the selector and returns the new version and its parts.\nfunction normalizeSelector(selector) {\n  var parts = selector.match(/[^\\s>]+|>/ig);\n  selector = (parts) ? parts.join(' ') : '';\n  return {\n    normalized: selector,\n    parts: parts || []\n  };\n}\n\n// Parses the selector event string and returns event information.\nfunction parseEvent(event) {\n  var eventParts = event.match(/^((?:start|end|update)Element|text):?(.*)/);\n  if (eventParts === null) {\n    return null;\n  }\n  var eventType = eventParts[1];\n  var selector = normalizeSelector(eventParts[2]);\n  return {\n    selector: selector,\n    type: eventType,\n    name: (eventParts[2]) ? eventType + ': ' + selector.normalized\n                          : eventType\n  };\n}\n\n// Compiles a given selector object to a finite automata\n// and returns its last state.\nfunction getFinalState(selector) {\n  if (__own.call(this._finalStates, selector.normalized)) {\n    var finalState = this._finalStates[selector.normalized];\n  } else {\n    var n = selector.parts.length;\n    var immediate = false;\n    this._startState[this._lastState] = true;\n    for (var i = 0; i < n; i++) {\n      var part = selector.parts[i];\n      if (part === '>') {\n        immediate = true;\n      } else {\n        if (!immediate) {\n          this._fa.transition(this._lastState, '', this._lastState);\n        }\n        this._fa.transition(this._lastState, part, ++this._lastState);\n        immediate = false;\n      }\n    }\n    var finalState = this._lastState++;\n    this._finalStates[selector.normalized] = finalState;\n  }\n  return finalState;\n}\n\n// Emits XML for element opening tag.\nfunction emitStart(name, attrs) {\n  this.emit('data', '<' + name);\n  for (var attr in attrs) if (__own.call(attrs, attr)) {\n    this.emit('data', ' ' + attr + '=\"' + escape(attrs[attr]) + '\"');\n  }\n  this.emit('data', '>');\n}\n\n// Emits XML for element closing tag.\nfunction emitEnd(name) {\n  this.emit('data', '</' + name + '>');\n}\n\n// Emits XML for element text.\nfunction emitText(text) {\n  this.emit('data', escape(text));\n}\n\n// Emits a single element and its descendants, or an array of elements.\nfunction emitElement(element, name, onLeave) {\n  if (Array.isArray(element)) {\n    var i;\n    for (i = 0; i < element.length - 1; i++) {\n      emitOneElement.call(this, element[i], name);\n    }\n    emitOneElement.call(this, element[i], name, onLeave);\n  } else {\n    emitOneElement.call(this, element, name, onLeave);\n  }\n}\n\n// Emits child element collection and their descendants.\n// Works only with preserved nodes.\nfunction emitChildren(elements) {\n  var i;\n  for (i = 0; i < elements.length; i++) {\n    var element = elements[i];\n    if (typeof element === 'object') {\n      emitStart.call(this, element.$name, element.$);\n      emitChildren.call(this, element.$children);\n      emitEnd.call(this, element.$name);\n    } else {\n      emitText.call(this, element);\n    }\n  }\n}\n\n// Recursively emits a given element and its descendants.\nfunction emitOneElement(element, name, onLeave) {\n  if (typeof element === 'object') {\n    emitStart.call(this, name, element.$);\n    if (__own.call(element, '$children')) {\n      emitChildren.call(this, element.$children);\n    } else {\n      var hasText = false;\n      for (var child in element) {\n        if (__own.call(element, child) && child !== '$' && child != '$name') {\n          if (child === '$text') {\n            hasText = true;\n          } else {\n            emitElement.call(this, element[child], child);\n          }\n        }\n      }\n      if (hasText) {\n        emitText.call(this, element.$text);\n      }\n    }\n  } else {\n    emitStart.call(this, name, element.$);\n    emitText.call(this, element);\n  }\n  if (!onLeave) {\n    emitEnd.call(this, name);\n  }\n}\n\n// Starts parsing the source stream and emitting various events.\n// The Expat parser is assigned several listeners for this purpose.\nfunction parse() {\n  var self = this;\n  var xml = new expat.Parser('utf-8');\n  this._parser = xml;\n  this._suspended = false;\n  var stack = [];\n  var trace = {};\n  var curr = {\n    element: {},\n    collect: this._collect,\n    fullText: '',\n    space: 0,\n    path: '',\n    context: {}\n  };\n  var fa = this._fa;\n  fa.setState(this._startState);\n\n  // A listener is assigned on opening tag encounter.\n  // Here we traverse the configured finite automata use the stack\n  // to form the context and trace for selector event emission.\n  xml.on('startElement', function(name, attr) {\n    self.emit('startElement', name, attr);\n    stack.push(curr);\n    trace[curr.path] = curr.element;\n    var context = Object.create(curr.context);\n    var element = {\n      $: attr,\n      $name: name,\n      $text: ''\n    };\n    var parent = curr.element;\n    curr = {\n      element: element,\n      collect: false,\n      fullText: '',\n      space: 0,\n      path: curr.path + '/' + name,\n      context: context\n    };\n    self._collect = false;\n    fa.enter(name, [element, context, trace]);\n    if (self._preserveLevel > 0) {\n      element.$children = [];\n    }\n    name = element.$name;\n    curr.collect = self._collect;\n    if (curr.collect) {\n      var container;\n      if (__own.call(parent, name)) {\n        container = parent[name];\n        container.push(element);\n      } else {\n        container = [element];\n        parent[name] = container;\n      }\n    } else {\n      parent[name] = element;\n      context[name] = element;\n    }\n    if (self._bufferLevel === 0 && self._emitData) {\n      emitStart.call(self, name, element.$);\n    }\n  });\n\n  // A listener is assigned on closing tag encounter.\n  // Current node structure object is finalized. A selector listener is\n  // invoked with current node, context, and trace; these arguments are\n  // removed from the stack afterwards.\n  xml.on('endElement', function(name) {\n    self.emit('endElement', name);\n    var prev = stack.pop();\n    var element = curr.element;\n    var text = curr.fullText;\n    var attr = element.$;\n    if (typeof attr !== 'object') {\n      attr = {};\n    }\n    var name = element.$name;\n    self._name = name;\n    delete element.$;\n    delete element.$text;\n    delete element.$name;\n    var val = element;\n    if (isEmpty(element) && isEmpty(attr)) {\n      val = text;\n    } else if (!isEmpty(attr)) {\n      element.$ = attr;\n    }\n    if (text !== '') {\n      element.$text = text;\n    }\n    if (self._bufferLevel > 0 || self._preserveLevel > 0) {\n      element.$name = name;\n    }\n    curr.context[name] = val;\n    if (curr.collect) {\n      var container = prev.element[name];\n      container[container.length - 1] = val;\n    } else {\n      prev.element[name] = val;\n    }\n    fa.leave([element, curr.context, trace]);\n    if (self._preserveLevel > 0) {\n      prev.element.$children.push(val);\n    }\n    if (self._bufferLevel === 0 && self._emitData) {\n      emitEnd.call(self, name);\n    }\n    curr = prev;\n    this._collect = curr.collect;\n  });\n\n  // Collect node text part by part\n  // (and trim leading and trailing whitespace).\n  xml.on('text', function(text) {\n    curr.element.$text = text;\n    fa.run('state', [curr.element, curr.context, trace]);\n    if (self._bufferLevel === 0 && self._emitData) {\n      emitText.call(self, text);\n    }\n    curr.fullText += text;\n    /*\n    var spaced = curr.element.$text.substr(0, 1);\n    spaced = (spaced !== '') && (spaced.trim() === '');\n    var after = curr.element.$text.substr(-1, 1);\n    after = (after !== '') && (after.trim() === '');\n    switch (curr.space) {\n      // No words yet (pass through spaces).\n      case 0:\n        if (trimmed !== '') {\n          curr.space = after ? 2 : 1;\n        }\n        break;\n\n      // Immediately after text or entity.\n      case 1:\n        if (trimmed === '') {\n          curr.space = 2;\n        } else {\n          if (spaced) {\n            curr.fullText += ' ';\n          }\n          if (after) {\n            curr.space = 2;\n          }\n        }\n        break;\n\n      // Some words were emitted, pass through spaces again.\n      // Emit spaces only when a word is encountered afterwards.\n      case 2:\n        if (trimmed !== '') {\n          curr.fullText += ' ';\n          curr.space = 1;\n        }\n        break;\n    }\n    text = self._preserveWhitespace > 0 ? text : trimmed;\n    if (self._preserveLevel > 0) {\n      if (text !== '') {\n        curr.element.$children.push(text);\n      }\n    }\n    curr.fullText += text;\n    */\n  });\n\n\n  // This prelude array and string are used during encoding detection.\n  // Incoming buffers are collected and parsing is postponed,\n  // but only until the first tag.\n  var prelude = '';\n  var preludeBuffers = [];\n\n  // Parse incoming chunk.\n  // Convert to UTF-8 or emit errors when appropriate.\n  var parseChunk = function(data) {\n    if (self._encoder) {\n      data = self._encoder.convert(data);\n    }\n    if (!xml.parse(data, false)) {\n      self.emit('error', new Error(xml.getError()+\" in line \"+xml.getCurrentLineNumber()));\n    }\n  }\n\n  // Pass data from stream to parser.\n  this._stream.on('data', function(data) {\n    if (self._encoding) {\n      parseChunk(data);\n    } else {\n      // We can't parse when the encoding is unknown, so we'll look into\n      // the XML declaration, if there is one. For this, we need to buffer\n      // incoming data until a full tag is received.\n      preludeBuffers.push(data);\n      prelude += data.toString();\n      if (/^\\s*<[^>]+>/.test(prelude)) {\n        var matches = prelude.match(/^\\s*<\\?xml[^>]+encoding=\"(.+?)\"[^>]*\\?>/);\n        self._encoding = matches ? matches[1] : 'utf8';\n        self._encoder = makeEncoder(self._encoding);\n        for (var i = 0, n = preludeBuffers.length; i < n; i++) {\n          parseChunk(preludeBuffers[i]);\n        }\n      }\n    }\n  });\n\n  // End parsing on stream EOF and emit an *end* event ourselves.\n  this._stream.on('end', function() {\n    if (!xml.parse('', true)) {\n      self.emit('error', new Error(xml.getError()+\" in line \"+xml.getCurrentLineNumber()));\n    }\n    self.emit('end');\n  });\n}\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/wiktionary/finite-automata.js":"module.exports = FiniteAutomata;\nfunction FiniteAutomata() {\n  this._symbols = {};\n  this._states = {};\n  this._deterministic = true;\n  this._state = {};\n  this._callbacks = {\n    enter: {},\n    leave: {},\n    state: {},\n    flag: {}\n  };\n  this._stack = [];\n  this._stackPtr = -1;\n}\n\nvar __own = Object.prototype.hasOwnProperty;\n\nfunction extend(target, source) {\n  for (var key in source) if (__own.call(source, key)) {\n    target[key] = source[key];\n  }\n}\n\nfunction run(type, args) {\n  var cbs = this._callbacks[type];\n  for (var cb in this._state) if (__own.call(this._state, cb)) {\n    if (__own.call(cbs, cb)) {\n      var length = cbs[cb].length;\n      var cbList = cbs[cb];\n      for (var i = 0; i < length; i++) {\n        cbList[i].apply(global, args);\n      }\n    }\n  }\n}\n\nFiniteAutomata.prototype.isDeterministic = function() {\n  return this._deterministic;\n};\n\nFiniteAutomata.prototype.on = function(type, state, cb) {\n  if (!__own.call(this._callbacks, type)) {\n    this._callbacks[type] = {};\n  }\n  var typeCbs = this._callbacks[type];\n  if (!__own.call(typeCbs, state)) {\n    typeCbs[state] = [];\n  }\n  typeCbs[state].push(cb);\n  return this;\n};\n\nFiniteAutomata.prototype.setState = function(state, args) {\n  this._state = state;\n  run.call(this, 'enter', args);\n  run.call(this, 'state', args);\n  return this;\n};\n\nFiniteAutomata.prototype.nextState = function(symbol) {\n  var newState = {};\n  for (var st in this._state) if (__own.call(this._state, st)) {\n    if (__own.call(this._states, st)) {\n      var next = this._states[st];\n      if (__own.call(next, symbol)) {\n        extend(newState, next[symbol]);\n      }\n      if (__own.call(next, '')) {\n        extend(newState, (next['']));\n      }\n    }\n  }\n  return newState;\n};\n\nFiniteAutomata.prototype.go = function(symbol, args) {\n  var next = this.nextState(symbol)\n  this.setState(next, args);\n  return this;\n};\n\nFiniteAutomata.prototype.leave = function(args) {\n  this._stack[this._stackPtr] = undefined;\n  run.call(this, 'leave', args);\n  this._state = this._stack[--this._stackPtr];\n  return this;\n};\n\nFiniteAutomata.prototype.enter = function(symbol, args) {\n  if (args == null) {\n    args = [];\n  }\n  var next = this.nextState(symbol);\n  this._stack[++this._stackPtr] = next;\n  this._state = next;\n  run.call(this, 'flag');\n  run.call(this, 'enter', args);\n  return this;\n};\n\nFiniteAutomata.prototype.run = function(state, args) {\n  run.call(this, state, args);\n};\n\nFiniteAutomata.prototype.transition = function(stateFrom, symbol, stateTo) {\n  this._symbols[symbol] = true;\n  var s;\n  if (__own.call(this._states, stateFrom)) {\n    s = this._states[stateFrom];\n  } else {\n    s = this._states[stateFrom] = {};\n  }\n  var exists = __own.call(s, symbol);\n  if (exists) {\n    s = s[symbol];\n  } else {\n    s = s[symbol] = {};\n  }\n  if (!__own.call(s, stateTo)) {\n    s[stateTo] = true;\n    this._deterministic &= !exists;\n  }\n  return this;\n};\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/corpus/brown.js":"\nvar fs = require('fs'),\n    path = require('path'),\n    util = require('util'),\n    walk = require('walk');\n\nvar Mapping = require('./mapping');\n\nfunction BrownCorpus(file, options) {\n    this.options = options || {};\n    \n    if (file.indexOf('~') == 0) {\n        file = path.join(process.env['HOME'], file.substring(1));\n    }\n\n    this.fileName = file;\n    this.output = this.options.output || path.join(__dirname, 'en-brown.tag.vocab')\n    this.outputDist = this.options.output || path.join(__dirname, 'en-brown.tag.dist')\n    this.outputSentences = this.options.outputSentences || path.join(__dirname, 'en-brown.sentences');\n    this.skipLines = this.options.skipLines || 0;\n    this.limitLines = this.options.limitLines || 0;\n    this.mapping = Mapping.load(path.join(__dirname, '/universal_tagset/en-brown.map'));\n};\n\nBrownCorpus.prototype.parse = function () {\n    var self = this;\n    var walker = walk.walk(this.fileName, { followLinks: false });\n    var files = [];\n    walker.on('file', function (root, stat, next) {\n        files.push(root + \"/\" + stat.name);\n        next();\n    });\n    walker.on('end', function () {\n        self.files = files;\n        self.loadFiles(self.files);\n    });\n};\n\nBrownCorpus.prototype.suffix = function (token, length, minLength) {\n    if (token.indexOf('\\'') >= 0) {\n        return token.substring(token.indexOf('\\''));\n    }\n    else if (token.indexOf('`') >= 0) {\n        return token.substring(token.indexOf('`'));\n    }\n\n    return token;\n};\n\nBrownCorpus.prototype.loadFiles = function (files) {\n    var mapping = this.mapping;\n    var sentences = [];\n    var tags = {};\n    var vocab = {};\n    var startTime = new Date().getTime();\n    var skipLines = 0;\n    var limitLines = 0;\n    for (var i = 0; i < files.length; i++) {\n        var file = files[i];\n        var lines = fs.readFileSync(file).toString().split('\\n');\n        for (var l = 0; l < lines.length; l++) {\n            var line = lines[l];\n            if (line && line.trim().length > 0) {\n                // Ensure that we can artificially skip a number of lines\n                if (this.skipLines > 0 && skipLines < this.skipLines) {\n                    skipLines++;\n                    continue;\n                }\n\n                // Ensure that we can artificially limit the number of \n                // lines we wish to process\n                if (this.limitLines > 0) {\n                    if (limitLines < this.limitLines) {\n                        limitLines++;\n                    }\n                    else {\n                        break;\n                    }\n                }\n\n                line = line.trim();\n                var newLine = [];\n                var items = line.split(' ');\n                var lineTags = {};\n\n                // iterate over the token/tags \n                var prevTag = \"*\";\n                var prevTag2 = \"*\";\n                for (var k = 0; k < items.length; k++) {\n                    var item = items[k];\n                    var token = item.split('/');\n                    if (token && token.length == 2) {\n                        var word = token[0].toLowerCase();\n                        var suffix = this.suffix(word);\n                        if (typeof vocab[word] == 'undefined') {\n                            vocab[word] = { f: 0, t: {} };\n                        }\n                        vocab[word].f++;\n\n                        // Update the suffix into the vocabulary if appropriate\n                        if (word != suffix) {\n                            if (typeof vocab[suffix] == 'undefined') {\n                                vocab[suffix] = { f: 0, t: {} };\n                            }\n                            vocab[suffix].f++;\n                        }\n\n\n                        var tag = token[1].toUpperCase();\n                        if (typeof mapping[tag] != 'undefined') {\n                            tag = mapping[tag];\n\n                            // Append the tag to the list of possible tags found on the given word\n                            if (typeof vocab[word].t[tag] == 'undefined') {\n                                vocab[word].t[tag] = 1;\n                            }\n                            vocab[word].t[tag]++;\n\n                            // update the tag associated with the suffix\n                            if (word != suffix) {\n                                if (typeof vocab[suffix].t[tag] == 'undefined') {\n                                    vocab[suffix].t[tag] = 1;\n                                }\n                                vocab[suffix].t[tag]++;\n                            }\n\n                            if (typeof lineTags[tag] == 'undefined') {\n                                lineTags[tag] = 1;\n                            }\n\n                            if (prevTag) {\n                                var bitag = prevTag + \"+\" + tag;\n                                if (typeof lineTags[bitag] == 'undefined') {\n                                    lineTags[bitag] = 1;\n                                }\n                                lineTags[bitag]++;\n\n                                if (prevTag2) {\n                                    var tritag = prevTag2 + \"+\" + prevTag + \"+\" + tag;\n                                    if (typeof lineTags[tritag] == 'undefined') {\n                                        lineTags[tritag] = 1; \n                                    }\n                                    lineTags[tritag]++;\n                                }\n\n                                prevTag2 = prevTag;\n                            }\n                            lineTags[tag]++;\n                            prevTag = tag;\n                        }\n                        newLine.push(token[0] + \"/\" + tag);\n                    }\n                }\n\n                var bitag = prevTag + \"+STOP\";\n                var tritag = prevTag2 + \"+\" + prevTag + \"+STOP\";\n                if (!lineTags.hasOwnProperty(\"STOP\"))\n                    lineTags[\"STOP\"] = 1;\n\n                if (!lineTags.hasOwnProperty(bitag))\n                    lineTags[bitag] = 1;\n\n                if (!lineTags.hasOwnProperty(tritag))\n                    lineTags[tritag] = 1;\n                \n                lineTags[\"STOP\"]++;\n                lineTags[bitag]++;\n                lineTags[tritag]++;\n\n                // iterate over the found tags for the given example\n                // and update the aggregate tags for the document frequency\n                for (var t in lineTags) {\n                    if (typeof tags[t] == 'undefined') {\n                        tags[t] = { f: 0, df: 0 };\n                    }\n\n                    tags[t].f += lineTags[t];\n                    tags[t].df++;\n                }\n\n                sentences.push(newLine.join(' '));\n            }\n        }\n    }\n\n    for (var t in tags) {\n        var idf = Math.log(sentences.length / (1 + tags[t].df));\n        var tf = tags[t].f;\n        tags[t].idf = idf;\n        tags[t].tfidf = Math.round(tf * idf);\n        tags[t].dist = Math.round(100 * tags[t].df / sentences.length);\n    }\n\n    var sortedTags = this.dist(tags);\n    for (var i = 0; i < sortedTags.length; i++) {\n        var t = sortedTags[i];\n        var line = util.format('%s\\t%s\\t%s%\\n', t.t, t.f, t.dist);\n        fs.appendFileSync(this.outputDist, line);\n    }\n\n    var sortedVocab = [];\n    for (var v in vocab) {\n        var tags = [];\n        for (var t in vocab[v].t) {\n            tags.push(t + \"/\" + vocab[v].t[t]);\n        }\n        var item = vocab[v];\n        item.v = v;\n        item.tags = tags;\n        sortedVocab.push(item);\n    }\n    sortedVocab = sortedVocab.sort(function (a, b) { return b.f - a.f });\n    for (var i = 0; i < sortedVocab.length; i++) {\n        var item = sortedVocab[i];\n        var line = util.format('%s\\t%s\\t%s\\n', item.v, item.f, item.tags.join(','));\n        fs.appendFileSync(this.output, line);\n    }\n\n    for (var i = 0; i < sentences.length; i++) {\n        var item = sentences[i];\n        fs.appendFileSync(this.outputSentences, item + '\\n');\n    }\n\n    var endTime = new Date().getTime();\n    console.log(util.format('Wrote %s distributions from %s sentences to %s (in %sms)', sortedTags.length, sentences.length, \n                this.output, (endTime - startTime)));\n};\n\nBrownCorpus.prototype.dist = function (tags) {\n    var d = [];\n    for (var t in tags) {\n        var item = tags[t];\n        item.t = t;\n        d.push(item);\n    }\n\n    return d.sort(function (a, b) {\n        return b.f - a.f;\n    });\n};\n\nmodule.exports = BrownCorpus;\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/corpus/mapping.js":"\nvar fs = require('fs'),\n    path = require('path'),\n    util = require('util');\n\nmodule.exports = {};\nmodule.exports.load = function (mapFile) {\n    var mapping = {};\n    var lines = fs.readFileSync(mapFile).toString().split('\\n');\n    for (var l = 0; l < lines.length; l++) {\n        var line = lines[l];\n        if (!line || line.trim().length == 0) {\n            continue;\n        }\n        var items = line.split('\\t');\n        if (items.length != 2)\n            continue;\n\n        var btag = items[0];\n        var utag = items[1];\n        mapping[btag] = utag;\n    }\n\n    return mapping;\n};\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/corpus/penn.js":"\nvar util = require('util');\nvar Corpus = require('./corpus');\n\nfunction PennTreeBank(file, options) {\n    Corpus.call(this, file, 'en-ptb', options);\n};\n\nutil.inherits(PennTreeBank, Corpus);\nmodule.exports = PennTreeBank;\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/corpus/corpus.js":"\nvar fs = require('fs'),\n    path = require('path'),\n    util = require('util'),\n    walk = require('walk');\n\nvar Mapping = require('./mapping');\n\nfunction Corpus(file, mapName, options) {\n    this.options = options || {};\n\n    if (file.indexOf('~') == 0) {\n        file = path.join(process.env['HOME'], file.substring(1));\n    }\n\n    this.fileName = file;\n    this.mapName = mapName;\n    this.mapping = Mapping.load(path.join(__dirname, '/universal_tagset/' + mapName + '.map'));\n    this.output = this.options.output || path.join(__dirname, mapName + '.tag.vocab')\n    this.outputDist = this.options.output || path.join(__dirname, mapName + '.tag.dist')\n    this.outputSentences = this.options.outputSentences || path.join(__dirname, mapName + '.sentences');\n    this.perLine = this.options.hasOwnProperty('perLine') ? this.options.perLine : false;\n};\n\nCorpus.prototype.parse = function () {\n    var self = this;\n    var walker = walk.walk(this.fileName, { followLinks: false });\n    var files = [];\n    walker.on('file', function (root, stat, next) {\n        files.push(root + \"/\" + stat.name);\n        next();\n    });\n    walker.on('end', function () {\n        self.files = files;\n        self.loadFiles(self.files);\n    });\n};\n\nCorpus.prototype.suffix = function (token, length, minLength) {\n    if (token.indexOf('\\'') >= 0) {\n        return token.substring(token.indexOf('\\''));\n    }\n    else if (token.indexOf('`') >= 0) {\n        return token.substring(token.indexOf('`'));\n    }\n\n    return token;\n};\n\nCorpus.prototype.loadFiles = function (files) {\n    var self = this;\n    var sentences = [];\n    var tags = {};\n    var vocab = {};\n    var startTime = new Date().getTime();\n    for (var i = 0; i < files.length; i++) {\n        var file = files[i];\n        var lines = fs.readFileSync(file).toString().split('\\n');\n        var state = null; \n        for (var l = 0; l < lines.length; l++) {\n            var line = lines[l];\n            var isEmpty = !line || line.trim().length == 0;\n            if (isEmpty && state && state.hasOwnProperty(\"tokens\")) {\n                state.empty = true;\n            }\n            if (line && line.trim().length > 0) {\n                if (!state || state.empty || self.perLine) {\n                    if (state && state.empty) {\n                        self.updateState(state, null, \"STOP\");\n                        self.storeState(tags, sentences, state);\n                    }\n                    state = {};\n                    state.tokens = [];\n                    state.tags = {};\n                    state.prevTag = \"*\";\n                    state.prevTag2 = \"*\";\n                }\n\n                line = line.trim();\n                var tokenTagPairs = self.parseLine(line);\n                for (var p = 0; p < tokenTagPairs.length; p++) {\n                    var tokenTagPair = tokenTagPairs[p];\n                    var word = tokenTagPair[0];\n                    var suffix = self.suffix(word);\n                    var tag = tokenTagPair[1];\n                    \n                    // Add the vocabulary word and tag increments\n                    // for both the word and the suffix (should they be different)\n                    self.addVocabularyTag(vocab, word, tag);\n                    if (word != suffix)\n                        self.addVocabularyTag(vocab, suffix, tag);\n\n                    self.updateState(state, word, tag);\n                }\n            }\n        }\n    }\n\n    for (var t in tags) {\n        var idf = Math.log(sentences.length / (1 + tags[t].df));\n        var tf = tags[t].f;\n        tags[t].idf = idf;\n        tags[t].tfidf = Math.round(tf * idf);\n        tags[t].dist = Math.round(100 * tags[t].df / sentences.length);\n    }\n\n    var sortedTags = this.dist(tags);\n    for (var i = 0; i < sortedTags.length; i++) {\n        var t = sortedTags[i];\n        var line = util.format('%s\\t%s\\t%s%\\n', t.t, t.f, t.dist);\n        fs.appendFileSync(this.outputDist, line);\n    }\n\n    var sortedVocab = [];\n    for (var v in vocab) {\n        var tags = [];\n        for (var t in vocab[v].t) {\n            tags.push(t + \"/\" + vocab[v].t[t]);\n        }\n        var item = vocab[v];\n        item.v = v;\n        item.tags = tags;\n        sortedVocab.push(item);\n    }\n    sortedVocab = sortedVocab.sort(function (a, b) { return b.f - a.f });\n    for (var i = 0; i < sortedVocab.length; i++) {\n        var item = sortedVocab[i];\n        var line = util.format('%s\\t%s\\t%s\\n', item.v, item.f, item.tags.join(','));\n        fs.appendFileSync(this.output, line);\n    }\n\n    for (var i = 0; i < sentences.length; i++) {\n        var item = sentences[i];\n        fs.appendFileSync(this.outputSentences, item + '\\n');\n    }\n\n    var endTime = new Date().getTime();\n    console.log(util.format('Wrote %s distributions from %s sentences to %s (in %sms)', sortedTags.length, sentences.length, \n                this.output, (endTime - startTime)));\n};\n\nCorpus.prototype.dist = function (tags) {\n    var d = [];\n    for (var t in tags) {\n        var item = tags[t];\n        item.t = t;\n        d.push(item);\n    }\n\n    return d.sort(function (a, b) {\n        return b.f - a.f;\n    });\n};\n\nCorpus.prototype.storeState = function (tags, sentences, state) {\n    for (var t in state.tags) {\n        if (!tags.hasOwnProperty(t)) {\n            tags[t] = { f: 0, df: 0 };\n        }\n\n        tags[t].f += state.tags[t];\n        tags[t].df++;\n    }\n\n    sentences.push(state.tokens.join(' '));\n};\n\nCorpus.prototype.addStateTag = function (state, tag) {\n    if (!state.tags.hasOwnProperty(tag)) {\n        state.tags[tag] = 1;\n    }\n    state.tags[tag]++;\n};\n\nCorpus.prototype.updateState = function (state, word, tag) {\n    if (word) {\n        state.tokens.push(word + \"/\" + tag);\n    }\n    this.addStateTag(state, tag);\n    if (state.prevTag) {\n        var bitag = state.prevTag + \"+\" + tag;\n        this.addStateTag(state, bitag);\n    }\n    if (state.prevTag2 && state.prevTag) {\n        var tritag = state.prevTag2 + \"+\" + state.prevTag + \"+\" + tag;\n        this.addStateTag(state, tritag);\n    }\n\n    state.prevTag2 = state.prevTag;\n    state.prevTag = tag;\n};\n\nCorpus.prototype.addVocabularyTag = function (vocab, word, tag) {\n    if (!vocab.hasOwnProperty(word)) {\n        vocab[word] = { f: 0, t: {} };\n    }\n    vocab[word].f++;\n\n    if (!vocab[word].t.hasOwnProperty(tag)) {\n        vocab[word].t[tag] = 1;\n    }\n    vocab[word].t[tag]++;\n}\n\nCorpus.prototype.parseLine = function (line) {\n    // [ token/POS token/POS ]\n    // token/POS\n    // token/POS token/POS\n    if (line.indexOf('[') == 0 && line.indexOf(']') == line.length) {\n        line = line.substring(1, line.length - 1).trim();\n    }\n\n    var tokenPairs = line.split(' ');\n    var tokenTagPairs = [];\n    for (var i = 0; i < tokenPairs.length; i++) {\n        var tokenPair = tokenPairs[i].split('/');\n        if (tokenPair.length == 2) {\n            var token = tokenPair[0].toLowerCase();\n            var tag = tokenPair[1].toUpperCase();\n            if (this.mapping.hasOwnProperty(tag)) {\n                tag = this.mapping[tag];\n            }\n\n            tokenTagPairs.push([token, tag]);\n        }\n    }\n\n    return tokenTagPairs;\n};\n\nmodule.exports = Corpus;\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/corpus/twitter.js":"\nvar util = require('util');\nvar Corpus = require('./corpus');\n\nfunction TwitterTreeBank(file, options) {\n    Corpus.call(this, file, 'en-tweet', options);\n};\n\nutil.inherits(TwitterTreeBank, Corpus);\n\nTwitterTreeBank.prototype.parseLine = function (line) {\n    // token    POS\n    var tokenPair = line.split('\\t');\n    if (tokenPair.length == 1) {\n        tokenPair = [' ', tokenPair[0]];\n        console.log(line);\n    }\n    var token = tokenPair[0].toLowerCase();\n    var tag = tokenPair[1].toUpperCase();\n    if (this.mapping.hasOwnProperty(tag)) {\n        tag = this.mapping[tag];\n    }\n    return [[token,tag]];\n};\n\nmodule.exports = TwitterTreeBank;\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/corpus/iula.js":"var util = require('util');\nvar Corpus = require('./corpus');\n\nfunction IULATreeBank(file, options) {\n    Corpus.call(this, file, 'es-iula', options);\n};\n\nutil.inherits(IULATreeBank, Corpus);\n\nIULATreeBank.prototype.parseLine = function (line) {\n    // returns [token/POS token/POS]\n    // ID    FORM    LEMMA    CPOSTAG    POSTAG    FEATS    HEAD    DEPREL    PHEAD\n    var tokens = line.split('\\t');\n    if (tokens.length < 3)\n        return [];\n\n    var token = tokens[1];\n    var tag = tokens[3];\n    if (this.mapping.hasOwnProperty(tag)) {\n        tag = this.mapping[tag];\n    }\n\n    return [[token, tag]];\n};\n\nmodule.exports = IULATreeBank;\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/language/hmm.js":"\nvar fs = require('fs'),\n    path = require('path'),\n    util = require('util'),\n    readline = require('readline');\n\nvar Vocabulary = require('./vocabulary');\n\nfunction HiddenMarkovModel(options) {\n    this.options = options || {};\n    this.alpha = this.options.alpha || 0.2;\n    this.terms = 0;\n    this.lambdaV = [0.01, 0.41, 0.58]; // gram, bigram, trigram\n    this.tags = {'0': 'NOUN', '1': 'VERB', '2': 'ADJ', '3': 'ADV', '4': 'X', '5': 'NUM', \n                 '6': 'PRON', '7': 'CONJ', '8': 'DET', '9': 'PRT', '10': 'ADP', '11': '.', '*': '*'\n                };\n    this.tagLength = 12;\n};\n\nHiddenMarkovModel.prototype.restore = function (file) {\n    if (file.indexOf('~') == 0) {\n        file = path.join(process.env['HOME'], file.substring(1));\n    }\n    if (file.indexOf('.json') < 0) {\n        throw Error('Must be a valid json output');\n    }\n\n    // Set all appropriate local data\n    var fileData = require(file);\n    if (fileData.vocabulary) {\n        this.vocabulary = new Vocabulary();\n        this.vocabulary.restore(fileData.vocabulary);\n    }\n    if (fileData.tagFreq)\n        this.tagFreq = fileData.tagFreq;\n    if (fileData.data)\n        this.data = fileData.data;\n    if (fileData.vocabularyTagDistribution)\n        this.vocabularyTagDistribution = fileData.vocabularyTagDistribution;\n    if (fileData.vocabularyTagClasses)\n        this.vocabularyTagClasses = fileData.vocabularyTagClasses;\n    if (fileData.vocabularyTagProbability)\n        this.vocabularyTagProbability = fileData.vocabularyTagProbability;\n    if (fileData.alpha)\n        this.alpha = fileData.alpha;\n    if (fileData.options)\n        this.options = fileData.options;\n    if (fileData.terms)\n        this.terms = fileData.terms;\n    if (fileData.lambdaV)\n        this.lambdaV = fileData.lambdaV;\n    if (fileData.tagGramDistribution)\n        this.tagGramDistribution = fileData.tagGramDistribution;\n    if (fileData.totalTags)\n        this.totalTags = fileData.totalTags;\n    if (fileData.tagClasses)\n        this.tagClasses = fileData.tagClasses;\n};\n\n/**\n * Computes the cross-validation error by determing a simple summation of log probabilities \n * times the actual frequency within the validation hidden markov model. This should allow \n * us to graph the maximum log probability score based on the configured lambda parameters.\n */\nHiddenMarkovModel.prototype.crossValidate = function (validationHMM) {\n    var total = 0;\n    for (var tag in this.tagGramDistribution) {\n        var len = tag.split('+').length;\n\n        if (len == 3) {\n            var item = this.tagGramDistribution[tag];\n            var logp = item.logp;\n            var freq = 0;\n            if (validationHMM.tagGramDistribution.hasOwnProperty(tag)) {\n                freq = validationHMM.tagGramDistribution[tag].freq;\n            }\n\n            total += (freq * logp);\n        }\n    }\n\n    this.validationScore = total;\n    return this.validationScore;\n};\n\n/**\n * Estimates the probability distribution of a given tag sequence \n * over its appropriate predetermined tags using a backoff model.\n */\nHiddenMarkovModel.prototype.estimateTagDistribution = function () {\n    if (!this.tagGramDistribution) {\n        throw Error(\"Must load a tag sequence distribution first\");\n    }\n\n    var tagDistribution = this.tagGramDistribution;\n    for (var tag in tagDistribution) {\n        var item = tagDistribution[tag];\n        \n        // Estimate parameters q for the given n-gram distribution\n        // this can be done by first calculating the distribution \n        var itemFrequency = item.freq;\n        var tags = item.tag.split('+');\n        var minus1 = tags.slice(1);\n        var minus1Freq = 0;\n        if (minus1.length > 0) {\n            // use count of the distribution\n            var gram = minus1.join('+');\n            if (tagDistribution.hasOwnProperty(gram)) {\n                minus1Freq = tagDistribution[gram].freq;\n            }\n        }\n        else {\n            minus1Freq = this.totalTags;\n        }\n\n        // Calculate maximum likelihood of the given distribution\n        if (item.freq > 0 && minus1Freq > 0) {\n            item.ml = item.freq / minus1Freq;\n        }\n    }\n\n    for (var tag in tagDistribution) {\n        var item = tagDistribution[tag];\n        var tags = item.tag.split('+');\n        item.p = 0.0;\n        for (var t = 1; t <= tags.length; t++) {\n            var gram = tags.slice(tags.length - t).join('+');\n            if (gram && tagDistribution.hasOwnProperty(gram)) {\n                var gramItem = tagDistribution[gram];\n                if (gramItem.hasOwnProperty('ml')) {\n                    var lambda = this.lambdaV[t - 1];\n                    item.p += lambda * gramItem.ml;\n                }\n            }\n        }\n\n        item.logp = Math.log(item.p);\n    }\n};\n\n/**\n * Estimates the probability distribution of a given vocabulary \n * over its appropriate tags and equivalence classes.\n */\nHiddenMarkovModel.prototype.estimateVocabulary = function () {\n    if (!this.data) {\n        throw Error(\"Must load a vocabulary distribution first\");\n    }\n\n    // Iterate over the equivalence classes available to generate a local array\n    // usable for easy retrieval of the pos tags themself\n    var tagClasses = [];\n    var allStates = [];\n    for (var i = 0, l = this.vocabularyTagClasses.length; i < l; i++) {\n        var cls = this.vocabularyTagClasses[i];\n        var posItems = cls.split(',');\n        var pos = [];\n        for (var p = 0; p < posItems.length; p++) {\n            var posItem = posItems[p];\n            pos.push(parseInt(posItem));\n        }\n        tagClasses[i] = pos;\n\n        if (posItems.length == 1) {\n            allStates.push(pos);\n        }\n    }\n\n    this.tagClasses = tagClasses;\n    this.vocabularyTagProbability = [];\n\n    // iterate over the vocabulary and use the \n    // vocabulary tag distribution to estimate \n    // the probability of a tag given a word\n    for (var posEquivIndex = 0; posEquivIndex < this.tagClasses.length; posEquivIndex++) {\n        var pos = this.tagClasses[posEquivIndex];\n        var tagClassFreq = this.vocabularyTagDistribution[posEquivIndex];\n        var tagEqProb = {};\n        var posProb = [];\n        if (tagClassFreq) {\n            // Estimate the probability for each \n            // tag that this item can be assigned as \n            var total = 0;\n            for (var i = 0; i < pos.length; i++) {\n                // since we initialize the tag class and distributions\n                // based on the initial number of tags, all equivalence \n                // classes will be based on these numbers, so these should\n                // be immediately accessible by index\n                var p = pos[i]; \n                var tagFreq = this.vocabularyTagDistribution[p];\n\n                var eqProb = 0;\n                if (tagFreq) {\n                    eqProb = tagClassFreq / tagFreq; // P(eqcls w.r.t. t) = Freq(eqcls) / Freq(t)\n                    eqProb *= this.alpha;\n                    eqProb = Math.log(eqProb);\n                }\n\n                tagEqProb[p] = eqProb;\n                total += tagEqProb[p];\n            }\n\n            // divide each tag frequency equivalence probability \n            // by the aggregate sum of all of them\n            for (var i = 0; i < pos.length; i++) {\n                var p = pos[i];\n                var prob = 1 / pos.length;\n                posProb.push(prob);\n            }\n        }\n\n        this.vocabularyTagProbability[posEquivIndex] = posProb;\n    };\n    \n    // finally add the class state for all common base states\n    allStates.sort();\n    var total = 0;\n    var items = [];\n    this.tagFreq = {};\n    for (var i = 0; i < allStates.length; i++) {\n        var tagFreq = this.vocabularyTagDistribution[allStates[i]];\n        if (tagFreq) {\n            var tag = this.tags[allStates[i].toString()];\n            this.tagFreq[tag] = tagFreq;\n            items.push(allStates[i]);\n            total += tagFreq;\n        }\n    }\n\n    var probs = [];\n    for (var i = 0; i < items.length; i++) {\n        var tagFreq = this.vocabularyTagDistribution[items[i]];\n        var prob = tagFreq / total;\n        probs.push(prob);\n    }\n\n    if (items && probs) {\n        this.tagClasses.push(items);\n        this.vocabularyTagProbability.push(probs);\n    }\n};\n\n/**\n * Loads the given vocabulary and n-gram tag distribution files.\n */\nHiddenMarkovModel.prototype.load = function (vocabFile, tagGramDistFile, callback) {\n    this.loadVocab(vocabFile, function () {\n        this.loadTagDist(tagGramDistFile, callback);\n    });\n};\n\n/**\n * Loads the given n-gram tag distribution file which should \n * include all possible grams (up to n-grams) and their relevant \n * frequency distributions that were found within a tagged corpus.\n */\nHiddenMarkovModel.prototype.loadTagDist = function (tagDistFile, callback) {\n    var self = this;\n\n    this.tagGramDistribution = {};\n    this.totalTags = 0;\n\n    var inputStream = fs.createReadStream(tagDistFile);\n    var rl = readline.createInterface({ input: inputStream, terminal: false });\n    rl.on('line', function (line) {\n        var items = line.trim().split('\\t');\n        if (items.length < 3) {\n            line = null;\n            return;\n        }\n\n        // tag+tag+tag    freq    distribution%\n        var item = {};\n        item.tag = items[0];\n        item.freq = parseInt(items[1]);\n\n        self.tagGramDistribution[item.tag] = item;\n\n        // Aggregate all tag frequencies\n        if (item.tag.split('+').length == 1) {\n            self.totalTags += item.freq;\n        }\n    });\n    rl.on('close', function () {\n        callback();\n    });\n};\n\n/**\n * Loads the given vocabulary file into memory. This should allow \n * us to automatically create a proper vocabulary and vocabulary \n * tag distribution model. We will filter out duplicate entries \n * and be able to perform further reduction techniques here. In \n * addition, the vocabulary tag distribution will give us an \n * idea how how many times a particular part of speech occurs \n * as well as how many times a word ends up with a set of tags \n * (noted as our equivalence classes).\n */\nHiddenMarkovModel.prototype.loadVocab = function (vocabFile, tagCount, callback) {\n    var self = this;\n\n    this.vocabulary = new Vocabulary();\n    this.data = { pos: [], posFreq: [], eqCls: [] };\n    this.vocabularyTagDistribution = [];\n    this.vocabularyTagClasses = [];\n\n    // Pre-calculate the tag distribution according to the number of tags\n    // use the default universal tag set count of 12\n    // (0 = NOUN, 1 = VERB, 2 = ADJ, 3 = ADV, 4 = X, 5 = NUM, 6 = PRON, 7 = PRT, 8 = CONJ, 9 = DET, 10 = ADP, 11 = .)\n    if (typeof tagCount == 'function') {\n        callback = tagCount;\n        tagCount = 12;\n    }\n    else if (typeof tagCount == 'undefined') {\n        tagCount = 12;\n    }\n\n    for (var i = 0; i < tagCount; i++) {\n        this.vocabularyTagClasses.push(i.toString());\n        this.vocabularyTagDistribution[i] = 0;\n    }\n\n    var inputStream = fs.createReadStream(vocabFile);\n    var rl = readline.createInterface({ input: inputStream, terminal: false });\n    var index = 0;\n    rl.on('line', function (line) {\n        var items = line.trim().split('\\t');\n        if (items.length < 3) {\n            items.push('11');\n        }\n\n        // id   vocab   pos,pos    pos-freq,pos-freq\n        var item = { pos: [] };\n        var token = items[1].toLowerCase();\n        var posItems = items[2].split(',');\n        var sortedPos = [];\n        for (var p = 0; p < posItems.length; p++) {\n            item.pos.push(parseInt(posItems[p]));\n            sortedPos.push(item.pos[p]);\n        }\n\n        if (item.pos.length == 1) {\n            if (isNaN(item.pos[0])) {\n                return;\n            }\n        }\n\n        if (items.length == 4) {\n            var posFreqItems = items[3].split(',');\n            var posFreq = [];\n            for (var k = 0; k < posFreqItems.length; k++) {\n                var f = parseInt(posFreqItems[k]);\n                posFreq.push(f);\n            }\n            item.posFreq = posFreq;\n        }\n\n        var existingIndex = self.vocabulary.get(token);\n        if (existingIndex) {\n            var aposIndex = self.data.pos[existingIndex];\n            var aposClass = self.vocabularyTagClasses[aposIndex];\n            var apos = aposClass.split(',');\n            var aposFreq = self.data.posFreq[existingIndex];\n            var result = self.mergeVocabulary(apos, aposFreq, item);\n            item = result;\n        }\n\n        // For each equivalance class, where the pos that a term \n        // falls into (i.e. if a term falls into just having a single \n        // NOUN associated with it), then use that as an equivalence class \n        // where we can count up the times that type of class occurs\n        var pos = item.pos;\n        var equivalenceClass = sortedPos.join(',');\n        var equivalenceIndex = self.vocabularyTagClasses.indexOf(equivalenceClass);\n        if (equivalenceIndex < 0) {\n            equivalenceIndex = self.vocabularyTagClasses.length;\n            self.vocabularyTagClasses.push(equivalenceClass);\n            self.vocabularyTagDistribution[equivalenceIndex] = 0;\n        }\n        self.vocabularyTagDistribution[equivalenceIndex]++;\n\n        // For each part of speech tag associated with the term\n        // increment a frequency for that kind of tag\n        for (var k = 0; k < pos.length; k++) {\n            var p = pos[k].toString();\n            var pEquivClassIndex = self.vocabularyTagClasses.indexOf(p);\n            if (pEquivClassIndex < 0) {\n                pEquivClassIndex = self.vocabularyTagClasses.length;\n                self.vocabularyTagClasses.push(p);\n                self.vocabularyTagDistribution[pEquivClassIndex] = 0;\n            }\n            self.vocabularyTagDistribution[pEquivClassIndex]++;\n        }\n\n        if (existingIndex) {\n            self.data.pos[existingIndex] = equivalenceIndex;\n            if (item.posFreq)\n                self.data.posFreq[existingIndex] = item.posFreq;\n        }\n        else {\n            self.data.pos[self.terms] = equivalenceIndex;\n            if (item.posFreq) {\n                self.data.posFreq[self.terms] = item.posFreq;\n            }\n            self.vocabulary.addTerm(token, self.terms++);\n        }\n\n        process.stdout.clearLine();\n        process.stdout.cursorTo(0);\n        process.stdout.write('Processed ' + (index++) + ' total items');\n        posItems = null;\n        items = null;\n        line = null;\n    });\n    rl.on('close', function () {\n        self.vocabulary.computeHash();\n        callback();\n    });\n};\n\n/**\n * Merges the two terms in the case there was a kind of duplicate entry \n * in the list of vocabulary terms (such that due to casing or other \n * lemma rules that may apply).\n */\nHiddenMarkovModel.prototype.mergeVocabulary = function (apos, aposFreq, b) {\n    var mergedItem = {};\n    var useFrequency = false;\n\n    // Select the pos and pos frequencies from a\n    var newPos = {};\n    for (var i = 0; i < apos.length; i++) {\n        var aPosTag = apos[i];\n        var freq = 1;\n        if (aposFreq) {\n            useFrequency = true;\n            freq = aposFreq[i];\n        }\n\n        if (newPos.hasOwnProperty(aPosTag)) {\n            newPos[aPosTag] += freq;\n        }\n        else {\n            newPos[aPosTag] = freq;\n        }\n    }\n\n    // Select the pos and pos frequencies from b\n    for (var i = 0; i < b.pos.length; i++) {\n        var bPosTag = b.pos[i];\n        var freq = 1;\n        if (b.posFreq) {\n            useFrequency = true;\n            freq = b.posFreq[i];\n        }\n\n        if (newPos.hasOwnProperty(bPosTag)) {\n            newPos[bPosTag] += freq;\n        }\n        else {\n            newPos[bPosTag] = freq;\n        }\n    }\n\n    mergedItem.pos = [];\n    if (useFrequency)\n        mergedItem.posFreq = [];\n\n    // Set all appropriate pos items\n    for (var pItem in newPos) {\n        mergedItem.pos.push(parseInt(pItem));\n        if (useFrequency) {\n            mergedItem.posFreq.push(newPos[pItem]);\n        }\n    }\n\n    mergedItem.pos = mergedItem.pos.sort();\n    return mergedItem;\n};\n\n/**\n * Implements the standard viterbi algorithm for calculating the \n * most probable path given the k-th order markov chain applies. \n * This should use the given tokens as an observation sequence \n * in order to determine the most probable tags for the tokens.\n */\nHiddenMarkovModel.prototype.viterbi = function (tokens) {\n    var states = [];\n    var stateProb = [];\n    for (var k = 0; k < tokens.length; k++) {\n        var token = tokens[k].toLowerCase();\n\n        var isNumeric = false;\n        var isMentionHashTag = false;\n        var isUrl = false;\n        var isRepeatedPunct = false;\n        if (token.match(/^\\d+\\%?/)) {\n            isNumeric = true;\n        }\n        else if (token.match(/^@[A-Za-z0-9_\\-]+/) || token.match(/^#\\w+/)) {\n            isMentionHashTag = true;\n        }\n        else if (token.indexOf('http://') == 0) {\n            isUrl = true;\n        }\n        else if (token.match(/^[!.=\\-:”“\"?,(){}\\[\\]*]+/)) {\n            isRepeatedPunct = true;\n        }\n\n        if (!isNumeric && !isMentionHashTag && !isUrl && !isRepeatedPunct) {\n            var vocabIndex = this.vocabulary.get(token);\n            var eqIndex = this.tagClasses.length - 1;\n\n            if (vocabIndex) {\n                eqIndex = this.data.pos[vocabIndex];\n            }\n\n            var tagProbK = this.vocabularyTagProbability[eqIndex];\n\n            var stateK = this.tagClasses[eqIndex];\n            var tags = [];\n\n            for (var s = 0; s < stateK.length; s++) {\n                var state = stateK[s].toString();\n                tags.push(this.tags[state]);\n            }\n\n            if (vocabIndex) {\n                var posFreqV = this.data.posFreq[vocabIndex];\n                if (posFreqV && tags.length == posFreqV.length) {\n                    var newTagProbK = [];\n                    var total = 0.0;\n                    for (var i = 0; i < tags.length; i++) {\n                        var posFreq = posFreqV[i];\n                        if (this.tagFreq.hasOwnProperty(tags[i])) {\n                            var prob = (posFreq / this.tagFreq[tags[i]]);\n                            total += prob;\n                            newTagProbK.push(prob);\n                        }\n                    }\n\n                    for (var i = 0; i < newTagProbK.length; i++) {\n                        newTagProbK[i] = newTagProbK[i] / total;\n                    }\n\n                    if (newTagProbK.length == tags.length) {\n                        tagProbK = newTagProbK;\n                    }\n                }\n            }\n\n            states.push(tags);\n            stateProb.push(tagProbK);\n        }\n        else if(isNumeric) {\n            states.push(['NUM']);\n            stateProb.push([1]);\n        }\n        else if (isMentionHashTag) {\n            states.push(['X']);\n            stateProb.push([1]);\n        }\n        else if (isUrl) {\n            states.push(['X']);\n            stateProb.push([1]);\n        }\n        else if (isRepeatedPunct) {\n            states.push(['.']);\n            stateProb.push([1]);\n        }\n    }\n\n    // Initialize the base state where pi(0,*,*) = 1 and pi(0,u,v) = 0 for all (u,v)\n    // such that u != * or v != *\n    var pi = [{ '*': { '*': 1 }}];\n    var path = [];\n    for (var labelU in this.tags) {\n        var tagU = this.tags[labelU];\n        if (tagU == '*')\n            continue;\n        pi[0][tagU] = {};\n        for (var labelV in this.tags) {\n            var tagV = this.tags[labelV];\n            if (tagV == '*')\n                continue;\n            pi[0][tagU][tagV] = 0;\n        }\n    }\n\n    /*\n    var allStates = {};\n    var lastDistribution = this.tagClasses[this.tagClasses.length - 1];\n    for (var l = 0; l < lastDistribution.length; l++) {\n        var item = lastDistribution[l][0].toString();\n        var prob = this.vocabularyTag[this.tagClasses.length - 1][l];\n        allStates[this.tags[item]] = prob;\n    }*/\n\n    for (var k = 1; k <= tokens.length; k++) {\n        var tempPath = {};\n        var tempPi = {};\n        var prevPi = pi[k-1];\n        var statesU = this.getPossibleStates(k-1, states, '*');\n        var statesV = this.getPossibleStates(k, states, '*');\n        var statesW = this.getPossibleStates(k-2, states, '*');\n\n        // Get the emission probabiity for the word token at k - 1\n        var emissionsV = stateProb[k-1];\n\n        for (var uIndex = 0; uIndex < statesU.length; uIndex++) {\n            var u = statesU[uIndex];\n            tempPi[u] = {};\n            tempPath[u] = {};\n            for (var vIndex = 0; vIndex < statesV.length; vIndex++) {\n                var v = statesV[vIndex];\n                var emissionV = emissionsV[vIndex];\n                var argMax = undefined;\n                for (var wIndex = 0; wIndex < statesW.length; wIndex++) {\n                    var w = statesW[wIndex];\n                    var trigram = w + '+' + u + '+' + v;\n                    var trigramDist = this.tagGramDistribution[trigram];\n                    if (!trigramDist)\n                        trigramDist = { logp: Math.log(0.000001) };\n\n                    var transitionQ = trigramDist.logp;\n                    var logEmissionV = Math.log(emissionV);\n                    if (logEmissionV == 0) {\n                        logEmissionV = -1;\n                    }\n\n                    var prob = prevPi[w][u] * transitionQ * logEmissionV;\n                    if (!argMax || prob < argMax.p) {\n                        argMax = { 'p': prob, 'u': u, 'v': v, 'k': k, 'w': w };\n                    }\n                }\n\n                if (!argMax) {\n                    continue;\n                }\n\n                tempPi[u][v] = argMax.p;\n                tempPath[u][v] = argMax.w;\n            }\n        }\n\n        pi.push(tempPi);\n        path.push(tempPath);\n    }\n\n    var prevPi = pi[pi.length - 1];\n    var maxArg;\n    for (var u in prevPi) {\n        for (var v in prevPi[u]) {\n            var stopGram = u + '+' + v + \"+STOP\";\n            var distProb = 1;\n            if (this.tagGramDistribution.hasOwnProperty(stopGram)) {\n                distProb = Math.log(this.tagGramDistribution[stopGram].p);\n                if (distProb == 0) {\n                    distProb = Math.log(0.9999999);\n                }\n            }\n            else {\n                distProb = Math.log(0.000001);\n            }\n\n            var prob = prevPi[u][v] * distProb;\n            if (!maxArg || maxArg.p < prob) {\n                maxArg = { u: u, v: v, p: prob };\n            }\n        }\n    }\n\n    // store the best tag sequence in reverse for brevity, and invert the \n    // backpointers list as well during this processing\n    var y = [ maxArg.v, maxArg.u ];\n    path = path.reverse();\n\n    // in each case,\n    // the following best tag, is the one listed under the backpointer \n    // for the current best known tag\n    var currentBestY = maxArg.v;\n    var currentBestY1 = maxArg.u;\n    for (var k = 0; k < path.length; k++) {\n        var tempCurrentBest = path[k][currentBestY1][currentBestY];\n        currentBestY = currentBestY1;\n        currentBestY1 = tempCurrentBest;\n        if (tempCurrentBest != \"*\")\n            y.push(tempCurrentBest);\n    }\n\n    y = y.reverse();\n\n    return { 'y': y, 'x': tokens };\n};\n\nHiddenMarkovModel.prototype.getPossibleStates = function (k, states, defaultState) {\n    if (k < 1) {\n        return [defaultState];\n    }\n    else {\n        return states[k-1];\n    }\n};\n\nmodule.exports = HiddenMarkovModel;\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/language/vocabulary.js":"\nvar perfect = require('./mphf');\nvar RadixTree = require('./radixtree');\n\nfunction Vocabulary() {\n    this.tree = {};\n//    this.tokens = [];\n    this.terms = 0;\n//    this.radix = new RadixTree();\n};\n\n/*\nVocabulary.prototype.enumerate = function (callback, tree) {\n    var root = tree || this.tree;\n    for (var t in root) {\n        var node = root[t];\n        if (node._$) {\n            callback(node._$);\n        }\n\n        this.enumerate(callback, node);\n    }\n};\n*/\n\nVocabulary.prototype.restore = function (vocabData) {\n    if (vocabData.tables)\n        this.tables = vocabData.tables;\n    /*\n    if (vocabData.radix)\n        this.radix.rootNode = vocabData.radix.rootNode;\n        */\n};\n\nVocabulary.prototype.computeHash = function () {\n    this.tables = perfect.create(this.tree);\n    delete this.tree;\n\n    /*\n    this.tokens = this.tokens.sort();\n    for (var i = 0; i < this.tokens.length; i++) {\n        var token = this.tokens[i];\n        //this.radix.insert(token);\n    }\n    delete this.tokens;\n    */\n};\n\nVocabulary.prototype.get = function (token) {\n    /*\n    var exists = false;\n    if (this.radix) {\n        exists = this.radix.find(token);\n    }\n    if (!exists)\n        return undefined;\n        */\n\n    if (this.tables) {\n        return perfect.lookup(this.tables[0], this.tables[1], token);\n    }\n    else if (this.tree.hasOwnProperty(token)) {\n        return this.tree[token];\n    }\n    else {\n        return undefined;\n    }\n    /*\n    var root = this.tree;\n    for (var ii = 0, ll = token.length; ii < ll; ii++) {\n        var c = token[ii];\n\n        if (!root.hasOwnProperty(c)) {\n            return undefined;\n        }\n\n        root = root[c];\n    }\n\n    if (!root._$) {\n        return undefined;\n    }\n\n    return root._$;\n    */\n};\n\nVocabulary.prototype.addTerm = function (token, data) {\n    /*\n    var root = this.tree;\n    for (var ii = 0, ll = token.length; ii < ll; ii++) {\n        var c = token[ii];\n\n        if (!root.hasOwnProperty(c)) {\n            root[c] = {};\n        }\n        root = root[c];\n\n        if (ii == ll-1) {\n            if (!root._$) {\n                this.terms++;\n            }\n            root._$ = data || 1;\n        }\n    }\n    */\n    if (!this.tree.hasOwnProperty(token)) {\n        this.terms++;\n    //    this.tokens.push(token);\n    }\n    this.tree[token] = data;\n};\n\nmodule.exports = Vocabulary;\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/language/mphf.js":"\n// Forked from github.com/mixu by Mikito Takada <mikito.takada@gmail.com>\n// Calculates a distinct hash function for a given string. Each value of the\n// integer d results in a different hash value.\nfunction hash( d, str) {\n  if(d == 0) { d = 0x811c9dc5; }\n  for(var i = 0; i < str.length; i++) {\n    // http://en.wikipedia.org/wiki/Fowler%E2%80%93Noll%E2%80%93Vo_hash_function\n    // http://isthe.com/chongo/src/fnv/hash_32.c\n    // multiply by the 32 bit FNV magic prime mod 2^32\n    d += (d << 1) + (d << 4) + (d << 7) + (d << 8) + (d << 24);\n    // xor the bottom with the current octet\n    d ^= str.charCodeAt(i);\n  }\n  return d & 0x7fffffff;\n}\n\nexports.hash = hash;\n\n// Computes a minimal perfect hash table using the given Javascript object hash. It\n// returns a tuple (G, V). G and V are both arrays. G contains the intermediate\n// table of values needed to compute the index of the value in V. V contains the\n// values of the dictionary.\n\nexports.create = function(dict) {\n  var size = Object.keys(dict).length,\n      buckets = [],\n      G = new Array(size),\n      values = new Array(size),\n      i, b, bucket;\n\n  // Place all of the keys into buckets\n  Object.keys(dict).forEach(function(key) {\n    var bkey = hash(0, key) % size;\n    if(!buckets[bkey]) {\n      buckets[bkey] = [];\n    }\n    buckets[bkey].push( key );\n  });\n\n  // Sort the buckets and process the ones with the most items first.\n  buckets.sort(function(a, b) { return b.length - a.length; });\n\n  for(b = 0; b < size; b++) {\n    if(buckets[b].length <= 1) break;\n    bucket = buckets[b];\n\n    var d = 1, item = 0, slots = [], slot, used = {};\n\n    // Repeatedly try different values of d until we find a hash function\n    // that places all items in the bucket into free slots\n    while(item < bucket.length) {\n      slot = hash(d, bucket[item]) % size;\n      if(values[slot] || used[slot]) {\n        d++;\n        item = 0;\n        slots = [];\n        used = {};\n      } else {\n        used[slot] = true;\n        slots.push(slot);\n        item++;\n      }\n    }\n\n    G[hash(0, bucket[0]) % size] = d;\n    for(i = 0; i < bucket.length; i++) {\n      values[slots[i]] = dict[bucket[i]];\n    }\n  }\n\n  // Only buckets with 1 item remain. Process them more quickly by directly\n  // placing them into a free slot. Use a negative value of d to indicate\n  // this.\n\n  var freelist = [];\n  for(i = 0; i < size; i++) {\n    if(typeof values[i] == 'undefined' ) {\n      freelist.push(i);\n    }\n  }\n\n  for(; b < size; b++ ) {\n    if (!buckets[b] || buckets[b].length == 0) break;\n    bucket = buckets[b];\n    slot = freelist.pop();\n\n    // We subtract one to ensure it's negative even if the zeroeth slot was used.\n    G[hash(0, bucket[0]) % size] = 0-slot-1;\n    values[slot] = dict[bucket[0]];\n  }\n\n  return [ G, values ];\n};\n\n\n// Look up a value in the hash table, defined by G and V.\nexports.lookup = function(G, V, key) {\n  var d = G[ hash(0,key) % G.length ];\n  if (d < 0) return V[ 0-d-1 ];\n  return V[hash(d, key) % V.length];\n};\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/language/radixtree.js":"/*\nThe MIT License\n\nCopyright (c) 2008 Javid Jamae\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n*/\n\n/**\n * Author: Javid Jamae\n * Website: http://www.javidjamae.com/\n */\nfunction RadixTree() {\n    this.rootNode = { k: \"\", e: [] };\n    \n    this.insert = function(key) {\n        this._insertInternal(key, this.rootNode)\n    }\n    \n    this._insertInternal= function(key, node) {\n        if (isStringSubNode(node, key)) {\n            this._addNodeOrRecurse(key, node);\n        } else if (key == node.k) {\n            this._makeNodeDataNode(key, node);\n        } else if (isStringPartialMatch(node, key)) {\n            this._splitNode(key, node);\n        } else {                \n            this._addNodeAsChild(key, node);\n        }\n    }\n    \n    this._addNodeOrRecurse = function(key, node) {\n        var addedToChild = false\n        var newText = getMatchingPortionOfString(node, key);\n        for (var j = 0; j<node.e.length; j++) {\n            if (node.e[j].k.startsWith(newText.charAt(0))) {\n                addedToChild = true\n                this._insertInternal(newText, node.e[j])\n                break\n            }\n        }\n\n        if (addedToChild == false) {\n            var n = { k: newText, e: [], r: true };\n            node.e.push(n);\n        }\n    }\n    \n    this._makeNodeDataNode = function(key, node) {\n        if (node.r) {\n            throw \"Duplicate key\";\n        }\n        node.r = true;\n    }\n    \n    this._addNodeAsChild = function(key, node) {\n        console.log(\"This doesn't ever seem to be called. If you see this alert, you just found a case where it is. You now have reason to remove this alert.\");\n        var n = deepCopy(node);\n        n.k = (getMatchingPortionOfNodeKey(node, key));\n        node.k = (key);\n        node.r = (true);\n        node.e.push(n);\n    }\n    \n    this._splitNode = function(key, node) {\n        var n1 = deepCopy(node);\n        n1.k = (getMatchingPortionOfNodeKey(node, key));\n        node.k = (getUnmatchingPortionOfString(node, key));\n        node.r = (false);\n        node.e = [];\n        node.e.push(n1);\n          \n        if(getNumberOfMatchingCharacters(node, key) < key.length) {\n            var n2 = { e: [], r: true };\n            n2.k = (getMatchingPortionOfString(node, key));\n            n2.r = (true);\n            node.e.push(n2);\n        } else {\n            node.r = (true);\n        }\n    }\n    \n    /**\n     * searchString - Any string to search for\n     * limit - the number of results to find before returning\n     */\n    this.search = function(searchString, recordLimit) {\n        var visitor = new Visitor()\n        \n        visitor.result = new Array()\n        \n        visitor.visit = function(key, parent, node) {\n            if (node.r) {\n                this.result.push(node.value)\n            }\n        }\n        visitor.shouldVisit = function(key, node) {\n            return node.k.startsWith(key) && this.result.length < recordLimit;\n        }\n        visitor.shouldRecurse = function(key, node){\n            return this.result.length < recordLimit;\n        }\n        visitor.shouldVisitChild = function(key, childNode) {\n            return childNode.k.startsWith(key.charAt(0)) && this.result.length < recordLimit;\n        }\n\n        this.visit(searchString, visitor)\n        \n        return visitor.result\n    }   \n    \n    this.find = function(key) {\n        var visitor = new Visitor()\n        visitor.visit = function(key, parent, node) {\n            if (node.r) {\n                this.result = true\n            }\n        }\n        visitor.shouldVisit = function(key, node) {\n            return key == node.k\n        }\n        visitor.shouldRecurse = function(key, node){\n            return isStringSubNode(node, key)\n        }\n        visitor.shouldVisitChild = function(key, childNode) {\n            return childNode.k.startsWith(key.charAt(0))\n        }\n        this.shouldBreakAfterFindingChild = function() {\n            return true;\n        }\n\n        this.visit(key, visitor)\n        return visitor.result\n    }\n\n    this.visit = function(key, visitor) {\n        this._visitInternal(key, visitor, null, this.rootNode);\n    }\n    \n    this._visitInternal = function(prefix, visitor, parent, node) {\n        if (visitor.shouldVisit(prefix, node)) {\n            visitor.visit(prefix, parent, node);\n        }\n        if (visitor.shouldRecurse(prefix, node)) {\n            var newText = getMatchingPortionOfString(node, prefix);\n            for (var j = 0; j < node.e.length; j++) {\n                // recursively search the child nodes\n                if (visitor.shouldVisitChild(newText, node.e[j])) {\n                    this._visitInternal(newText, visitor, node, node.e[j]);\n                    if (visitor.shouldBreakAfterFindingChild()) {\n                        break;\n                    }\n                }\n            }\n        }\n    }\n    \n    this.getNumberOfRealNodes = function() {\n        var visitor = new Visitor()\n        visitor.result = 0;\n        \n        visitor.visit = function(key, parent, node) {\n            if (node.r) {\n                this.result++;\n            } \n        }\n        visitor.shouldVisit = function(key, node) {\n            return true\n        }\n        visitor.shouldRecurse = function(key, node){\n            return true\n        }\n        visitor.shouldVisitChild = function(key, childNode) {\n            return true\n        }\n        \n        this.visit(\"\", visitor)\n        return visitor.result\n    }\n    \n    this.getNumberOfNodes = function() {\n        var visitor = new Visitor()\n        visitor.result = 0;\n        \n        visitor.visit = function(key, parent, node) {\n            this.result++;\n        }\n        visitor.shouldVisit = function(key, node) {\n            return true\n        }\n        visitor.shouldRecurse = function(key, node){\n            return true\n        }\n        visitor.shouldVisitChild = function(key, childNode) {\n            return true\n        }\n        \n        this.visit(\"\", visitor)\n        return visitor.result\n    }\n}\n\nfunction escapeRegExp(str) {\n      return str.replace(/[\\-\\[\\]\\/\\{\\}\\(\\)\\*\\+\\?\\.\\\\\\^\\$\\|]/g, \"\\\\$&\");\n}\n\nString.prototype.startsWith = function(str) {\n    return this.match(\"^\" + escapeRegExp(str)) == str\n}\n\nArray.prototype.contains = function (element) {\n    for (var i = 0; i < this.length; i++) {\n        if (this[i] == element) \n            {\n                return true;\n            }\n        }\n    return false;\n};\n\nfunction isStringSubNode(node, someString) {\n    if (node.k == \"\") {\n        return true;\n    } else {\n        return (getNumberOfMatchingCharacters(node, someString) < someString.length)\n            && (getNumberOfMatchingCharacters(node, someString) >= node.k.length);\n    }\n}\n\nfunction getNumberOfMatchingCharacters(node, key) {\n    var result = 0\n    while (result < key.length && result < node.k.length) {\n        if (key.charAt(result) != node.k.charAt(result)) {\n            break;\n        }\n        result++\n    }\n    return result;\n}\n\nfunction getMatchingPortionOfString(node, someString) {\n    return someString.substring(getNumberOfMatchingCharacters(node, someString));\n}\n\nfunction getMatchingPortionOfNodeKey(node, someString) {\n    return node.k.substring(getNumberOfMatchingCharacters(node, someString));\n}\n\nfunction getUnmatchingPortionOfString(node, someString) {\n    return someString.substring(0, getNumberOfMatchingCharacters(node, someString));\n}\n\nfunction isStringPartialMatch(node, someString) {\n    return getNumberOfMatchingCharacters(node, someString) > 0 \n        && getNumberOfMatchingCharacters(node, someString) < node.k.length;\n}\n\nfunction deepCopy(node) {\n    var result = {};\n    result.k = node.k;\n    result.e = node.e;\n    result.r = node.r;\n    return result;\n}    \n\nfunction Visitor() {\n    var result;\n    this.getResult = function() {\n        return this.result\n    }\n    this.shouldBreakAfterFindingChild = function() {\n        return false;\n    }\n}\n\nmodule.exports = RadixTree;\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/tagging/hmm_tagger.js":"\nvar PartOfSpeechTagger = require('./partofspeech_tagger'),\n    util = require('util'),\n    path = require('path');\n\nvar HiddenMarkovModel = require('./../language/hmm');\n\nvar HmmTagger = function (options) {\n    this.options = options || {};\n    this.modelFile = this.options.model || path.join(__dirname, '../../../bin/en.hmm.json');\n    this.hmm = new HiddenMarkovModel();\n    this.hmm.restore(this.modelFile);\n};\n\nutil.inherits(HmmTagger, PartOfSpeechTagger);\n\n/**\n * Prototype method for tagging a given string \n * for parts of speech appropriately.\n */\nHmmTagger.prototype.tag = function (tokens, callback) {\n    return this.hmm.viterbi(tokens).y;\n};\n\nmodule.exports = HmmTagger;\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/tagging/partofspeech_tagger.js":"\nvar PartOfSpeechTagger = function () {};\n\n/**\n * Prototype method for tagging a given string \n * for parts of speech appropriately.\n */\nPartOfSpeechTagger.prototype.tag = function (tokens, callback) {\n};\n\nmodule.exports = PartOfSpeechTagger;\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/tagging/treetagger.js":"\nvar PartOfSpeechTagger = require('./partofspeech_tagger'),\n    Mapping = require('./../corpus/mapping');\nvar util = require('util'),\n    path = require('path'),\n    fs = require('fs');\nvar tt = require('treetagger');\n\nvar TreeTagger = function (options) {\n    this.options = options || {};\n    this.tagger = new tt(this.options);\n    this.mapUniversal = this.options.hasOwnProperty('mapUniversal') ? this.options.mapUniversal : true;\n    this.mapping = Mapping.load(path.join(__dirname, '../corpus/universal_tagset/en-ptb.map'));\n};\n\nutil.inherits(TreeTagger, PartOfSpeechTagger);\n\n/**\n * Prototype method for tagging a given string \n * for parts of speech appropriately.\n */\nTreeTagger.prototype.tag = function (tokens, callback) {\n    var self = this;\n    this.tagger.tag(tokens.join(' '), function (err, results) {\n        var tags = [];\n        if (results) {\n            for (var i = 0; i < results.length; i++) {\n                if (self.mapUniversal) {\n                    if (!self.mapping[results[i].pos]) {\n                        if (results[i].pos == 'SENT') {\n                            tags.push('.');\n                        }\n                        else {\n                            console.log(results[i]);\n                        }\n                    }\n                    tags.push(self.mapping[results[i].pos]);\n                }\n                else {\n                    tags.push(results[i].pos);\n                }\n            }\n        }\n\n        callback(err, tags);\n    });\n};\n\nmodule.exports = TreeTagger;\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/glossary/glossary.js":"\nvar fs = require('fs'),\n    path = require('path'),\n    util = require('util');\n\nvar HmmTagger = require('../tagging/hmm_tagger');\nvar Tokenizer = require('../tokenizers/tweet_tokenizer');\nvar Word = require('../language/word');\n\nfunction Glossary(options) {\n    this.options = options || {};\n    this.lang = this.options.lang || \"en\";\n    this.spacing = this.options.spacing || \" \";\n    this.tagger = new HmmTagger(this.options);\n    this.tokenizer = new Tokenizer(this.options);\n    this.mapping = this._loadMapping(this.options.glossMap || path.join(__dirname, '../../../bin/glossdata/' + this.lang + '.gloss.map'));\n    this.ngramMapping = this._nGramMapping(this.mapping);\n    this.tagTime = 0.0;\n    this.tokenTime = 0.0;\n    this.collapseTime = 0.0;\n};\n\nGlossary.prototype._nGramMapping = function (mapping) {\n    var newMap = {};\n    for (var prop in mapping) {\n        var item = mapping[prop];\n        var unigram = {};\n        var bigram = {};\n        var trigram = {};\n        var quadgram = {};\n        for (var term in item) {\n            var terms = term.split(this.spacing);\n            if (terms.length == 2) {\n                if (!bigram.hasOwnProperty(terms[0])) {\n                    bigram[terms[0]] = {};\n                }\n                bigram[terms[0]][terms[1]] = 1;\n            }\n            else if (terms.length == 3) {\n                if (!trigram.hasOwnProperty(terms[0])) {\n                    trigram[terms[0]] = {};\n                }\n                if (!trigram[terms[0]].hasOwnProperty(terms[1])) {\n                    trigram[terms[0]][terms[1]] = {};\n                }\n                trigram[terms[0]][terms[1]][terms[2]] = 1;\n            }\n            else if (terms.length == 4) {\n                if (!quadgram.hasOwnProperty(terms[0])) {\n                    quadgram[terms[0]] = {};\n                }\n                if (!quadgram[terms[0]].hasOwnProperty(terms[1])) {\n                    quadgram[terms[0]][terms[1]] = {};\n                }\n                if (!quadgram[terms[0]][terms[1]].hasOwnProperty(terms[2])) {\n                    quadgram[terms[0]][terms[1]][terms[2]] = {};\n                }\n                quadgram[terms[0]][terms[1]][terms[2]][terms[3]] = 1;\n            }\n            else {\n                unigram[term] = 1;\n            }\n        }\n        newMap[prop] = [unigram,bigram,trigram,quadgram];\n    }\n\n    return newMap;\n};\n\nGlossary.prototype._loadMapping = function (mapFile) {\n    var mapping = {};\n    var lines = fs.readFileSync(mapFile).toString().split('\\n');\n    for (var i = 0; i < lines.length; i++) {\n        var line = lines[i];\n        if (!line || line.trim().length == 0)\n            continue;\n\n        var item = line.trim().split('\\t');\n        if (item.length == 2) {\n            var splitItems = item[1].split(',');\n            var map = {};\n            for (var l = 0; l < splitItems.length; l++) {\n                map[splitItems[l]] = 1;\n            }\n            mapping[item[0]] = map;\n        }\n    }\n    return mapping;\n};\n\nGlossary.prototype.parse = function (text, ignoreChunking) {\n    // reset root term\n    if (this.root) {\n        delete this.root;\n    }\n    \n    var time = new Date().getTime();\n    text = this.tokenizer.clean(text);\n    var tokens = this.tokenizer.tokenize(text);\n    var endTime = new Date().getTime();\n    this.tokenTime += (endTime - time);\n    this.terms = tokens.length;\n    var results = this.tagger.tag(tokens);\n    var newEndTime = new Date().getTime();\n    this.tagTime += (newEndTime - endTime);\n    var prevWord;\n    for (var i = 0; i < results.length; i++) {\n        if (!tokens[i]) {\n            continue;\n        }\n\n        var w = new Word(tokens[i], results[i], i, this.mapping);\n        if (prevWord) {\n            prevWord.next = w;\n        }\n        prevWord = w;\n\n        if (!this.root) {\n            this.root = w;\n        }\n    }\n\n    this.filter();\n    this.collapse(false);\n    if (!ignoreChunking)\n        this.collapse(true);\n    this.collapseTime += (new Date().getTime());\n};\n\nGlossary.prototype.filter = function () {\n    var initial = true;\n    var current = this.root;\n    var keepCurrent = false;\n    do {\n        var skip = current.filter(this.mapping, this.ngramMapping);\n        if (skip > 0) {\n            for (var i = 0; i < skip; i++) {\n                current = current.next;\n            }\n        }\n    } while (current);\n};\n\nGlossary.prototype.collapse = function (chunk) {\n    var initial = true;\n    var current = this.root;\n    var keepCurrent = false;\n    while (keepCurrent || (current && current.next)) {\n        if (!keepCurrent && !initial) {\n            current = current.next;\n        }\n        else {\n            initial = false;\n            keepCurrent = false;\n        }\n\n        if (!current)\n            break;\n\n        if (!chunk)\n            keepCurrent = current.collapse(this.mapping, this.ngramMapping);\n        else\n            keepCurrent = current.chunk(this.mapping);\n    }\n};\n\nGlossary.prototype.unfiltered = function () {\n    var unfiltered = [];\n    var current = this.root;\n\n    do {\n        if (!current.isFiltered && current.tag != '.') {\n            unfiltered.push(current.distinct || current.term);\n        }\n        current = current.next;\n    } while (current);\n\n    return unfiltered;\n}\n\nGlossary.prototype.concepts = function () {\n    var concepts = [];\n    var current = this.root;\n\n    do {\n        if ((current.isNoun || current.isHashtag || current.isMention || current.isUrl) && !current.isTempOrTimeNoun && !current.isSubjectPron) {\n            concepts.push(current.distinct || current.term);\n        }\n        else if (current.isDet && current.next && current.next.isVerb && current.next.next && current.next.next.isNoun) {\n            concepts.push((current.next.distinct || current.next.term) + current.spacing + (current.next.next.distinct || current.next.next.term));\n            current = current.next.next;\n        }\n        current = current.next;\n    } while (current);\n\n    return concepts;\n};\n\nGlossary.prototype.format = function () {\n    var concepts = [];\n    var current = this.root;\n\n    do {\n        concepts.push(current.format());\n        current = current.next;\n    } while (current);\n\n    return concepts;\n};\n\nGlossary.prototype.toJSON = function () {\n    var current = this.root;\n    var words = [];\n\n    do {\n        words.push(current.toJSON());\n        current = current.next;\n    } while (current);\n\n    return words;\n};\n\nGlossary.prototype.itemIndex = function (i) {\n    var current = this.root;\n    var index = 0;\n\n    while (current && index < i) {\n        current = current.next;\n        index++;\n    }\n\n    if (!current)\n        return undefined;\n    else\n        return current.toJSON();\n};\n\nGlossary.prototype.relations = function () {\n    var results = [];\n    var current = this.root;\n\n    do {\n        if ((current.isNoun || current.isHashtag || current.isMention || current.isUrl)) {\n            var relations = [];\n            // N-V-[N|A]\n            if (current.next && current.next.isVerb && current.next.next && \n                    (current.next.next.isNoun || current.next.next.isAdjCard)) {\n                if (!current.isSubjectPron)\n                    relations.push(current.term); // N\n                relations.push(current.next.term); // V\n                relations.push(current.next.next.term); // N\n                if (current.next.next.next && current.next.next.next.isAdp && current.next.next.next.next.isNoun) {\n                    relations.push(current.next.next.next.term); // ADP\n                    relations.push(current.next.next.next.next.term); // N\n                    current = current.next.next.next.next;\n                }\n                else {\n                    current = current.next.next;\n                }\n            }\n            // N-V-ADP-[N|A]\n            else if (current.next && current.next.isVerb && current.next.next && current.next.next.isAdp &&  current.next.next.next && \n                    (current.next.next.next.isNoun || current.next.next.next.isAdjCard)) {\n                if (!current.isSubjectPron)\n                    relations.push(current.term); // N\n                relations.push(current.next.term); // V\n                relations.push(current.next.next.term); // ADP\n                relations.push(current.next.next.next.term); // N\n                current = current.next.next.next;\n            }\n            // N-L-V-[N|A]\n            else if (current.next && current.next.isLink && current.next.next && current.next.next.isVerb && current.next.next.next && \n                    (current.next.next.next.isNoun || current.next.next.next.isAdjCard)) {\n                if (!current.isSubjectPron)\n                    relations.push(current.term); // N\n                if (current.next.isNeg) //[N]\n                    relations.push(current.next.negation + current.next.spacing);\n                relations.push(current.next.next.term); // V\n                relations.push(current.next.next.next.term); // N\n\n                if (current.next.next.next.next && current.next.next.next.next.isAdp && current.next.next.next.next.next && current.next.next.next.next.next.isNoun) {\n                    relations.push(current.next.next.next.next.term); // ADP\n                    relations.push(current.next.next.next.next.next.term); // N\n                    current = current.next.next.next.next.next;\n                }\n                else {\n                    current = current.next.next.next;\n                }\n            }\n            // N-L-V-ADP-[N|A]\n            else if (current.next && current.next.isLink && current.next.next && current.next.next.isVerb && current.next.next.next && \n                    current.next.next.next.isAdp && current.next.next.next.next && \n                    (current.next.next.next.next.isNoun || current.next.next.next.next.isAdjCard)) {\n                if (!current.isSubjectPron)\n                    relations.push(current.term);\n                if (current.next.isNeg)\n                    relations.push(current.next.negation + current.next.spacing);\n                relations.push(current.next.next.term);\n                relations.push(current.next.next.next.term);\n                relations.push(current.next.next.next.next.term);\n                current = current.next.next.next.next;\n            }\n            else if (current.next && current.next.isAdp && current.next.next && current.next.next.isNoun) {\n                relations.push(current.term);\n                relations.push(current.next.term);\n                relations.push(current.next.next.term);\n                current = current.next.next.next;\n            }\n            else if (current.next && current.next.isVerb && current.next.next && current.next.next.isDet && \n                    current.next.next.next && current.next.next.next.isVerb && current.next.next.next.next && \n                    current.next.next.next.next.isNoun) {\n                relations.push(current.term);\n                relations.push(current.next.term);\n                relations.push(current.next.next.term);\n                relations.push(current.next.next.next.term);\n                relations.push(current.next.next.next.next.term);\n                current = current.next.next.next.next;\n            }\n\n            if (relations.length > 0)\n                results.push(relations.join(current.spacing));\n        }\n        current = current.next;\n    } while (current);\n\n    return results;\n};\n\nmodule.exports = Glossary;\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/language/word.js":"\nvar util = require('util');\n\nfunction Word(term, tag, i, mapping, lemma) {\n    this.term = term;\n    if (this.term.toLowerCase() != this.term) {\n        this.distinct = this.term.toLowerCase();\n    }\n    if (this.term.match(/^#\\w+/)) {\n        this.isHashtag = true;\n        this.term = this.term.substring(1);\n        if (this.distinct) {\n            this.distinct = this.distinct.substring(1);\n        }\n    }\n    this.tag = tag;\n    this.position = i;\n    this.children = [];\n    this.spacing = \" \";\n    this.negation = \"not\";\n    this.lemma = lemma;\n    this._updateState(mapping);\n}\n\nWord.prototype.copy = function (mapping) {\n    var result = new Word(this.term, this.tag, this.position, mapping, this.lemma);\n    for (var item in this) {\n        if (item == 'spacing' || item == 'negation' || item == 'next' || item == 'children' || item == 'orig' || typeof(item) == 'function')\n            continue;\n\n        result[item] = this[item];\n    }\n    return result;\n};\n\nWord.prototype.toJSON = function () {\n    var result = {};\n    for (var item in this) {\n        if (item == 'spacing' || item == 'negation' || item == 'next')\n            continue;\n\n        if (this[item] && typeof(this[item]) != 'function') {\n            if (item == 'children') {\n                if (this[item].length > 0) {\n                    result[item] = [];\n                    for (var i = 0; i < this[item].length; i++) {\n                        result[item][i] = this[item][i].toJSON();\n                    }\n                }\n            }\n            else if (item == 'orig') {\n                result[item] = this[item].toJSON();\n            }\n            else {\n                result[item] = this[item];\n            }\n        }\n    }\n    return result;\n};\n\nWord.prototype.format = function () {\n    return this.term + ' ' + this.tag + (this.isLink ? ' LINK': '') \n            + (this.isCop ? ' COP' : '')\n            + (this.isUrl ? ' URL' : '') \n            + (this.isMention ? ' MENTION' : '') \n            + (this.isHashtag ? ' HASHTAG' : '')\n            + (this.isNeg ? ' NEG' : '') \n            + (this.isTempOrTimeNoun ? ' TIMENOUN' : '');\n};\n\nWord.prototype.filter = function (mapping, ngramMapping) {\n    // collapse contraction\n    if (this.next && this.next.isContract) {\n        this.mergeLeft(mapping);\n        return 0;\n    }\n\n    // correct adp-non-scorable context\n    if (this.isSubjectPron && this.next && this.next.isAdpNonScorable) {\n        this.next.tag = \"VERB\";\n        this.next.isAdp = false;\n        this.next.isAdpNonScorable = false;\n    }\n\n    // check filtered items and assign them\n    var filters = this.checkFiltered(ngramMapping);\n    if (filters > -1) {\n        this.isFiltered = true;\n        var current = this;\n        for (var i = 0; i < filters; i++) {\n            if (current.next) {\n                current = current.next;\n            }\n            current.isFiltered = true;\n        }\n        return filters + 1;\n    }\n    else if (this.isAdpNonScorable) {\n        this.isFiltered = true;\n    }\n\n    return 1;\n};\n\nWord.prototype.collapse = function (mapping, ngramMapping) {\n    // check linkings to collapse\n    var times = this.checkMap(ngramMapping, 'link');\n    if (times > 0) {\n        for (var i = 0; i < times; i++) {\n            this.mergeLeft(mapping);\n        }\n        return true;\n    }\n    times = this.checkMap(ngramMapping, 'cop');\n    if (times > 0) {\n        for (var i = 0; i < times; i++) {\n            this.mergeLeft(mapping);\n        }\n        return true;\n    }\n\n    // Join all determiner cases where possible\n    if (this.isDet && this.next) {\n        var next = this.next;\n        if (next.isDet || next.isAdj || next.isAdv || next.isNoun || next.isVerb) {\n            this.mergeRight(mapping);\n            return true;\n        }\n        else if (next.isVerb && !next.isLink && next.next && (next.next.isAdj || next.next.isNoun)) {\n            this.mergeRight(mapping);\n            this.mergeRight(mapping);\n            return true;\n        }\n    }\n};\n\nWord.prototype.checkFiltered = function (mapping) {\n    var times = this.checkMap(mapping, 'stop');\n    if (times < 0) {\n        times = this.checkMap(mapping, 'names');\n    }\n    return times;\n};\n\nWord.prototype.checkMap = function (mapping, prop) {\n    // check quadgram for the term\n    var term = this.distinct || this.term;\n    if (this.next && this.next.next && this.next.next.next && mapping[prop][3].hasOwnProperty(term)) {\n        var nextTerm = this.next.distinct || this.next.term;\n        if (mapping[prop][3][term].hasOwnProperty(nextTerm)) {\n            var nextNextTerm = this.next.next.distinct || this.next.next.term;\n            if (mapping[prop][3][term][nextTerm].hasOwnProperty(nextNextTerm)) {\n                var nextNextNextTerm = this.next.next.next.distinct || this.next.next.next.term;\n                if (mapping[prop][3][term][nextTerm][nextNextTerm].hasOwnProperty(nextNextNextTerm)) {\n                    // successful stop-term found in quadrigram\n                    return 3;\n                }\n            }\n        }\n    }\n    // check trigram\n    if (this.next && this.next.next && mapping[prop][2].hasOwnProperty(term)) {\n        var nextTerm = this.next.distinct || this.next.term;\n        if (mapping[prop][2][term].hasOwnProperty(nextTerm)) {\n            var nextNextTerm = this.next.next.distinct || this.next.next.term;\n            if (mapping[prop][2][term][nextTerm].hasOwnProperty(nextNextTerm)) {\n                // successful stop-term found in trigram\n                return 2;\n            }\n        }\n    }\n    // check bigram\n    if (this.next && mapping[prop][1].hasOwnProperty(term)) {\n        var nextTerm = this.next.distinct || this.next.term;\n        if (mapping[prop][1][term].hasOwnProperty(nextTerm)) {\n            // successful stop-term found in trigram\n            return 1;\n        }\n    }\n    // check unigram\n    if (mapping[prop][0].hasOwnProperty(term)) {\n        // successful stop-term found in trigram\n        return 0;\n    }\n\n    return -1;\n};\n\nWord.prototype.chunk = function (mapping) {\n    if (this.isUrl || this.isMention || this.isHashtag)\n        return false;\n\n    // correct adj-link to noun-link\n    if (this.isAdjCard && this.next && this.next.isLink) {\n        this.tag = \"NOUN\";\n        this.isNoun = true;\n        delete this.isAdjCard;\n        delete this.isAdj;\n        return false;\n    }\n    \n    // join all to-phrase chunks\n    if (this.isTo && this.next && this.next.isVerb) {\n        this.mergeRight(mapping);\n        return true;\n    }\n\n    // join linking or copulae verb phrases\n    if (this.isLink && this.next && this.next.isLink) {\n        this.mergeLeft(mapping);\n        return true;\n    }\n\n    // join copulae verb phrases\n    if (this.isCop && this.next && this.next.isCop) {\n        this.mergeLeft(mapping);\n        return true;\n    }\n\n    // join verb phrases (non-linking)\n    if (this.isVerb && !this.isLink && this.next && this.next.isVerb && !this.next.isLink && !this.next.isAdv) {\n        this.mergeLeft(mapping);\n        return true;\n    }\n\n    // join verb-to-verb phrases (non-linking)\n    if (this.isVerb && !this.isLink && this.next && this.next.isTo && this.next.next && this.next.next.isVerb && \n        !this.next.next.isLink && !this.next.next.isAdv) {\n        this.mergeLeft(mapping);\n        this.mergeLeft(mapping);\n        return true;\n    }\n\n    // linking negation\n    if (this.isLink && this.next && this.next.isNeg) {\n        this.mergeLeft(mapping);\n        return true;\n    }\n\n    // negated verb\n    if (this.isNeg && !this.isLink && this.next && this.next.isVerb && !this.next.isLink && !this.next.isAdv) {\n        this.mergeRight(mapping);\n        return true;\n    }\n\n    // special case of linking verb (verb - temporal/time noun - verb) wherein (verb verb is a linking phrase with an injected timenoun)\n    // this is true for english in cases such as: should rarely be, should now be...etc\n    if (this.isVerb && this.next && this.next.isTempOrTimeNoun && this.next.next && this.next.next.isVerb) {\n        var bigramSpec = this.term + this.spacing + this.next.next.term;\n        if (this.checkLinking(mapping, bigramSpec)) {\n            this.mergeLeft(mapping);\n            this.mergeLeft(mapping);\n            this.isLink = true;\n            return true;\n        }\n    }\n\n    // adverb verb\n    if (this.isAdv && this.next && this.next.isVerb && !this.next.isLink) {\n        this.mergeRight(mapping);\n        return true;\n    }\n\n    // adverb adverb\n    if (this.isAdv && this.next && this.next.isAdv) {\n        this.mergeLeft(mapping);\n        return true;\n    }\n\n    // adverb prt\n    if (this.isAdv && this.next && this.next.isPrt) {\n        this.mergeLeft(mapping);\n        return true;\n    }\n\n    // adverb adject\n    if (this.isAdv && this.next && !this.next.isLink && this.next.isAdj) {\n        this.mergeRight(mapping);\n        return true;\n    }\n\n    // adverb noun\n    if (this.isAdv && this.next && this.next.isNoun && (!this.next.isHashtag && !this.next.isUrl && !this.next.isMention)) {\n        this.mergeRight(mapping);\n        return true;\n    }\n\n    // adj adj\n    if (this.isAdjCard && this.next && this.next.isAdjCard) {\n        this.mergeRight(mapping);\n        return true;\n    }\n\n    // adj noun\n    if (this.isAdjCard && this.next && this.next.isNoun) {\n        this.mergeRight(mapping);\n        return true;\n    }\n\n    // adj verb\n    if (this.isAdjCard && this.next && (this.next.isVerb || this.next.isAdv) && !this.next.isLink) {\n        if (this.next.next && this.next.next.isLink) {\n            this.mergeLeft(mapping);\n            return true;\n        }\n    }\n\n    // noun adj\n    if (this.isNoun && this.next && this.next.isAdjCard) {\n        this.mergeLeft(mapping);\n        return true;\n    }\n\n    // noun-noun\n    if (this.isNoun && this.next) {\n        var next = this.next;\n        if (next.isNoun && (!next.isHashtag && !next.isUrl && !next.isUrl)) {\n            if (this.isTempOrTimeNoun && next.isTempOrTimeNoun) {\n                this.mergeLeft(mapping);\n                return true;\n            }\n            else if (!this.isTempOrTimeNoun && !next.isTempOrTimeNoun) {\n                if (this.isNeg) {\n                    this.mergeRight(mapping);\n                    return true;\n                }\n                else {\n                    this.mergeLeft(mapping);\n                    return true;\n                }\n            }\n        }\n    }\n\n    return false;\n};\n\nWord.prototype.bigram = function () {\n    if (this.next) {\n        var format = this.term + this.spacing;\n        if (this.distinct)\n            format = this.distinct + this.spacing;\n        if (this.next.distinct)\n            format += this.next.distinct;\n        else\n            format += this.next.term;\n        return format;\n    }\n    else return \"\";\n};\n\nWord.prototype.trigram = function () {\n    if (this.next && this.next.next) return this.bigram() + this.spacing + this.next.next.term;\n    else return \"\";\n};\n\nWord.prototype.quadrigram = function () {\n    if (this.next && this.next.next && this.next.next.next) return this.trigram() + this.spacing + this.next.next.next.term;\n    else return \"\";\n};\n\nWord.prototype.fivegram = function () {\n    if (this.next && this.next.next && this.next.next.next && this.next.next.next.next) return this.quadrigram() + this.spacing + this.next.next.next.next.term;\n    else return \"\";\n};\n\nWord.prototype._updateState = function (mapping) {\n    var lowerTerm = this.term.toLowerCase();\n    this.isUrl = this.isUrl || this.term.indexOf('http') == 0;\n    this.isMention = this.hasOwnProperty('isMention') ? this.isMention : this.term.match(/^@[A-Za-z0-9\\-_]+/) ? true : false;\n    this.isPron = this.tag == 'PRON';\n    this.isAdv = this.tag == 'ADV';\n    this.isAdp = this.tag == 'ADP';\n    this.isPrt = this.tag == 'PRT';\n    this.isTo = this.hasOwnProperty('isTo') ? this.isTo : mapping['to'].hasOwnProperty(lowerTerm);\n    this.beginsDet = this.hasOwnProperty('beginsDet') ? this.beginsDet : this.isDet;\n    this.isVerb = this.isVerb || this.tag == 'VERB' || this.isLink || this.isCop;\n    this.isSubjectPron = this.hasOwnProperty('isSubjectPron') ? this.isSubjectPron : (this.isPron && mapping['subjp'].hasOwnProperty(lowerTerm));\n    this.isAdpNonScorable = this.hasOwnProperty('isAdpNonScorable') ? this.isAdpNonScorable : mapping['adp0'].hasOwnProperty(lowerTerm);\n    this.isContract = this.hasOwnProperty('isContract') ? this.isContract : mapping['contr'].hasOwnProperty(lowerTerm);\n    this.checkTerms(mapping, lowerTerm);\n    this.isConjOr = this.hasOwnProperty('isConjOr') ? this.isConjOr : (this.isConj && mapping['or'].hasOwnProperty(lowerTerm));\n};\n\nWord.prototype.checkTerms = function (mapping, term, inspectRight, takeLeft) {\n    var lowerTerm = term.toLowerCase();\n    this.isDet = this.tag == 'DET';\n    this.isConj = this.tag == 'CONJ';\n    this.isAdj = this.tag == 'ADJ';\n    this.isNum = this.tag == 'NUM';\n    this.isAdjCard = this.isNum || this.isAdj;\n    this.isQTerm = this.hasOwnProperty('isQTerm') ? this.isQTerm : mapping['ques'].hasOwnProperty(lowerTerm);\n    this.isQToken = this.hasOwnProperty('isQToken') ? this.isQToken : this.term.indexOf('?') > -1;\n    this.isNoun = this.tag == 'NOUN' || this.tag == 'PRON' || this.isUrl || this.isHashtag || this.isMention;\n    this.isNeg = this.isNeg || (inspectRight && this.next.isNeg) || this.checkNegation(mapping, lowerTerm);\n    this.isCop = inspectRight ? ((this.isCop && this.next.isCop) || this.checkCopulae(mapping, lowerTerm)) : this.isCop || this.checkCopulae(mapping, lowerTerm);\n    this.isLink = inspectRight ? ((this.isLink && this.next.isLink) || this.checkLinking(mapping, lowerTerm)) : this.isLink || this.checkLinking(mapping, lowerTerm);\n    this.isTempOrTimeNoun = inspectRight ? (takeLeft ? this.isTempOrTimeNoun : this.next.isTempOrTimeNoun) : (this.isTempOrTimeNoun || this.checkTimeNouns(mapping, lowerTerm));\n    this.isCoordNegation = this.hasOwnProperty('isCoordNegation') ? this.isCoordNegation : ((this.isConj || this.isAdv) && mapping['coor*'].hasOwnProperty(lowerTerm));\n    this.isClause = this.hasOwnProperty('isClause') ? this.isClause : (this.isAdp && mapping['clause'].hasOwnProperty(lowerTerm));\n    this.isAmplifier = ((this.isAdjCard || this.isAdv || this.isNoun || this.isPrt) && (this.next || this.children) && mapping['ampl'].hasOwnProperty(lowerTerm));\n};\n\nWord.prototype.checkTimeNouns = function (mapping, term) {\n    return mapping['timen'].hasOwnProperty(term);\n};\n\nWord.prototype.checkNegation = function (mapping, term) {\n    return mapping['*'].hasOwnProperty(term) || mapping['link*'].hasOwnProperty(term);\n};\n\nWord.prototype.checkLinking = function (mapping, term) {\n    return mapping['link'].hasOwnProperty(term) || mapping['link*'].hasOwnProperty(term);\n};\n\nWord.prototype.checkCopulae = function (mapping, term) {\n    return mapping['cop'].hasOwnProperty(term);\n};\n\nWord.prototype.checkStopTerm = function (mapping, term) {\n    return mapping['stop'].hasOwnProperty(term);\n};\n\nWord.prototype.mergeLeft = function (mapping) {\n    this.joinNext(mapping, true);\n};\n\nWord.prototype.mergeRight = function (mapping) {\n    this.joinNext(mapping, false);\n};\n\nWord.prototype.joinNext = function (mapping, takeLeft) {\n    if (this.next) {\n        if (this.children.length == 0)\n            this.orig = this.copy(mapping);\n\n        if (this.next.children.length == 0) {\n            this.children.push(this.next.copy(mapping));\n        }\n        else {\n            for (var i = 0; i < this.next.children.length; i++) {\n                this.children.push(this.next.children[i]);\n            }\n        }\n\n        var concat = false;\n        if (this.next.isContract)\n            concat = true;\n\n        if (!this.hasOwnProperty('termMap')) {\n            this.termMap = [];\n            this.termMap.push(this.term);\n        }\n\n        this.termMap.push(this.next.term);\n        this.term = concat ? (this.term + this.next.term) : (this.term + this.spacing + this.next.term);\n        if (this.distinct && this.next.distinct) {\n            this.distinct = concat ? (this.distinct + this.next.distinct) : (this.distinct + this.spacing + this.next.distinct);\n        }\n        else if (this.distinct && !this.next.distinct) {\n            this.distinct = concat ? (this.distinct + this.next.term) : (this.distinct + this.spacing + this.next.term);\n        }\n        else if (!this.distinct && this.next.distinct) {\n            this.distinct = concat ? (this.term + this.next.distinct) : (this.term + this.spacing + this.next.distinct);\n        }\n        if (this.lemma && this.next.lemma)\n            this.lemma = concat ? (this.lemma + this.next.lemma) : (this.lemma + this.spacing + this.next.lemma);\n\n        if (this.isDet && !takeLeft) {\n            this.distinct = this.next.distinct || this.next.term;\n        }\n\n        this.beginsDet = this.beginsDet || this.isDet;\n        this.beginsPron = this.beginsPron || this.isPron;\n        this.tag = takeLeft ? this.tag : this.next.tag;\n        this.checkTerms(mapping, this.term, true, takeLeft);\n        this.isAdv = takeLeft ? this.isAdv : this.next.isAdv;\n        this.isAdp = takeLeft ? this.isAdp : this.next.isAdp;\n        this.isPron = takeLeft ? this.isPron : this.next.isPron;\n        this.endsDet = this.next.isDet;\n        this.isVerb = takeLeft ? this.isVerb : this.next.isVerb;\n        this.isFiltered = this.isFiltered && this.next.isFiltered;\n        if (!this.isFiltered) {\n            delete this.isFiltered;\n        }\n\n        // Forward link to the following word after that\n        var newNext = this.next.next;\n        if (!newNext) {\n            delete this.next;\n        }\n        else {\n            this.next = newNext;\n        }\n    }\n};\n\nmodule.exports = Word;\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/sentiment/bayesanalyser.js":"\nvar util = require('util'),\n    path = require('path'),\n    fs = require('fs');\n\nvar Glossary = require('./../glossary/glossary');\n\nfunction BayesSentimentAnalyser(options) {\n    this.options = options || {};\n    this.lang = this.options.lang || \"en\";\n    this.glossary = new Glossary(this.options);\n    this.spacing = this.options.spacing || \" \";\n    this.mapping = this._loadMapping(this.options.sentimentMap || path.join(__dirname, '../../../bin/glossdata/' + this.lang + '.sentiment.map'));\n    this.ngramMapping = this._nGramMap(this.mapping);\n    this.tagTime = 0.0;\n    this.classifyTime = 0.0;\n};\n\nBayesSentimentAnalyser.prototype._nGramMap = function (mapping) {\n    var unigramMapping = {};\n    var bigramMapping = {};\n    var trigramMapping = {};\n    for (var term in mapping) {\n        var terms = term.split(this.spacing);\n        if (terms.length == 2) {\n            if (!bigramMapping.hasOwnProperty(terms[0])) {\n                bigramMapping[terms[0]] = {};\n            }\n            bigramMapping[terms[0]][terms[1]] = mapping[term];\n        }\n        else if (terms.length == 3) {\n            if (!trigramMapping.hasOwnProperty(terms[0])) {\n                trigramMapping[terms[0]] = {};\n            }\n            if (!trigramMapping[terms[0]].hasOwnProperty(terms[1])) {\n                trigramMapping[terms[0]][terms[1]] = {};\n            }\n            trigramMapping[terms[0]][terms[1]][terms[2]] = mapping[term];\n        }\n        else {\n            unigramMapping[terms] = mapping[term];\n        }\n    }\n\n    return [unigramMapping, bigramMapping, trigramMapping];\n};\n\nBayesSentimentAnalyser.prototype._loadMapping = function (mapFile) {\n    var mapping = {};\n    var lines = fs.readFileSync(mapFile).toString().split('\\n');\n    for (var i = 0; i < lines.length; i++) {\n        var line = lines[i];\n        if (!line || line.trim().length == 0)\n            continue;\n\n        var items = line.trim().toLowerCase().split('\\t');\n        if (items.length == 2) {\n            var score = parseInt(items[0]);\n            var terms = items[1].split(',');\n            for (var t = 0; t < terms.length; t++) {\n                var term = terms[t];\n                mapping[term] = score;\n                if (this.glossary.lem) {\n                    var lemItems = this.glossary.lem.lemmatize(term);\n                    if (lemItems.length > 0) {\n                        var lemTerm = lemItems[0].text.toLowerCase();\n                        if (lemTerm != term) {\n                            mapping[lemTerm] = score;\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    return mapping;\n};\n\nBayesSentimentAnalyser.prototype._tagScorable = function (text) {\n    this.glossary.parse(text);\n\n    var current = this.glossary.root;\n    var totalScored = 0.0;\n    var total = 0.0;\n    var time = new Date().getTime();\n    do {\n        if (!current) {\n            break;\n        }\n\n        // check bigrams (distinct or simple bigram)\n        var bigramSkip = false;\n        var distinctBigram = null;\n        if (current.termMap && current.termMap.length > 1 && current.next) {\n            var last = current.termMap[current.termMap.length - 1];\n            var next = current.next.term;\n            if (current.next.distinct) {\n                next = current.next.distinct;\n            }\n\n            distinctBigram = last + current.spacing + next;\n        }\n\n        var embeddedBigrams = {};\n        var foundEmbedded = false;\n        if (current.termMap && current.termMap.length > 1) {\n            for (var i = 0; i < current.termMap.length - 1; i++) {\n                var embeddedBigram = current.termMap[i] + current.spacing + current.termMap[i+1];\n                if (this.mapping.hasOwnProperty(embeddedBigram)) {\n                    embeddedBigrams[embeddedBigram] = this.mapping[embeddedBigram];\n                    foundEmbedded = true;\n                }\n            }\n        }\n\n        var bigram = current.bigram();\n        if (this.mapping.hasOwnProperty(bigram)) {\n            var score = this.mapping[bigram];\n            current.score = score / 2.0;\n            current.isScorable = true;\n            current.next.score = score / 2.0;\n            current.next.isScorable = true;\n            totalScored += 2;\n            total += score;\n            current = current.next;\n            bigramSkip = true;\n        }\n        // use the term map to iterate over bigrams beginning at the end of the term map\n        else if (this.mapping.hasOwnProperty(distinctBigram)) {\n            var score = this.mapping[distinctBigram];\n            current.score = score / 2.0;\n            current.isScorable = true;\n            current.next.score = score / 2.0;\n            current.next.isScorable = true;\n            totalScored += 2;\n            total += score;\n            current = current.next;\n            bigramSkip = true;\n        }\n\n        if (!current.isFiltered && !bigramSkip) {\n            if (this.mapping.hasOwnProperty(current.term)) {\n                var score = this.mapping[current.term];\n                current.score = score;\n                current.isScorable = true;\n                totalScored++;\n                total += score;\n            }\n            else if (current.lemma && this.mapping.hasOwnProperty(current.lemma)) {\n                var score = this.mapping[current.lemma];\n                current.score = score;\n                current.isScorable = true;\n                totalScored++;\n                total += score;\n            }\n            else if (current.distinct && this.mapping.hasOwnProperty(current.distinct)) {\n                var score = this.mapping[current.distinct];\n                current.score = score;\n                current.isScorable = true;\n                totalScored++;\n                total += score;\n            }\n            else if (current.children && current.children.length > 0) {\n                var localCount = 0.0;\n                var localScore = 0.0;\n                var count = 0.0;\n\n                // check the original term (count towards score is it is not a negation term)\n                if (current.orig && !current.orig.isFiltered && !current.orig.isNeg) {\n                    count++;\n                    var score = null;\n                    if (this.mapping.hasOwnProperty(current.orig.term)) {\n                        score = this.mapping[current.orig.term];\n                    }\n                    else if (current.orig.lemma && this.mapping.hasOwnProperty(current.orig.lemma)) {\n                        score = this.mapping[current.orig.lemma];\n                    }\n                    else if (current.orig.distinct && this.mapping.hasOwnProperty(current.orig.distinct)) {\n                        score = this.mapping[current.orig.distinct];\n                    }\n\n                    if (score) {\n                        current.orig.score = score;\n                        current.orig.isScorable = true;\n                        current.isScorable = true;\n                        localScore += score;\n                        localCount++;\n                        totalScored++;\n                        total += score;\n                    }\n                }\n\n                // iterate over the child elements\n                for (var i = 0; i < current.children.length; i++) {\n                    count++;\n                    var child = current.children[i];\n                    if (child.isFiltered || child.isNeg) {\n                        if (child.isNeg && current.orig && current.orig.isNeg && current.isNeg) {\n                            // negated negation (i.e. don't forget..etc)\n                            delete current.isNeg;\n                        }\n                        continue;\n                    }\n\n                    var score = null;\n                    if (this.mapping.hasOwnProperty(child.term)) {\n                        score = this.mapping[child.term];\n                    }\n                    else if (child.lemma && this.mapping.hasOwnProperty(child.lemma)) {\n                        score = this.mapping[child.lemma];\n                    }\n                    else if (child.distinct && this.mapping.hasOwnProperty(child.distinct)) {\n                        score = this.mapping[child.distinct];\n                    }\n\n                    if (score) {\n                        if (i == 0 && current.orig && current.orig.isAmplifier && current.orig.score < 0) {\n                            current.orig.score *= -1;\n                            localScore += (current.orig.score); // correct for amplified context\n                            total += (current.orig.score); // correct for amplified context\n                            score *= current.orig.score / 2.0;\n                        }\n                        else if (i > 0 && current.children[i - 1].isAmplifier && current.children[i - 1].score < 0) {\n                            current.children[i - 1].score *= -1;\n                            localScore += (current.children[i - 1].score); // correct for amplified context\n                            total += (current.children[i - 1].score); // correct for amplified context\n                            score *= current.children[i - 1].score / 2.0;\n                        }\n                        child.score = score;\n                        child.isScorable = true;\n                        current.isScorable = true;\n                        localScore += score;\n                        localCount++;\n                        totalScored++;\n                        total += score;\n                    }\n                }\n\n                // set the current score to the relative distribution\n                if (localCount > 0) {\n                    current.isScorable = true;\n                    current.score = (localScore / Math.max(localCount, 1.0));\n                }\n                else if (foundEmbedded) {\n                    var totalScore = 0.0;\n                    var totalFound = 0.0;\n                    for (var e in embeddedBigrams) {\n                        var score = embeddedBigrams[e];\n                        totalFound++;\n                        totalScore += score;\n                    }\n                    current.score = totalScore / totalFound;\n                    current.isScorable = true;\n                    totalScored++;\n                    total += current.score;\n                }\n            }\n        }\n        current = current.next;\n    } while (current);\n\n    this.tagTime += (new Date().getTime() - time);\n    return { terms: totalScored, score: total, value: (score / Math.max(totalScored, 1.0)) };\n};\n\nBayesSentimentAnalyser.prototype.classify = function (text) {\n    this._tagScorable(text);\n    var endTime = new Date().getTime();\n\n    var semClauseCursor;\n    var semCursor;\n    var scoredTerms = 0.0;\n    var current = this.glossary.root;\n    var negation = false;\n    while (current) {\n        if (current.isVerb) {\n            var orientation = 1.0;\n            var score = 0.0;\n            if (current.isNeg) {\n                orientation = -1.0;\n                if (semCursor && semCursor.isNeg) {\n                    orientation = 1.0;\n                }\n            }\n            else if (semCursor && semCursor.semOrientation) {\n                orientation = semCursor.semOrientation;\n            }\n\n            if (current.isScorable) {\n                score += current.score * orientation;\n                scoredTerms++;\n            }\n\n            current.semScore = score;\n            current.semOrientation = orientation;\n\n            if (semClauseCursor && current.semScore != 0.0 && semClauseCursor.semScore) {\n                var amplifier = Math.abs(semClauseCursor.semScore);\n                var clauseOrientation = semClauseCursor.semOrientation ? semClauseCursor.semOrientation : 1.0;\n                current.semScore *= (amplifier * clauseOrientation);\n                semClauseCursor = null;\n            }\n\n            semCursor = current;\n            if (current.next && (current.next.isClause || current.next.isQTerm)) {\n                semClauseCursor = current;\n            }\n            \n            if (current.next && current.next.isTo) {\n                current = current.next.next;\n            }\n            else {\n                current = current.next;\n            }\n            continue;\n        }\n        else if (current.isScorable) {\n            // maintain context only in cases where the verb phrase of a semantic cursor is eliticing emotional context\n            var maintainContext = false;\n            if (semCursor && semCursor.isVerb && semCursor.children && semCursor.children.length > 0 && \n                semCursor.children[semCursor.children.length - 1].isScorable) {\n                    maintainContext = true;\n            }\n\n            if (!maintainContext && current.isNoun && current.isScorable && current.beginsDet && semCursor && !semClauseCursor) {\n                if (current.isNeg)\n                    current.semOrientation = -1;\n                else\n                    current.semOrientation = 1;\n\n                current.semScore = current.score * current.semOrientation;\n                scoredTerms++;\n                current = current.next;\n                continue;\n            }\n\n            if (semCursor && semCursor.semOrientation) {\n                current.semOrientation = semCursor.semOrientation;\n            }\n            else {\n                current.semOrientation = 1;\n            }\n\n            if (current.isNeg) {\n                current.semOrientation = -1;\n                if (semCursor && semCursor.isNeg) {\n                    current.semOrientation = 1.0;\n                }\n            }\n\n            current.semScore = current.score * current.semOrientation;\n            // correct when the current semantic cursor has a positive orientation and should adjust the negative orientation of this term\n            if (current.score < 0 && current.semOrientation == 1 && semCursor && semCursor.score > 0 && semCursor.semOrientation == 1) {\n                current.origSemScore = current.semScore;\n                current.semScore *= -1;\n            }\n            scoredTerms++;\n\n            if (semClauseCursor) {\n                var amplifier = Math.abs(semClauseCursor.semScore);\n                var clauseOrientation = semClauseCursor.semOrientation ? semClauseCursor.semOrientation : 1.0;\n                if (amplifier) {\n                    current.semScore *= amplifier * clauseOrientation;\n                }\n                else if (clauseOrientation) {\n                    current.semScore *= clauseOrientation;\n                }\n                semClauseCursor = null;\n            }\n\n            if (current.next && (current.next.isClause || current.next.isQTerm)) {\n                semClauseCursor = current;\n            }\n            else if (current.next && current.next.isConjOr) {\n                semCursor = current;\n                current = current.next;\n                continue;\n            }\n            else if (current.next && current.next.isTo) {\n                semCursor = current;\n                current = current.next;\n                continue;\n            }\n        }\n        else if (current.isNeg) {\n            current.semOrientation = -1;\n            if (semCursor && semCursor.isNeg) {\n                current.semOrientation = 1;\n            }\n            semCursor = current;\n            current = current.next;\n            continue;\n        }\n        else if (current.isConjOr && semCursor) {\n            current.semOrientation = semCursor.semOrientation;\n            semCursor = current;\n            current = current.next;\n            continue;\n        }\n        else if ((current.isAdp || current.isConj) && semCursor) {\n            current.semOrientation = semCursor.semOrientation;\n            semCursor = current;\n            current = current.next;\n            continue;\n        }\n        else if (current.next && current.next.isConj && semCursor) {\n            current = current.next;\n            continue;\n        }\n        else if (current.isTo && semCursor) {\n            current = current.next;\n            continue;\n        }\n        else if (current.isNoun && semCursor && semCursor.semOrientation && current.next && current.next.isScorable) {\n            current = current.next;\n            continue;\n        }\n\n        semCursor = null;\n        current = current.next;\n    }\n\n    var polarity = 0.0;\n    if (scoredTerms > 0) {\n        var current = this.glossary.root;\n        var totalScore = 0.0;\n        var finalOrientation = 1;\n        while (current) {\n            if (current.isAmplifier && current.semScore && current.next && current.next.semScore) {\n                totalScore += (Math.abs(current.semScore) * current.next.semScore) / 2.0;\n            }\n            else if (current.semScore) {\n                totalScore += current.semScore;\n                if (current.isHashtag && current.origSemScore) {\n                    current.semScore = current.origSemScore;\n                    delete current.origSemScore;\n                }\n                if (totalScore >= 0 && current.semScore < 0 && current.isHashtag) {\n                    finalOrientation = -1;\n                }\n                else if (totalScore <= 0 && current.semScore > 0 && current.isHashtag) {\n                    finalOrientation = 0;\n                }\n            }\n            else if (current.isHashtag && current.semOrientation == -1) {\n                finalOrientation = -1;\n            }\n            current = current.next;\n        }\n        if ((finalOrientation == -1 && totalScore >= 0) || (finalOrientation == 0 && totalScore < 0)) {\n            // typically tweets include hashtags that often signify ironic twists\n            // or summarize the overall sentiment of an entire text, use this \n            // as a key for further classification\n            totalScore *= -1;\n            if (totalScore == 0) {\n                totalScore = -1;\n            }\n\n            this.glossary.isNegatedOrientation = true;\n        }\n        polarity = totalScore / scoredTerms;\n    }\n\n    this.classifyTime += ((new Date().getTime() - endTime));\n\n    return polarity;\n};\n\nmodule.exports = BayesSentimentAnalyser;\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/sentiment/sentiwordnet_analyser.js":"\nvar util = require('util'),\n    path = require('path'),\n    fs = require('fs');\n\nvar Glossary = require('./../glossary/glossary');\nvar BayesSentimentAnalyser = require('./bayesanalyser');\n\nfunction SentiwordnetAnalyser(options) {\n    this.options = options || {};\n    this.lang = this.options.lang || \"en\";\n    this.glossary = new Glossary(this.options);\n    this.spacing = this.options.spacing || \" \";\n    this.posMap = { 'a': 'ADJ', 'n': 'NOUN', 'r': 'ADV', 'v': 'VERB' };\n    this.mapping = this._loadMapping(this.options.sentimentMap || path.join(__dirname, '../../../bin/glossdata/' + this.lang + '.sentwordnet.map'));\n    this.ngramMapping = this._nGramMap(this.mapping);\n    this.tagTime = 0.0;\n    this.classifyTime = 0.0;\n};\n\nutil.inherits(SentiwordnetAnalyser, BayesSentimentAnalyser);\n\nSentiwordnetAnalyser.prototype._loadMapping = function (mapFile) {\n    var mapping = {}; // term: { pos: avgposscore-avgnegscore }\n    var lines = fs.readFileSync(mapFile).toString().split('\\n');\n    for (var i = 0; i < lines.length; i++) {\n        var line = lines[i];\n        if (!line || line.indexOf('#') == 0 || line.trim().length == 0) {\n            continue;\n        }\n\n        // # POS   ID      PosScore        NegScore        SynsetTerms     Gloss\n        var items = line.trim().split('\\t');\n        var pos = items[0];\n        pos = this.posMap[pos];\n        var posScore = parseFloat(items[2]);\n        var negScore = parseFloat(items[3]);\n        var synsetTerms = items[4];\n\n        if (!synsetTerms) {\n            console.log(line);\n            continue;\n        }\n        var terms = synsetTerms.split(' ');\n        for (var t = 0; t < terms.length; t++) {\n            var term = terms[t].split('#')[0];\n            // multi_word_phrase\n            term = term.split('_').join(this.glossary.spacing);\n            if (!mapping.hasOwnProperty(term)) {\n                mapping[term] = {};\n            }\n            if (!mapping[term].hasOwnProperty(pos)) {\n                mapping[term][pos] = { 'pos': [], 'neg': [] };\n            }\n            mapping[term][pos].pos.push(posScore);\n            mapping[term][pos].neg.push(negScore);\n        }\n    }\n\n    // Iterate over all terms to calculate the average score\n    for (var term in mapping) {\n        for (var pos in mapping[term]) {\n            var item = mapping[term][pos];\n            var avgPositive = 0.0;\n            var avgNegative = 0.0;\n            if (item.pos && item.pos.length > 0) {\n                avgPositive = item.pos.reduce(function (a, b) { return a + b; }) / item.pos.length;\n            }\n            if (item.neg && item.neg.length > 0) {\n                avgNegative = item.neg.reduce(function (a, b) { return a + b; }) / item.neg.length;\n            }\n\n            var result = avgPositive - avgNegative;\n            mapping[term][pos] = result;\n        }\n    }\n\n    return mapping;\n};\n\nSentiwordnetAnalyser.prototype._tagScorable = function (text) {\n    this.glossary.parse(text);\n\n    var current = this.glossary.root;\n    var totalScored = 0.0;\n    var total = 0.0;\n    var time = new Date().getTime();\n    do {\n        if (!current) {\n            break;\n        }\n\n        // check bigrams (distinct or simple bigram)\n        var bigramSkip = false;\n        var distinctBigram = null;\n        if (current.termMap && current.termMap.length > 1 && current.next && current.next.term) {\n            var last = current.termMap[current.termMap.length - 1];\n            var next = current.next.term;\n            if (current.next.distinct) {\n                next = current.next.distinct;\n            }\n\n            distinctBigram = last + current.spacing + next;\n        }\n\n        var bigram = current.bigram();\n        if (bigram && this.mapping.hasOwnProperty(bigram)) {\n            var speechScore = this.mapping[bigram];\n            var score;\n            if (speechScore.hasOwnProperty(current.tag)) {\n                score = speechScore[current.tag];\n            }\n            else if (speechScore.hasOwnProperty(current.next.tag)) {\n                score = speechScore[current.tag];\n            }\n            if (score) {\n                current.score = score / 2.0;\n                current.isScorable = true;\n                current.next.score = score / 2.0;\n                current.next.isScorable = true;\n                totalScored += 2;\n                total += score;\n                current = current.next;\n                bigramSkip = true;\n            }\n        }\n        // use the term map to iterate over bigrams beginning at the end of the term map\n        else if (distinctBigram && this.mapping.hasOwnProperty(distinctBigram)) {\n            var speechScore = this.mapping[distinctBigram];\n            var score;\n            if (!current.next) {\n                console.log(distinctBigram);\n            }\n            if (speechScore.hasOwnProperty(current.tag)) {\n                score = speechScore[current.tag];\n            }\n            else if (speechScore.hasOwnProperty(current.next.tag)) {\n                score = speechScore[current.next.tag];\n            }\n            if (score) {\n                current.score = score / 2.0;\n                current.isScorable = true;\n                current.next.score = score / 2.0;\n                current.next.isScorable = true;\n                totalScored += 2;\n                total += score;\n                current = current.next;\n                bigramSkip = true;\n            }\n        }\n\n        if (!current.isFiltered && !bigramSkip) {\n            if (this.mapping.hasOwnProperty(current.term) && this.mapping[current.term].hasOwnProperty(current.tag)) {\n                var score = this.mapping[current.term][current.tag];\n                current.score = score;\n                current.isScorable = true;\n                totalScored++;\n                total += score;\n            }\n            else if (current.lemma && this.mapping.hasOwnProperty(current.lemma) && this.mapping[current.lemma].hasOwnProperty(current.tag)) {\n                var score = this.mapping[current.lemma][current.tag];\n                current.score = score;\n                current.isScorable = true;\n                totalScored++;\n                total += score;\n            }\n            else if (current.distinct && this.mapping.hasOwnProperty(current.distinct) && this.mapping[current.distinct].hasOwnProperty(current.tag)) {\n                var score = this.mapping[current.distinct][current.tag];\n                current.score = score;\n                current.isScorable = true;\n                totalScored++;\n                total += score;\n            }\n            else if (current.children && current.children.length > 0) {\n                var localCount = 0.0;\n                var localScore = 0.0;\n                var count = 0.0;\n\n                // check the original term (count towards score is it is not a negation term)\n                if (current.orig && !current.orig.isFiltered && !current.orig.isNeg) {\n                    count++;\n                    var score = null;\n                    if (this.mapping.hasOwnProperty(current.orig.term) && this.mapping[current.orig.term].hasOwnProperty(current.orig.tag)) {\n                        score = this.mapping[current.orig.term][current.orig.tag];\n                    }\n                    else if (current.orig.lemma && this.mapping.hasOwnProperty(current.orig.lemma) && this.mapping[current.orig.lemma].hasOwnProperty(current.orig.tag)) {\n                        score = this.mapping[current.orig.lemma][current.orig.tag];\n                    }\n                    else if (current.orig.distinct && this.mapping.hasOwnProperty(current.orig.distinct) && this.mapping[current.orig.distinct].hasOwnProperty(current.orig.tag)) {\n                        score = this.mapping[current.orig.distinct][current.orig.tag];\n                    }\n\n                    if (score) {\n                        current.orig.score = score;\n                        current.orig.isScorable = true;\n                        current.isScorable = true;\n                        localScore += score;\n                        localCount++;\n                        totalScored++;\n                        total += score;\n                    }\n                }\n\n                // iterate over the child elements\n                for (var i = 0; i < current.children.length; i++) {\n                    count++;\n                    var child = current.children[i];\n                    if (child.isFiltered || child.isNeg) {\n                        continue;\n                    }\n\n                    var score = null;\n                    if (this.mapping.hasOwnProperty(child.term) && this.mapping[child.term].hasOwnProperty(child.tag)) {\n                        score = this.mapping[child.term][child.tag];\n                    }\n                    else if (child.lemma && this.mapping.hasOwnProperty(child.lemma) && this.mapping[child.lemma].hasOwnProperty(child.tag)) {\n                        score = this.mapping[child.lemma][child.tag];\n                    }\n                    else if (child.distinct && this.mapping.hasOwnProperty(child.distinct) && this.mapping[child.distinct].hasOwnProperty(child.tag)) {\n                        score = this.mapping[child.distinct][child.tag];\n                    }\n\n                    if (score) {\n                        child.score = score;\n                        child.isScorable = true;\n                        current.isScorable = true;\n                        localScore += score;\n                        localCount++;\n                        totalScored++;\n                        total += score;\n                    }\n                }\n\n                // set the current score to the relative distribution\n                if (localCount > 0) {\n                    current.isScorable = true;\n                    current.score = (localScore / Math.max(count, 1.0));\n                }\n            }\n        }\n        current = current.next;\n    } while (current);\n\n    this.tagTime += (new Date().getTime() - time);\n    return { terms: totalScored, score: total, value: (score / Math.max(totalScored, 1.0)) };\n};\n\nmodule.exports = SentiwordnetAnalyser;\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/crawlers/content_crawler.js":"\nvar events = require('events'),\n    request\t= require('request'),\n    async = require('async'),\n    util = require('util'),\n    progress = require('request-progress'),\n    Stream = require('stream'),\n    fs = require('fs'),\n    JSONStream = require('JSONStream');\n\nvar rateLimiter = require('./ratelimiter');\n\nfunction ContentCrawler(options) {\n    Stream.call(this);\n    var self = this;\n\n    this.options = options || {};\n    this.readable = true;\n    this.writable = true;\n    this._q = async.queue(function (q, cb) { self._process(q, cb); }, this.options.concurrency || 1);\n    this._q.drain = function () { self.end(); };\n    rateLimiter.call(this._q);\n    this._q.rateLimit(this.options.rateLimit || 1000);\n    this._q.resume();\n}\n\nutil.inherits(ContentCrawler, Stream);\n\nContentCrawler.prototype._process = function (queueItem, callback) {\n    var self = this;\n    self.emit('download-begin', queueItem);\n    request.get(queueItem.loc, function (err, response, body) {\n        self.emit('download-end', queueItem);\n        if (err) {\n            callback(err);\n        } else {\n            self.emit('data', [body, queueItem.loc]);\n            callback();\n        }\n    });\n};\n\nContentCrawler.prototype.write = function (data) {\n    this._q.push(data);\n};\n\nContentCrawler.prototype.crawl = function (files) {\n    for (var i = 0; i < files.length; i++) {\n        var fileStream = fs.createReadStream(files[i]);\n        fileStream.pipe(JSONStream.parse()).pipe(this);\n    }\n};\n\nContentCrawler.prototype.end = function () {\n    this.emit('end');\n};\n\nmodule.exports = ContentCrawler;\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/bin/browncorpus.js":"\nvar path = require('path');\nvar salient = require('./../');\nvar BrownCorpus = salient.corpus.BrownCorpus;\n\nvar args = process.argv;\nif (!args || args.length == 2) {\n    console.log('usage: node browncorpus.js ~/Public/brown --lines=10000 --skip=10000');\n    return;\n}\n\nvar file = args[2];\nvar outputDir = __dirname;\nvar lines = 0;\nvar skip = 0;\nif (args.length > 3) {\n    for (var i = 0; i < args.length; i++) {\n        if (args[i].indexOf('--') == 0) {\n            var item = args[i].split('=');\n            if (item[0] == '--lines') {\n                lines = parseInt(item[1]);\n            }\n            else if (item[0] == '--skip') {\n                skip = parseInt(item[1]);\n            }\n        }\n    }\n}\n\nvar brown = new BrownCorpus(file);\nbrown.skipLines = skip;\nbrown.limitLines = lines;\nbrown.output = path.join(outputDir, 'en-brown.tag.vocab');\nbrown.outputDist = path.join(outputDir, 'en-brown.tag.dist');\nbrown.outputSentences = path.join(outputDir, 'en-brown.sentences');\nbrown.parse();\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/bin/corpuscoverage.js":"\nvar path = require('path'),\n    util = require('util'),\n    fs = require('fs');\n\nvar salient = require('./../');\n\nvar args = process.argv;\nif (!args || args.length < 5) {\n    console.log('usage: node corpuscoverage en.wik.dist en.wik.vocab en-brown.tag.vocab');\n    return;\n}\n\nvar wikDistFile = args[2];\nvar wikVocabFile = args[3];\nvar corpusVocabFile = args[4];\n\nfunction readDist(file) {\n    var dist = {};\n    var lines = fs.readFileSync(file).toString().split('\\n');\n    for (var l = 0; l < lines.length; l++) {\n        var line = lines[l];\n        if (line.indexOf('#') == 0) {\n            continue;\n        }\n\n        var items = line.split('\\t');\n        if (items.length != 4) {\n            continue;\n        }\n\n        var i = parseInt(items[0]);\n        var tag = items[1];\n        var freq = items[2];\n        dist[tag] = { i: i, t: tag, f: freq };\n    }\n\n    return dist;\n};\n\nfunction readDict(file, corpus) {\n    var vocab = {};\n    var lines = fs.readFileSync(file).toString().split('\\n');\n    for (var l = 0; l < lines.length; l++) {\n        var line = lines[l];\n        var items = line.split('\\t');\n        if (!items || items.length < 2)\n            continue;\n\n        if (corpus) {\n            // TOKEN FREQUENCY POS/FREQ,POS/FREQ OR TOKEN FREQUENCY\n            var w = items[0].toLowerCase();\n            vocab[w] = { freq: parseInt(items[1]), id: l.toString(), w: w }\n            if (items.length > 2) {\n                vocab[w].pos = items[2];\n            }\n        }\n        else if (items.length > 2) {\n            // ID TOKEN POS,POS\n            var w = items[1].toLowerCase();\n            vocab[w] = { pos: items[2], id: items[0], w: w };\n        }\n    }\n\n    vocab._length = lines.length;\n    return vocab;\n}\n\nfunction mapCorpus(vocab, dist) {\n    for (var v in vocab) {\n        var pos = vocab[v].pos;\n        if (pos) {\n            var result = [];\n            var resultFreq = [];\n            var items = pos.split(',');\n            for (var i = 0; i < items.length; i++) {\n                var item = items[i].split('/');\n                var tag = item[0];\n                var tagFreq = item[1];\n                if (typeof dist[tag] != 'undefined') {\n                    var tagId = dist[tag].i;\n                    if (result.indexOf(tagId) < 0) {\n                        result.push(tagId);\n                        resultFreq.push(tagFreq);\n                    }\n                }\n            }\n            vocab[v].pos = result.join(',');\n            vocab[v].posFreq = resultFreq.join(',');\n        }\n    }\n}\n\nvar dict = readDict(wikVocabFile, false);\nvar corpusV = readDict(corpusVocabFile, true);\nvar distribution = readDist(wikDistFile);\n\nmapCorpus(corpusV, distribution);\n\n// Determine whether the corpus is a subset, supset, overlap or disjoint set of the vocabulary\nvar corpusCovered = 0;\nvar corpusUncovered = 0;\nvar uncoveredOutput = path.join(__dirname, 'uncovered.corpus.vocab');\nvar coveredOutput = path.join(__dirname, 'covered.corpus.vocab');\nvar uncoveredSorted = [];\nvar coveredSorted = [];\nfor (var c in corpusV) {\n    if (typeof dict[c] != 'undefined') {\n        coveredSorted.push(corpusV[c]);\n        corpusCovered++;\n    }\n    else {\n        var item = corpusV[c];\n        // determine if the item falls under a numeric or money category ala regex\n        if (c.match(/\\$?\\d+/)) {\n            continue;\n        }\n\n        uncoveredSorted.push(item);\n        corpusUncovered++;\n    }\n}\n\nuncoveredSorted = uncoveredSorted.sort(function (a, b) { return b.freq - a.freq });\nfor (var i = 0; i < corpusUncovered; i++) {\n    var item = uncoveredSorted[i];\n    var additionLine = util.format('%s\\t%s\\t%s\\t%s\\n', dict._length + i, item.w, item.pos, item.posFreq);\n    fs.appendFileSync(uncoveredOutput, additionLine);\n}\n\ncoveredSorted = coveredSorted.sort(function (a, b) { return b.freq - a.freq });\nfor (var i = 0; i < corpusCovered; i++) {\n    var item = coveredSorted[i];\n    var additionLine = util.format('%s\\t%s\\t%s\\t%s\\n', dict._length + i, item.w, item.pos, item.posFreq);\n    fs.appendFileSync(coveredOutput, additionLine);\n}\n\n\n// Determine whether the vocabulary is a subset, supset, overlap or disjoint set of the corpus\nvar countCovered = 0;\nvar countUncovered = 0;\nfor (var c in dict) {\n    if (typeof corpusV[c] != 'undefined') {\n        countCovered++;\n    }\n    else {\n        countUncovered++;\n    }\n}\n\nconsole.log(util.format('%s found in vocab, %s not found in vocab. %s% covered', corpusCovered,\n            corpusUncovered, Math.round(100 * corpusCovered / (corpusCovered + corpusUncovered))));\n\nconsole.log(util.format('%s found in corpus, %s not found in corpus. %s% covered', countCovered,\n            countUncovered, Math.round(100 * countCovered / (countCovered + countUncovered))));\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/bin/crawl.js":"\nvar fs = require('fs'),\n    path = require('path');\n\nvar JSONStream = require('JSONStream');\nvar salient = require('./../');\n\nvar args = require('minimist')(process.argv);\nif (!args.sitemaps && !args.content && args._.length < 2) {\n    console.log('Usage: node crawl.js --sitemaps=true --concurrency=2 --output=tmp.txt http://www.example.com/sitemap.xml');\n    console.log('\\t node crawl.js --content=true ./tmp.txt --delayms=500');\n    console.log('\\t node crawl.js --content=true --contentdir=./dump/ --delayms=500 --offset=1000 ./tmp.txt');\n    console.log('\\t node crawl.js --state=true ./state.crawl');\n    return;\n}\n\n\n// Setup additional command line options\nif (args.state) {\n    var file = fs.readFileSync(args._.slice(2)[0]);\n    var fileState = JSON.parse(file.toString());\n    if (fileState.args) {\n        args = fileState.args;\n        if (fileState.index) {\n            args.offset = fileState.index;\n        }\n    }\n}\n\nvar options = {};\nif (args.concurrency) {\n    options.concurrency = args.concurrency;\n}\nif (args.delayms) {\n    options.rateLimit = args.delayms;\n}\n\nvar crawler = null;\nfunction downloadBegin(queueItem) {\n    process.stdout.clearLine();\n    process.stdout.cursorTo(0);\n    process.stdout.write('Crawling ' + queueItem.loc + \"...\");\n};\nfunction downloadProgress(queueItem, state) {\n    process.stdout.clearLine();\n    process.stdout.cursorTo(0);\n    process.stdout.write(\"Downloading \" + queueItem.loc + \" [\" + state.received + \" bytes]\");\n};\nfunction downloadEnd(queueItem) {\n    process.stdout.clearLine();\n    process.stdout.cursorTo(0);\n    process.stdout.write('Processing ' + queueItem.loc + ' content...');\n};\n\n\nif (args.content) {\n    crawler = new salient.crawlers.ContentCrawler(options);\n}\nelse if (args.sitemaps) {\n    crawler = new salient.crawlers.SitemapCrawler(options);\n}\n\ncrawler.on('download-begin', downloadBegin);\ncrawler.on('download-progress', downloadProgress);\ncrawler.on('download-end', downloadEnd);\ncrawler.on('parse-end', function (queueItem) {\n    process.stdout.clearLine();\n    process.stdout.cursorTo(0);\n    console.log('✓ Successfully processed ' + queueItem.loc + ' [' + queueItem.count + ' urls]');\n});\ncrawler.on('end', function () {\n    if (crawler.processed) {\n        console.log('Found ' + crawler.processed + ' total urls' + (args.output ? ' outputed to ' + args.output : ''));\n    }\n});\n\nvar readOffset = 0;\nvar index = args.offset || 0;\nvar skip = index > 0;\nvar readOffset = 0;\nif (args.output) {\n    var output = fs.createWriteStream(args.output);\n    crawler.pipe(JSONStream.stringify(false)).pipe(output);\n}\nelse if (args.contentdir) {\n    setInterval(function () {\n        saveState();\n    }, 5000);\n\n    crawler.on('data', function (data) {\n        if (skip && readOffset < index) {\n            process.stdout.clearLine();\n            process.stdout.cursorTo(0);\n            process.stdout.write('Skipping lines ' + readOffset + '...');\n            readOffset++;\n        }\n        else {\n            if (skip) skip = false;\n            var loc = data[1];\n            loc = loc.substring(loc.lastIndexOf('/') + 1);\n            index++;\n            fs.writeFile(path.join(args.contentdir, loc), data[0], function (err) {\n                if (err) console.log(err);\n                data = null;\n            });\n        }\n    });\n}\n\ncrawler.crawl(args._.slice(2));\n\nfunction saveState(exitProc) {\n    if (args.contentdir) {\n        fs.writeFile(path.join(args.contentdir, '_state.crawl'), JSON.stringify({ 'args': args, 'index': index }), function (err) { \n            if (exitProc) {\n                process.exit();\n            }\n        });\n    }\n}\n\nfunction exit() {\n    saveState(true);\n}\n\nprocess.on('SIGINT', exit);\nprocess.on('SIGTERM', exit);\nprocess.on('SIGUSR1', exit);\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/bin/gloss.js":"#!/usr/bin/env node\n\nvar salient = require('./../');\n\nvar args = require('minimist')(process.argv);\n\nif (args.help || args.h || args._.length < 2) {\n    console.log(\"Usage: node gloss.js 'this is a test'\");\n}\n\nvar g = new salient.glossary.Glossary();\ng.parse(args._.slice(2)[0]);\nvar items = g.toJSON();\nfor (var i = 0; i < items.length; i++) {\n    console.log(items[i]);\n}\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/bin/graph.js":"\nvar fs = require('fs'),\n    readline = require('readline'),\n    redis = require('redis'),\n    async = require('async'),\n    clc = require('cli-color'),\n    os = require('os'),\n    cluster = require('cluster'),\n    spin = require('term-spinner');\n\nvar salient = require('./../');\n\nvar args = require('minimist')(process.argv);\n\nfunction usage() {\n    console.log(\"Usage: node graph.js --importcsv=true --redishost='localhost' --redisport=1337 --redisdb=0 --importcsv_id=3 --importcsv_title=2 --importcsv_text=-1 --importcsv_link=1 --importcsv_idprefix='doc' --importskip=1 --importlimit=0 ./products.csv\");\n    console.log(\"       node graph.js --tfidf=true --docid='LGN0833' 'NOUN:engineers'\");\n    console.log(\"       node graph.js --compare --sim=cosine --docid1='LGN0833' --docid2='LGN0832'\");\n    console.log(\"       node graph.js --compare=doc_concepts --sim=cosine --docid1='LGN0833' --docid2='LGN0832'\");\n    console.log(\"       node graph.js --compare=terms --sim=cosine --docid1='noun:bike' --docid2='noun:helmet'\");\n    console.log(\"       node graph.js --index=true --docid='LGN0833'\");\n    console.log(\"       node graph.js --search=true 'NOUN:louis'\");\n    process.exit(0);\n    return;\n};\n\nif (args.help || args.h || !(args.search || args.importcsv || args.tfidf || args.compare || args.index)) {\n    return usage();\n}\n\n// Update options from the command line\nvar options = {};\nif (args.redishost) {\n    options.redisHost = args.redishost;\n}\nif (args.redisport) {\n    options.redisPort = args.redisport;\n}\nif (args.redisdb) {\n    options.redisDb = args.redisdb;\n}\nif (args.rediscluster) {\n    options.redisCluster = args.rediscluster;\n}\nif (args.nsprefix) {\n    options.nsPrefix = args.nsprefix;\n}\n\nvar startTime = new Date().getTime();\n\nif (args.tfidf) {\n    var id = args.docid;\n    var key = \"\";\n    var finalArgs = args._.slice(2);\n    if (finalArgs.length == 0 && typeof args.tfidf == 'string') {\n        key = args.tfidf;\n    } else if (finalArgs.length > 0) {\n        key = finalArgs[0];\n    }\n\n    var documentGraph = new salient.graph.DocumentGraph(options);\n    documentGraph.TFIDF(id, key, function (err, result) {\n        console.log(result);\n        process.exit(0);\n        return;\n    });\n}\nelse if (args.index && args.docid) {\n    var documentGraph = new salient.graph.DocumentGraph(options);\n    documentGraph.indexWeights(args.docid, function (success) {\n        process.exit(0);\n        return;\n    });\n}\nelse if (args.index) {\n    var startTime = new Date().getTime();\n    var threshold = 0.08;\n    if (args.doc_threshold) {\n        threshold = args.threshold;\n    }\n\n    var spinner = spin.new(spin.types.Box1);\n    var interval = setInterval(function () {\n        spinner.next();\n    }, 1000);\n\n    var progressPrint = function (progress) {\n        process.stdout.clearLine();\n        process.stdout.cursorTo(0);\n        process.stdout.write([spinner.current, \"Indexing\", progress.count, \"of\", progress.total, progress.percent, \"%\"].join(\" \"));\n    };\n    var progressComplete = function (progress) {\n        var endTime = new Date().getTime();\n        process.stdout.clearLine();\n        process.stdout.cursorTo(0);\n        clearInterval(interval);\n        console.log(\"Indexed\", progress.count, \"in\", (endTime - startTime) / 1000, \"seconds\");\n        process.exit(0);\n        return;\n    };\n\n    var documentGraph = new salient.graph.DocumentGraph(options);\n    if (args.index == \"doc_similar\") {\n        documentGraph.indexSimilarDocuments(threshold, progressPrint, progressComplete);\n    } else {\n        documentGraph.indexAllWeights(progressPrint, progressComplete);\n    }\n}\nelse if (args.search) {\n    var searchTerms = \"\";\n    var finalArgs = args._.slice(2);\n    var limit = 10;\n    if (args.searchlimit) {\n        limit = args.searchlimit;\n    }\n    if (finalArgs.length == 0 && typeof args.search == 'string') {\n        searchTerms = args.search;\n    } else {\n        searchTerms = finalArgs[0];\n    }\n\n    var startTime = new Date().getTime();\n    var searchOptions = {};\n    searchOptions.searchLimit = limit;\n    var documentGraph = new salient.graph.DocumentGraph(options);\n    documentGraph.search(searchTerms.toLowerCase().split(' '), function (err, results) {\n        var endTime = new Date().getTime();\n        var diff = (endTime - startTime) / 1000;\n        var ids = [];\n        var scores = [];\n        var count = 0;\n        if (results && results.length > 0) {\n            ids = results.shift();\n            scores = results.shift();\n            count = results.shift();\n            ids = ids.slice(0, limit);\n        }\n\n        console.log(clc.green.bold(\"Search returned:\"), ids.length, \"of\", count, \"results in\", diff, \"seconds\");\n        if (count > 0 && args.content) {\n            documentGraph.getContents(ids, function (err, results) {\n                var iter = 0;\n                while (results && results.length) {\n                    var content = results.shift();\n                    var title = results.shift();\n                    var link = results.shift();\n                    console.log(clc.bold(\"-------------------------------------\"));\n                    if (title) {\n                        console.log(clc.xterm(75).bold(title), clc.bold(scores[ids[iter]]), clc.xterm(100).bold('(id: ' + ids[iter] + ')'));\n                    } else {\n                        console.log(clc.xterm(75).bold(ids[iter]), clc.bold(scores[ids[iter]]), clc.xterm(100).bold('(id: ' + ids[iter] + ')'));\n                    }\n                    console.log(clc.bold(\"-------------------------------------\"));\n                    if (link) {\n                        console.log(clc.xterm(75).bold(link));\n                    } else {\n                        console.log(clc.bold(ids[iter]));\n                    }\n                    console.log(content);\n                    iter++;\n                }\n                process.exit(0);\n                return;\n            });\n        } else {\n            for (var i = 0; i < ids.length; i++) {\n                console.log(ids[i], scores[ids[i]]);\n            }\n            process.exit(0);\n            return;\n        }\n    }, searchOptions);\n}\nelse if (args.compare && args.docid1 && args.docid2) {\n    var id1 = args.docid1;\n    var id2 = args.docid2;\n\n    var print = function (err, result) {\n        if (err) {\n            console.log(err);\n        } else {\n            console.log(result);\n        }\n        process.exit(0);\n        return;\n    };\n    var documentGraph = new salient.graph.DocumentGraph(options);\n    if (args.compare == \"terms\") {\n        documentGraph.CosineContextSimilarity(id1, id2, print);\n    }\n    else if (args.compare == \"doc_concepts\") {\n        documentGraph.CosineConceptSimilarity(id1, id2, print);\n    } else {\n        documentGraph.CosineSimilarity(id1, id2, print);\n    }\n}\nelse if (args.importcsv) {\n    var finalArgs = args._.slice(2);\n    var inputFile = \"\";\n    if (finalArgs.length == 0 && typeof args.importcsv == 'string') {\n        inputFile = args.importcsv;\n    } else if (finalArgs.length > 0) {\n        inputFile = finalArgs[0];\n    }\n    if (inputFile.length == 0) {\n        console.log(\"error: invalid input file specified\");\n        process.exit(0);\n        return;\n    }\n\n    var numWorkers = os.cpus().length;\n    if (typeof args.cluster == 'number') {\n        numWorkers = args.cluster;\n    } else if (!args.cluster) {\n        numWorkers = 1;\n    }\n    if (cluster.isMaster) {\n        var totalWorkers = numWorkers;\n        var totalSpeed = 0;\n        var workerState = {};\n        var totalLines = 0;\n\n        var startTime = new Date().getTime();\n        var spinner = spin.new(spin.types.Box2);\n        var updateState = function () {\n            spinner.next();\n            updateTotals();\n\n            process.stdout.clearLine();\n            process.stdout.cursorTo(0);\n            var prefix = clc.xterm(85).bold(spinner.current + \" Processed \");\n            var linePrefix = totalLines + \" lines from \";\n            var workerPrefix = clc.xterm(120).bold(totalWorkers + \" workers \");\n            var speedPrefix = \"@ \" + clc.xterm(40).bold(totalSpeed.toFixed(2) + \"/sec\");\n            process.stdout.write(prefix + linePrefix + workerPrefix + speedPrefix);\n        };\n\n        var updateTotals = function () {\n            var _totalSpeed = 0;\n            var _totalLines = 0;\n            for (var w in workerState) {\n                if (workerState[w]) {\n                    _totalLines += workerState[w].lines;\n                    _totalSpeed += workerState[w].speed;\n                }\n            }\n            totalSpeed = _totalSpeed;\n            totalLines = _totalLines;\n        };\n\n        var interval = setInterval(updateState, 1000);\n        var workers = {};\n        for (var i = 0; i < numWorkers; i++) {\n            var worker = cluster.fork();\n            workers[worker.id] = worker;\n            worker.on('message', function (msg) {\n                workerState[this.id] = msg;\n            });\n        }\n\n        cluster.on('exit', function (worker, code, signal) {\n            if (workers.hasOwnProperty(worker.id)) {\n                delete workers[worker.id];\n                numWorkers--;\n            }\n            if (numWorkers == 0) {\n                updateTotals();\n                var endTime = new Date().getTime();\n                var diff = (endTime - startTime) / 1000.0;\n                process.stdout.clearLine();\n                process.stdout.cursorTo(0);\n                var prefix = clc.xterm(85).bold(\"✓ Processed \");\n                var linePrefix = totalLines + \" lines in \";\n                var time = clc.xterm(40).bold(diff.toFixed(2) + \" seconds\\r\\n\");\n                process.stdout.write(prefix + linePrefix + time);\n                process.exit(0);\n                return;\n            }\n        });\n    } else {\n        var affinity = 0;\n        if (cluster.worker) {\n            affinity = cluster.worker.id;\n        }\n\n        var lines = 0;\n        var readLines = 0;\n        var nextLine = affinity;\n        var skipLines = args.importskip || 0;\n        var limitLines = args.importlimit || 0;\n        var maxLine = skipLines + limitLines;\n        var time = new Date().getTime();\n        var iterCount = 0;\n        var speed = 0;\n        var documentGraph = new salient.graph.DocumentGraph(options);\n        var inputStream = fs.createReadStream(inputFile);\n        var reader = readline.createInterface({ input: inputStream, terminal: false });\n        var columns = 0;\n        reader.on('line', function (line) {\n            if (lines == 0) {\n                columns = line.split(',').length;\n                lines++;\n                return;\n            }\n\n            // next line to read = lines + affinity\n            if (cluster.worker && lines != nextLine) {\n                lines++;\n                return;\n            }\n\n            nextLine = lines + (affinity + numWorkers - 1);\n            lines++;\n            readLines++;\n            if (readLines <= skipLines) {\n                return;\n            }\n            if (limitLines > 0 && maxLine < readLines) {\n                process.send({ 'speed': speed, 'lines': readLines - skipLines });\n                reader.close();\n                return;\n            }\n\n            var data = [];\n            var index = -1;\n            for (var i = 0; i < columns; i++) {\n                var prevIndex = index + 1;\n                index = line.indexOf(',', index+1);\n                data.push(line.substring(prevIndex, index));\n            }\n            data.push(line.substring(index+1, line.length));\n\n            var id = lines;\n            var text = data[data.length - 1].trim();\n            if (text.length > 0 && text[0] == '\\\"' && text[1] == '\\\"') {\n                text = text.substring(1, text.length-1).trim();\n            }\n\n            var title = \"\";\n            var link = \"\";\n            if (args.importcsv_id) {\n                if (Math.abs(args.importcsv_id) < data.length) {\n                    if (args.importcsv_id < 0) {\n                        id = data[data.length + args.importcsv_id];\n                    } else {\n                        id = data[args.importcsv_id];\n                    }\n                }\n            }\n\n            if (args.importcsv_text) {\n                if (Math.abs(args.importcsv_text) < data.length) {\n                    if (args.importcsv_text < 0) {\n                        text = data[data.length + args.importcsv_text];\n                    } else {\n                        id = data[args.importcsv_text];\n                    }\n                }\n            }\n\n            if (args.importcsv_title) {\n                if (Math.abs(args.importcsv_title) < data.length) {\n                    if (args.importcsv_title < 0) {\n                        title = data[data.length + args.importcsv_title].trim();\n                    } else {\n                        title = data[args.importcsv_title].trim();\n                    }\n                }\n            }\n\n            if (args.importcsv_link) {\n                if (Math.abs(args.importcsv_link) < data.length) {\n                    if (args.importcsv_link < 0) {\n                        link = data[data.length + args.importcsv_link];\n                    } else {\n                        link = data[args.importcsv_link];\n                    }\n                }\n            }\n\n            var categories = [];\n            if (args.importcsv_categories) {\n                var cats = args.importcsv_categories.toString().split(',');\n                for (var i = 0; i < cats.length; i++) {\n                    var cat = parseInt(cats[i]);\n                    if (cat < 0) {\n                        categories.push(data[data.length + cat]);\n                    } else {\n                        categories.push(data[cat]);\n                    }\n                }\n            }\n\n            if (text.length == 0 && title.length == 0) {\n                return;\n            }\n\n            // process the given document text according to the given id/text\n            if (args.importcsv_idprefix) {\n                id = args.importcsv_idprefix + id;\n            }\n            documentGraph.readDocument(id, text, title, link, categories);\n\n            var newTime = new Date().getTime();\n            iterCount++;\n            if ((newTime - time) > 1000) {\n                speed = Math.round(100 * (iterCount / ((newTime - time) / 1000))) / 100;\n                iterCount = 0;\n                time = newTime;\n                process.send({ 'speed': speed, 'lines': readLines - 1 - skipLines });\n            }\n        });\n\n        reader.on('close', function () {\n            process.send({ 'speed': speed, 'lines': readLines - 1 - skipLines });\n            process.exit(0);\n            return;\n        });\n    }\n}\nelse {\n    return usage();\n}\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/bin/hmmloader.js":"\nvar fs = require('fs');\nvar path = require('path');\nvar salient = require('./../')\nvar model = new salient.language.HiddenMarkovModel();\n\nvar args = process.argv;\nif (!args || args.length == 2) {\n    console.log('usage: node hmmloader.js model.json model.tagdist.json language.hmm.json');\n    return;\n}\n\nvar modelVocabFile = 'model.json';\nvar modelTagDistFile = 'model.tagdist.json';\nif (args && args.length > 2) {\n    modelVocabFile = args[2];\n    if (args.length > 3)\n        modelTagDistFile = args[3];\n    if (args.length > 4)\n        outputFile = args[4];\n}\n\nmodel.restore(path.join(__dirname, modelVocabFile));\nmodel.restore(path.join(__dirname, modelTagDistFile));\n\nvar json = JSON.stringify(model);\nfs.writeFileSync(outputFile || 'output.hmm.json', json);\n\nmodule.exports = model;\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/bin/hmmtags.js":"\nvar path = require('path'),\n    util = require('util'),\n    fs = require('fs');\n\nvar salient = require('./../');\n\nvar args = process.argv;\nif (!args || args.length < 3) {\n    console.log('usage: node hmmtags en-brown.tag.dist [outputdir]');\n    return;\n}\n\nvar model = new salient.language.HiddenMarkovModel();\nvar tagDistFile = args[2];\nvar outputDir = __dirname;\nif (args.length == 4) {\n    outputDir = args[3];\n    if (outputDir.indexOf('./') == 0) {\n        outputDir = path.join(__dirname, outputDir);\n    } \n}\n\n// initialize a vocabulary with the size 12 tagsets\nmodel.loadTagDist(tagDistFile, function () {\n    model.estimateTagDistribution();\n    var s = JSON.stringify(model);\n    fs.writeFileSync(path.join(outputDir, 'model.tagdist.json'), s);\n});\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/bin/hmmtest.js":"\nvar path = require('path'),\n    util = require('util'),\n    fs = require('fs');\n\nvar salient = require('./../');\n\nvar args = process.argv;\nif (!args || args.length < 3) {\n    console.log('usage: node hmmtest en-brown.sentences [en.hmm.json] --lines=10000 --skip=10000');\n    return;\n}\n\nvar sentencesFile = args[2];\nvar modelFile = path.join(__dirname, 'en.hmm.json');\nvar lineLimit = 0;\nvar skip = 0;\nif (args.length > 3) {\n    for (var i = 3; i < args.length; i++) {\n        if (args[i].indexOf('--lines=') == 0) {\n            lineLimit = parseInt(args[i].split('=')[1]);\n        }\n        else if (args[i].indexOf('--skip=') == 0) {\n            skip = parseInt(args[i].split('=')[1]);\n        }\n        else {\n            modelFile = path.join(__dirname, args[3]);\n        }\n    }\n}\n\nvar model = new salient.language.HiddenMarkovModel();\nmodel.restore(modelFile);\n\nvar lines = fs.readFileSync(sentencesFile).toString().split('\\n');\nvar incorrectSentences = 0;\nvar totalIncorrectTags = 0;\nvar totalSentences = 0;\nvar totalTags = 0;\nvar startTime = new Date().getTime();\nvar limit = lineLimit > 0 ? skip + lineLimit : lines.length;\nvar incorrectSentenceGroups = {};\nvar incorrectTokensToTags = {};\nfor (var l = skip; l < limit; l++) {\n    var line = lines[l].trim();\n    if (line.length == 0)\n        continue;\n\n    var tokenPairs = line.split(' ');\n    var tokens = [];\n    var expectedTags = [];\n    for (var i = 0; i < tokenPairs.length; i++) {\n        if (tokenPairs[i].trim().length == 0)\n            continue;\n\n        var splitIndex = tokenPairs[i].trim().lastIndexOf('/');\n        var token = tokenPairs[i].trim().substring(0, splitIndex);\n        var tag = tokenPairs[i].trim().substring(splitIndex + 1);\n        tokens.push(token);\n        expectedTags.push(tag);\n    }\n\n    var results = model.viterbi(tokens);\n    var incorrectTags = 0;\n    var incorrectHigh = false;\n    for (var i = 0; i < expectedTags.length; i++) {\n        if (results.y.length > i) {\n            /*\n            if (results.y[i] == \"PRON\" && expectedTags[i] == \"NOUN\") {\n            }\n            else if (results.y[i] == \"NOUN\" && expectedTags[i] == \"PRON\") {\n            }\n            else if (results.y[i] == \"NUM\" && expectedTags[i] == \"NOUN\") {\n            }\n            else if (results.y[i] == \"NUM\" && expectedTags[i] == \"ADJ\") {\n            }\n            else if (results.y[i] == 'X' && expectedTags[i] == '.') {\n            }\n            else if (results.y[i] == '.' && expectedTags[i] == 'X') {\n            }\n            */\n            if (results.y[i] != expectedTags[i]) {\n                var tag = expectedTags[i] + \"/\" + results.y[i];\n                if (!incorrectTokensToTags.hasOwnProperty(tokens[i])) {\n                    incorrectTokensToTags[tokens[i]] = {};\n                }\n                if (!incorrectTokensToTags[tokens[i]].hasOwnProperty(tag)) {\n                    incorrectTokensToTags[tokens[i]][tag] = 0;\n                }\n                incorrectTokensToTags[tokens[i]][tag]++;\n                incorrectTags++;\n            }\n        }\n        else {\n            incorrectTags++;\n        }\n    }\n\n    if (incorrectTags > 0) {\n        incorrectSentences++;\n        totalIncorrectTags += incorrectTags;\n\n        var newLine = [];\n        for (var k = 0; k < results.y.length; k++) {\n            newLine.push(tokens[k] + \"/\" + results.y[k]);\n        }\n        /*\n        console.log('\\t', line);\n        console.log('\\t', newLine.join(' '));\n        */\n\n        if (!incorrectSentenceGroups.hasOwnProperty(incorrectTags.toString())) {\n            incorrectSentenceGroups[incorrectTags.toString()] = 0;\n        }\n        incorrectSentenceGroups[incorrectTags.toString()]++;\n    }\n\n    totalTags += expectedTags.length;\n    totalSentences++;\n}\nvar endTime = new Date().getTime();\n\nvar totalIncorrectTokensSort = [];\nfor (var t in incorrectTokensToTags) {\n    var item = incorrectTokensToTags[t];\n    var total = 0;\n    for (var tag in item) {\n        total += item[tag];\n    }\n    totalIncorrectTokensSort.push({t: t, tags: item, total: total});\n}\ntotalIncorrectTokensSort = totalIncorrectTokensSort.sort(function (a, b) { return a.total - b.total; });\n\n//console.log(totalIncorrectTokensSort);\nvar percentCorrect = 100.0 * ((totalSentences - incorrectSentences) / totalSentences);\nvar percentTagsCorrect = 100.0 * ((totalTags - totalIncorrectTags) / totalTags);\nconsole.log('Finished processing:', totalSentences, 'sentences in', (endTime - startTime) / 1000, 'seconds. rate/second:', totalSentences / ((endTime - startTime) / 1000));\nconsole.log('% sentences correct:', percentCorrect, ', total sentences:', (totalSentences - incorrectSentences), '/', totalSentences);\nconsole.log('% tags correct:', percentTagsCorrect, ', total tags:', (totalTags - totalIncorrectTags), '/', totalTags);\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/bin/hmmtrain.js":"\nvar path = require('path'),\n    util = require('util'),\n    fs = require('fs');\n\nvar salient = require('./../');\n\nvar args = process.argv;\nif (!args || args.length < 5) {\n    console.log('usage: node hmmtrain model.training.json model.validation.json model.test.json');\n    return;\n}\n\nvar trainModel = new salient.language.HiddenMarkovModel();\nvar validationModel = new salient.language.HiddenMarkovModel();\nvar testModel = new salient.language.HiddenMarkovModel();\nvar trainDataFile = args[2];\nvar validationDataFile = args[3];\nvar testDataFile = args[4];\n\ntrainModel.restore(path.join(__dirname, trainDataFile));\nvalidationModel.restore(path.join(__dirname, validationDataFile));\ntestModel.restore(path.join(__dirname, testDataFile));\n\nvar lambda = [];\nfor (var i = 1; i < 100; i++) {\n    lambda.push(i * 0.01);\n}\n\nvar lambdaV = [];\nfor (var i = 0; i < lambda.length; i++) {\n    for (var i2 = 0; i2 < lambda.length; i2++) {\n        for (var i3 = 0; i3 < lambda.length; i3++) {\n            var l1 = lambda[i];\n            var l2 = lambda[i2];\n            var l3 = lambda[i3];\n            if (l1 + l2 + l3 == 1.0) {\n                lambdaV.push([l1, l2, l3]);\n            }\n        }\n    }\n}\n\n// begin cross validation\nvar scores = [];\nvar maxScore;\nvar maxIndex = 0;\nvar percentComplete = 0;\nfor (var i = 0; i < lambdaV.length; i++) {\n    trainModel.lambdaV = lambdaV[i];\n    trainModel.estimateTagDistribution();\n    var score = trainModel.crossValidate(validationModel);\n    scores.push({ s: score, lambda: trainModel.lambdaV });\n    if (!maxScore || score > maxScore) {\n        maxScore = score;\n        maxIndex = i;\n    }\n    process.stdout.clearLine();\n    process.stdout.cursorTo(0);\n    process.stdout.write('Validation ' + (i + 1) + '/' + lambdaV.length + '. Best score: ' + maxScore + '.');\n}\n\nvar best = scores[maxIndex];\nconsole.log('\\nBest cross validation error:', best, 'out of', lambdaV.length, 'configurations');\n\n// begin test\ntrainModel.lambdaV = best.lambda;\ntrainModel.estimateTagDistribution();\nvar bestScore = trainModel.crossValidate(testModel);\nconsole.log('Test error score:', bestScore);\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/bin/iula.js":"\nvar path = require('path');\nvar salient = require('./../');\nvar IULATreeBank = salient.corpus.IULATreeBank;\n\nvar args = process.argv;\nif (!args || args.length == 2) {\n    console.log('usage: node iula ~/Public/IULA_Spanish_LSP_Treebank/IULA_Spanish_LSP_Treebank.conll');\n    return;\n}\n\nvar file = args[2];\nvar outputDir = __dirname;\n\nvar corp = new IULATreeBank(file);\ncorp.output = path.join(outputDir, 'es-iula.tag.vocab');\ncorp.outputDist = path.join(outputDir, 'es-iula.tag.dist');\ncorp.outputSentences = path.join(outputDir, 'es-iula.sentences');\ncorp.parse();\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/bin/mergevocabulary.js":"\nvar path = require('path'),\n    util = require('util'),\n    fs = require('fs');\n\nvar salient = require('./../');\n\nvar args = process.argv;\nif (!args || args.length < 4) {\n    console.log('usage: node mergevocabulary en.wik.vocab covered.corpus.vocab');\n    return;\n}\n\nvar wikVocabFile = args[2];\nvar corpusVocabFile = args[3];\n\nfunction readDict(file) {\n    var vocab = {};\n    var lines = fs.readFileSync(file).toString().split('\\n');\n    for (var l = 0; l < lines.length; l++) {\n        var line = lines[l];\n        var items = line.split('\\t');\n        if (!items || items.length < 2)\n            continue;\n\n        // ID TOKEN POS,POS POSFREQ,POSFREQ\n        var w = items[1].toLowerCase();\n        if (items.length == 4) {\n            vocab[w] = { pos: items[2], posFreq: items[3], id: items[0], w: w };\n        }\n        else if (items.length == 3) {\n            vocab[w] = { pos: items[2], id: items[0], w: w };\n        }\n    }\n\n    vocab._length = lines.length;\n    return vocab;\n}\n\nvar dict = readDict(wikVocabFile);\nvar corpusV = readDict(corpusVocabFile);\nvar newOutput = path.join(__dirname, 'corpus.vocab');\n\n// Determine whether the corpus is a subset, supset, overlap or disjoint set of the vocabulary\nvar newCorpusVocab = [];\nfor (var c in corpusV) {\n    if (typeof dict[c] != 'undefined') {\n        newCorpusVocab.push(corpusV[c]);\n    }\n}\n\n// Determine whether the vocabulary is a subset, supset, overlap or disjoint set of the corpus\nfor (var c in dict) {\n    if (typeof corpusV[c] == 'undefined') {\n        newCorpusVocab.push(dict[c]);\n    }\n}\n\nfor (var i = 0; i < newCorpusVocab.length; i++) {\n    var item = newCorpusVocab[i];\n    var additionLine = \"\";\n    if (item.posFreq) {\n        additionLine = util.format('%s\\t%s\\t%s\\t%s\\n', i, item.w, item.pos, item.posFreq);\n    }\n    else {\n        additionLine = util.format('%s\\t%s\\t%s\\n', i, item.w, item.pos);\n    }\n    fs.appendFileSync(newOutput, additionLine);\n}\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/bin/ngrammodel.js":"\nvar path = require('path'),\n    util = require('util'),\n    fs = require('fs');\n\nvar salient = require('./../');\n\nvar args = process.argv;\nif (!args || args.length < 3) {\n    console.log('usage: node ngrammodel en.wik.vocab');\n    return;\n}\n\nvar model = new salient.language.HiddenMarkovModel();\nvar vocabFile = args[2];\n\n// initialize a vocabulary with the size 12 tagsets\nmodel.loadVocab(vocabFile, 12, function () {\n    model.estimateVocabulary();\n    var s = JSON.stringify(model);\n    fs.writeFileSync(path.join(__dirname, 'model.json'), s);\n});\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/bin/parsewik-headings.js":"var salient = require('./../');\nvar path = require('path');\n\nvar args = process.argv;\nif (!args || args.length == 2) {\n    console.log('usage: node parsewik.js ~/enwiktionary-20140102-pages-articles.xml [outputdir]');\n    return;\n}\n\nvar file = args[2];\nvar outputDir = __dirname;\nif (args.length > 3) {\n    if (args[3] == \"~\") {\n        outputDir = process.env['HOME'];\n    }\n}\n\n// execute the parser with the given file\nvar parser = new salient.wiktionary.WiktionaryParser(file);\nparser.printHeadings = true;\nparser.output = path.join(outputDir, parser.lang + '.wik.vocab');\nparser.outputDist = path.join(outputDir, parser.lang + '.wik.dist');\nparser.parse();\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/bin/parsewik.js":"var salient = require('./../');\nvar path = require('path');\n\nvar args = process.argv;\nif (!args || args.length == 2) {\n    console.log('usage: node parsewik.js ~/enwiktionary-20140102-pages-articles.xml [outputdir]');\n    return;\n}\n\nvar file = args[2];\nvar outputDir = __dirname;\nif (args.length > 3) {\n    if (args[3] == \"~\") {\n        outputDir = process.env['HOME'];\n    }\n}\n\n// execute the parser with the given file\nvar parser = new salient.wiktionary.WiktionaryParser(file);\nparser.output = path.join(outputDir, parser.lang + '.wik.vocab');\nparser.outputDist = path.join(outputDir, parser.lang + '.wik.dist');\nparser.parse();\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/bin/ptb.js":"\nvar path = require('path');\nvar salient = require('./../');\nvar PennTreeBank = salient.corpus.PennTreeBank;\n\nvar args = process.argv;\nif (!args || args.length == 2) {\n    console.log('usage: node ptb ~/Public/ptb/pos');\n    return;\n}\n\nvar file = args[2];\nvar outputDir = __dirname;\n\nvar corp = new PennTreeBank(file);\ncorp.output = path.join(outputDir, 'en-ptb.tag.vocab');\ncorp.outputDist = path.join(outputDir, 'en-ptb.tag.dist');\ncorp.outputSentences = path.join(outputDir, 'en-ptb.sentences');\ncorp.parse();\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/bin/sentiment.js":"\nvar fs = require('fs'),\n    path = require('path');\n\nvar args = process.argv;\nif (!args || args.length < 3) {\n    console.log('usage: node sentiment senticorp.log --lines=100');\n    return;\n}\n\nvar salient = require('./..');\nvar analyser = new salient.sentiment.BayesSentimentAnalyser();\nvar sentimentFile = args[2];\nvar correctNegative = 0.0;\nvar correctPositive = 0.0;\nvar correctNeutral = 0.0;\nvar incorrect = 0.0;\nvar totalCorrect = 0.0;\nvar lines = fs.readFileSync(sentimentFile).toString().split('\\n');\nvar count = lines.length;\nif (args.length == 4) {\n    if (args[3].indexOf('--lines') > -1) {\n        count = parseInt(args[3].split('--lines=')[1]);\n    }\n}\n\nvar startTime = new Date().getTime();\nvar tokens = 0;\nvar skipped = 0;\nfor (var i = 0; i < count; i++) {\n    if (!lines[i] || lines[i].trim().length == 0) {\n        skipped++;\n        continue;\n    }\n\n    var items = lines[i].split('\\t');\n    var score = parseInt(items[0]);\n    var text = items[1];\n\n    var result = analyser.classify(text);\n    tokens += analyser.glossary.terms;\n    if (result < -0.1 && score < 0) {\n        correctNegative++;\n    }\n    else if (result > -0.1 && result < 0.1 && score == 0) {\n        correctNeutral++;\n    }\n    else if (result > 0.1 && score > 0) {\n        correctPositive++;\n    }\n    else {\n        console.log(score, result, text);\n        incorrect++;\n    }\n}\nvar endTime = new Date().getTime();\nvar seconds = (endTime - startTime) / 1000;\ntotalCorrect = correctPositive + correctNegative + correctNeutral;\nvar totalProcessed = count - skipped;\n\nconsole.log('processed', totalProcessed, 'in', seconds, 'seconds', totalProcessed / seconds, '/second');\nconsole.log('processed', tokens, 'tokens in', seconds, 'seconds', tokens / seconds, '/ second');\nconsole.log('correct:', totalCorrect, 'sentences vs incorrect:', incorrect, 'sentences');\nconsole.log('percent correct', (100.0 * totalCorrect / totalProcessed ), 'percent incorrect', (100.0 * incorrect / totalProcessed));\nconsole.log('total tokenization time', analyser.glossary.tokenTime, 'ms', analyser.glossary.tokenTime / totalProcessed, 'ms/iteration');\nconsole.log('total pos time', analyser.glossary.tagTime, 'ms', (analyser.glossary.tagTime) / totalProcessed, 'ms/iteration');\nconsole.log('total lemma time', analyser.glossary.lemmaTime, 'ms',  analyser.glossary.lemmaTime / totalProcessed, 'ms/iteration');\nconsole.log('total collapse time', analyser.glossary.collapseTime, 'ms',  analyser.glossary.collapseTime / totalProcessed, 'ms/iteration');\nconsole.log('total tag sentiment time', analyser.tagTime, 'ms', analyser.tagTime / totalProcessed, 'ms/iteration');\nconsole.log('total classify time', analyser.classifyTime, 'ms', analyser.classifyTime / totalProcessed, 'ms/iteration');\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/bin/sentistrength.js":"\nvar util = require('util'),\n    fs = require('fs'),\n    path = require('path'),\n    spawn = require('child_process').spawn;\n\nvar args = process.argv;\nif (!args || args.length < 3) {\n    console.log('usage: node sentistrength senticorp.log --lines=100');\n    return;\n}\n\nvar sentimentFile = args[2];\nvar lines = fs.readFileSync(sentimentFile).toString().split('\\n');\nvar count = lines.length;\nif (args.length == 4) {\n    if (args[3].indexOf('--lines=') > -1) {\n        count = parseInt(args[3].split('--lines=')[1]);\n    }\n}\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/bin/treetaggertest.js":"\nvar fs = require('fs'),\n    path = require('path'),\n    util = require('util'),\n    async = require('async');\nvar salient = require('./../');\n\nvar args = process.argv;\nif (!args || args.length < 3) {\n    console.log('usage: node treetaggertest en-brown.sentences');\n    return;\n}\n\nvar sentencesFile = args[2];\nvar tagger = new salient.tagging.TreeTagger();\n\nvar lines = fs.readFileSync(sentencesFile).toString().split('\\n');\nvar incorrectSentences = 0;\nvar totalIncorrectTags = 0;\nvar totalSentences = 0;\nvar totalTags = 0;\nvar startTime = new Date().getTime();\n\nasync.eachSeries(lines, function (line, callback) {\n    var tokenPairs = line.split(' ');\n    var tokens = [];\n    var expectedTags = [];\n    for (var i = 0; i < tokenPairs.length; i++) {\n        var tokenPair = tokenPairs[i].split('/');\n        tokens.push(tokenPair[0]);\n        expectedTags.push(tokenPair[1]);\n    }\n\n    tagger.tag(tokens, function (err, results) {\n        var incorrectTags = 0;\n        for (var i = 0; i < expectedTags.length; i++) {\n            if (results.length > i) {\n                if (results[i] != expectedTags[i]) {\n                    incorrectTags++;\n                }\n            }\n            else {\n                incorrectTags++;\n            }\n        }\n\n        if (incorrectTags > 0) {\n            incorrectSentences++;\n            totalIncorrectTags += incorrectTags;\n\n            var newLine = [];\n            for (var k = 0; k < results.length; k++) {\n                newLine.push(tokens[k] + \"/\" + results[k]);\n            }\n            /*\n            console.log('\\n\\t', line);\n            console.log('\\t', newLine.join(' '));\n            */\n        }\n\n        totalTags += expectedTags.length;\n        totalSentences++;\n        callback();\n    });\n}, function (err) {\n    var endTime = new Date().getTime();\n\n    var percentCorrect = 100.0 * ((totalSentences - incorrectSentences) / totalSentences);\n    var percentTagsCorrect = 100.0 * ((totalTags - totalIncorrectTags) / totalTags);\n    console.log('Finished processing:', totalSentences, 'sentences in', (endTime - startTime) / 1000, 'seconds. rate/second:', totalSentences / ((endTime - startTime) / 1000));\n    console.log('% sentences correct:', percentCorrect, ', total sentences:', (totalSentences - incorrectSentences), '/', totalSentences);\n    console.log('% tags correct:', percentTagsCorrect, ', total tags:', (totalTags - totalIncorrectTags), '/', totalTags);\n});\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/bin/tweetbank.js":"\nvar path = require('path');\nvar salient = require('./../');\nvar TwitterTreeBank = salient.corpus.TwitterTreeBank;\n\nvar args = process.argv;\nif (!args || args.length == 2) {\n    console.log('usage: node ptb ~/Public/twitter');\n    return;\n}\n\nvar file = args[2];\nvar outputDir = __dirname;\n\nvar corp = new TwitterTreeBank(file);\ncorp.output = path.join(outputDir, 'en-tweet.tag.vocab');\ncorp.outputDist = path.join(outputDir, 'en-tweet.tag.dist');\ncorp.outputSentences = path.join(outputDir, 'en-tweet.sentences');\ncorp.parse();\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/object.js":"\nexports.extend = function (defaults, options) {\n    var runner = {};\n    for (var attr in defaults) { runner[attr] = defaults[attr]; }\n    for (var attr in options) { runner[attr] = options[attr]; }\n    return runner;\n};\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/scripts/import/sentiwordnet.js":"var fs = require('fs'),\n    readline = require('readline'),\n    mongo = require('mongojs'),\n    argv = require('optimist').argv;\n\nvar database = argv.database || \"wordnet\";\nvar host = argv.h || \"localhost\";\nvar username = argv.u || \"\";\nvar password = argv.p || \"\";\nvar collections = [\"sentiword\",\"synsets\"];\n\nvar connection = host + \"/\" + database;\nif (username && password) {\n    connection = username + \":\" + password + \"@\" + connection;\n}\n\n// Setup the mongo connection db\nvar db = mongo(connection, collections);\n\nvar rd = readline.createInterface({\n    input: fs.createReadStream('./sentiwordnet-3.0.txt'),\n    ouput: process.stdout,\n    terminal: false\n});\n\nfunction Word(pos, id, posScore, negScore, synsets, gloss) {\n    this._id = pos + id;\n    this.pos = pos;\n    this.posScore = posScore;\n    this.negScore = negScore;\n    this.sysnsets = synsets;\n    this.gloss = gloss;\n};\n\nfunction Synset(synset, pos, posScore, negScore) {\n    this._id = synset + '#' + pos;\n    this.word = synset;\n    this.posScore = posScore;\n    this.negScore = negScore;\n    this.score = posScore + negScore;\n}\n\nvar words = [];\nfunction init() {\n    rd.on('line', function (line) {\n        if (line.indexOf('#') != 0) {\n            var items = line.split('\\t');\n            var pos = items[0];\n            var id = items[1];\n            var posScore = parseFloat(items[2]);\n            var negScore = parseFloat(items[3]) * -1;\n            var synsets = items[4].split(' ');\n            var gloss = items[5];\n            var word = new Word(pos, id, posScore, negScore, synsets, gloss);\n            db.sentiword.save(word);\n            for (var s = 0; s < synsets.length; s++) {\n                var synset = synsets[s];\n                var synsplit = synset.split('#');\n                var text = synsplit[0];\n                db.synsets.update({\"_id\":text + \"#\" + pos}, {\"$inc\":{\"posScore\":posScore, \"negScore\":negScore, \"score\":posScore + negScore}}, {upsert: true});\n            }\n        }\n    });\n    rd.on('close', function () {\n        process.exit(0);\n    });\n};\n\ninit();\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/crawlers/ratelimiter.js":"\n\n/**\n* Add rate limiting capabilities to a queue.\n* The timeout should indicate the minimum number of milliseconds before\n* invoking the next item in the queue.\n* So for a 100/min. rate limit, use (60000 / 100)\n*\n* Invoke like\n*      rateLimiter.call(queue);\n*      queue.rateLimit(timeout);\n*/\nvar rateLimiter = function () {\n\tvar queue = this;\n\n\t// keep the original methods, so we can invoke them later on\n\tvar _push = queue.push;\n\tvar _drain = queue.drain;\n\n\t// copy the original tasks\n\tvar internalQueue = queue.tasks.splice(0);\n\tqueue.tasks = [];\n\n\t// number of items that need to finish\n\tvar toProcess = internalQueue.length;\n\n\t// internal state\n\tthis.$rateLimitTimeout = 0;\n\tthis.$rateLimitBusy = false;\n\tthis.$rateLimitInterval = null;\n\n\t// wrapper around push that pushes to our own queue instead of tasks\n\tthis.push = function (data, callback) {\n\t\tif(data.constructor !== Array) {\n\t\t\tdata = [data];\n\t\t}\n\t\tdata.forEach(function (task) {\n\t\t\tinternalQueue.push({ data: task, callback: callback });\n\t\t});\n\n\t\ttoProcess += data.length;\n\t};\n\n\t// kick off the rate limiter\n\tthis.rateLimit = function (timeout) {\n\t\tthis.$rateLimitTimeout = timeout || 1000;\n\t\tthis.$processNextRateLimited();\n\t};\n\n\t// invoke the rate limit implementation with a timeout\n\tthis.$processNextRateLimited = function () {\n\t\tvar self = this;\n\n\t\tself.$rateLimitInterval = setInterval(function () {\n\t\t\tself.$processNextRateLimitedImpl();\n\t\t}, self.$rateLimitTimeout);\n\t};\n\n\t// process the next item in our rate limited queue\n\tthis.$processNextRateLimitedImpl = function () {\n\t\tvar self = this;\n\n\t\t// grab next item...\n\t\t// disadvantage of this approach (doing it in the impl) is that the event loop will die\n\t\t// 1 timeout later than required. Might need to fix that one day.\n\t\tvar item = internalQueue.shift(0);\n\t\tif (!item) {\n\t\t\treturn;\n\t\t}\n\n\t\t// we're busy\n\t\tself.$rateLimitBusy = true;\n\n\t\t// the 'drain' function can be overwritten at this point,\n\t\t// as we don't have control over when it happens...\n\t\tif (self.drain !== $emptyDrain) {\n\t\t\t_drain = self.drain;\n\t\t\tself.drain = $emptyDrain;\n\t\t}\n\n\t\t// invoke original push function of the queue\n\t\t_push(item.data, function () {\n\t\t\tif (typeof item.callback === \"function\") {\n\t\t\t\titem.callback.apply(this, arguments);\n\t\t\t}\n\n\t\t\ttoProcess -= 1;\n\n\t\t\tif (toProcess === 0) {\n\t\t\t\tself.$rateLimitBusy = false;\n\n\t\t\t\tclearInterval(self.$rateLimitInterval);\n\n\t\t\t\tif (typeof _drain === \"function\") {\n\t\t\t\t\t_drain();\n\t\t\t\t}\n\t\t\t}\n\t\t});\n\t};\n\n\tvar $emptyDrain = function () {};\n\n\t// the original drain function of a queue needs to be overwritten\n\t// otherwise the user will be flooded with messages\n\tthis.drain = $emptyDrain;\n};\n\nmodule.exports = rateLimiter;\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/crawlers/sitemap_crawler.js":"\nvar events = require('events'),\n    request\t= require('request'),\n    async = require('async'),\n    util = require('util'),\n    progress = require('request-progress'),\n    Stream = require('stream');\n\nvar XmlStream = require('./../wiktionary/xml-stream'),\n    StringReader = require('./string_reader');\n\nfunction SitemapCrawler(options) {\n    Stream.call(this);\n    var self = this;\n\n    this.options = options || {};\n    this._sitemapQueue = async.queue(function (q, cb) { self._process(q, cb); }, this.options.concurrency || 1);\n    this._sitemapQueue.drain = function () { self.emit('end'); };\n    this.readable = true;\n    this.processed = 0;\n};\n\nutil.inherits(SitemapCrawler, Stream);\n\nSitemapCrawler.prototype._process = function (queueItem, callback) {\n    var self = this;\n    self.emit('download-begin', queueItem);\n    progress(request.get(queueItem.loc, function (err, response, body) {\n        self.emit('download-end', queueItem);\n        if (err) {\n            return callback(err);\n        } else {\n            var reader = new StringReader(body);\n            var xml = new XmlStream(reader);\n            queueItem.count = 0;\n\n            xml.on('updateElement: sitemap', function (item) {\n                self._sitemapQueue.push(item);\n                queueItem.count++;\n                self.emit('queue-sitemap', item);\n            });\n            xml.on('updateElement: url', function (item) {\n                var urlItem = { loc: item.loc,\n                                lastmod: item.lastmod,\n                                changefreq: item.changefreq,\n                                priority: item.priority };\n                queueItem.count++;\n                self.emit('data', urlItem);\n                item = null;\n            });\n            xml.on('end', function () {\n                self.processed += queueItem.count;\n                self.emit('parse-end', queueItem);\n                return callback();\n            });\n            reader.open();\n        }\n    }), { throttle: 100, delay: 1000 })\n    .on('progress', function (state) {\n        self.emit(\"download-progress\", queueItem, state);\n    });\n};\n\nSitemapCrawler.prototype.crawl = function (sitemaps) {\n    this._sitemapQueue.pause();\n\n    this.processed = 0;\n    for (var i = 0; i < sitemaps.length; i++) {\n        this._sitemapQueue.push({loc: sitemaps[i]});\n    }\n\n    // Resume processing\n    this._sitemapQueue.resume();\n};\n\nmodule.exports = SitemapCrawler;\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/crawlers/string_reader.js":"var util = require('util');\nvar Stream = require('stream');\nvar Buffer = require('buffer').Buffer;\n\n/**\n * A Readable stream for a string or Buffer.\n *\n * This works for both strings and Buffers.\n */\nfunction StringReader(str) {\n    this.data = str;\n}\nutil.inherits(StringReader, Stream);\nmodule.exports = StringReader;\n\nStringReader.prototype.open =\nStringReader.prototype.resume = function () {\n    if (this.encoding && Buffer.isBuffer(this.data)) {\n      this.emit('data', this.data.toString(this.encoding));\n    }\n    else {\n        this.emit('data', this.data);\n    }\n    this.emit('end');\n    this.emit('close');\n}\n\nStringReader.prototype.setEncoding = function (encoding) {\n    this.encoding = encoding;\n}\n\n\nStringReader.prototype.pause = function () {\n}\n\nStringReader.prototype.destroy = function () {\n  delete this.data;\n}\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/graph/document.js":"\nvar redis = require(\"redis\"),\n    RedisStream = require('redis-stream'),\n    RedisCluster = require('node-redis-cluster').RedisCluster;\n\nvar Glossary = require(\"./../glossary/glossary\"),\n    object = require(\"./../object\"),\n    async = require('async'),\n    _ = require('underscore')._;\n\n// Default options for the document graph\nvar defaultOptions = {\n    redisPort: 6379,\n    redisHost: \"0.0.0.0\",\n    redisDb: 0,\n    nsPrefix: \"\",\n    separator: \":\",\n    searchLimit: 100\n};\n\nvar TERMS = \"terms\";\nvar CONTENT = \"content\";\nvar TITLE = \"title\";\nvar LINK = \"href\";\nvar CATEGORY = \"category\";\n\nvar TOTAL = \"+\";\nvar WEIGHT = \"w\";\n\nvar TERM_CONTEXT = \"tctx\";\nvar TERM_DOC = \"td\";\nvar DOC_TERM = \"d\";\nvar DOC_SIM = \"dsim\";\nvar SEARCH = \"search\";\n\nvar CATEGORY_TERM = \"cgt\";\nvar CATEGORY_DOC = \"cgd\";\nvar TERM_CATEGORY = \"tcg\";\nvar DOC_CATEGORY = \"dcg\";\n\n// DocumentGraph represents a client for processing documents \n// in order to process document text in order to build a graph of \n// connections between document ids, text, parts of speech context \n// and the language as it is presented through the data.\nfunction DocumentGraph(options) {\n    this.options = object.extend(defaultOptions, options || {});\n    this._gloss = new Glossary();\n    \n    // Parse redis cluster configuration in the form of:\n    // \"host:port,host:port,host:port\"\n    if (this.options.redisCluster) {\n        var addresses = this.options.redisCluster.split(',');\n        var config = [];\n        for (var i = 0; i < addresses.length; i++) {\n            var items = addresses[i].split(':');\n            var host = \"0.0.0.0\";\n            var port = 6379;\n            if (items.length == 1) {\n                port = parseInt(items[0]);\n            } else {\n                if (items[0].length > 0) {\n                    host = items[0];\n                }\n                port = parseInt(items[1]);\n            }\n            config.push({ port: port, host: host });\n        }\n        this._redisCluster = RedisCluster.create(config, { return_buffers: true });\n    } else {\n        this._redisClient = redis.createClient(this.options.redisPort, this.options.redisHost, { return_buffers: true });\n        this._redisClient.select(this.options.redisDb);\n        this._redisStream = RedisStream(this.options.redisPort, this.options.redisHost, this.options.redisDb);\n    }\n\n    this._k_Total = this._fmt(TOTAL);\n    this._k_TotalTerms = this._fmt(TOTAL, TERMS);\n};\n\nDocumentGraph.prototype._fmt = function() {\n    var args = [];\n    if (this.options.nsPrefix.length > 0) {\n        args.push(this.options.nsPrefix);\n    }\n    for (var i in arguments) {\n        if (arguments[i] && arguments[i].tag) {\n            args.push(arguments[i].tag.toLowerCase());\n            args.push(arguments[i].distinct || arguments[i].term.toLowerCase());\n        } else if (arguments[i] && arguments[i].length && arguments[i].length > 0) {\n            args.push(arguments[i]);\n        }\n    }\n    return args.join(this.options.separator);\n};\n\nDocumentGraph.prototype._fmtNode = function (node) {\n    var args = [];\n    if (node.tag) {\n        args.push(node.tag.toLowerCase());\n        args.push(node.distinct || node.term.toLowerCase());\n    }\n    return args.join(this.options.separator);\n};\n\nDocumentGraph.prototype._client = function (key) {\n    return this._multi || this._redisClient || this._redisCluster.getRedis(key);\n};\n\n// _incrEdges will increment the total number on the given key, followed by \n// adding/updating the edge weight between the id <-> key. If there is a previous \n// key node present, then we will update/add the edge between the previous key \n// and the newly provided present key, followed by returning the new key.\nDocumentGraph.prototype._incrEdges = function (id, key, prevKey, weight, categories) {\n    // increment total frequency on the key\n    var k_TotalKey = this._fmt(TOTAL, key);\n    var k_TotalTerms = this._k_TotalTerms;\n\n    if (!this._redisCluster && !this._multi) {\n        this._multi = this._redisClient.multi();\n    }\n\n    this._client(k_TotalKey).incrby(k_TotalKey, weight);\n    this._client(k_TotalTerms).incrby(k_TotalTerms, weight);\n\n    // increment/add an edge: id -> key\n    // increment/add an edge: id <- key\n    var k_DocTermId = this._fmt(DOC_TERM, id);\n    var k_TermDocKey = this._fmt(TERM_DOC, key);\n    this._client(k_DocTermId).zincrby(k_DocTermId, weight, key);\n    this._client(k_TermDocKey).zincrby(k_TermDocKey, weight, id);\n\n    // increment/add an edge for all categories -> key\n    // increment/add an edge for all categories <- key\n    // increment/add an edge for all categories -> id\n    // increment/add an edge for all categories <- id\n    if (categories) {\n        for (var c in categories) {\n            if (c && c.length > 0) {\n                var k_CategoryTermC = this._fmt(CATEGORY_TERM, c);\n                var k_TermCategoryKey = this._fmt(TERM_CATEGORY, key);\n                var k_DocCategoryId = this._fmt(DOC_CATEGORY, id);\n                var k_CategoryDocC = this._fmt(CATEGORY_DOC, c);\n\n                this._client(k_CategoryTermC).zincrby(k_CategoryTermC, categories[c], key);\n                this._client(k_TermCategoryKey).zincrby(k_TermCategoryKey, categories[c], c);\n                this._client(k_DocCategoryId).zincrby(k_DocCategoryId, categories[c], c);\n                this._client(k_CategoryDocC).zincrby(k_CategoryDocC, categories[c], id);\n            }\n        }\n    }\n\n    // increment/add edges for co-occurance between terms\n    // increment/add an edge for context: context:prevKey -> key\n    // increment/add an edge for current: context:prevKey <- key\n    if (prevKey) {\n        var k_TermContextPrevKey = this._fmt(TERM_CONTEXT, prevKey);\n        var k_TermContextKey = this._fmt(TERM_CONTEXT, key);\n        this._client(k_TermContextPrevKey).zincrby(k_TermContextPrevKey, weight, key);\n        this._client(k_TermContextKey).zincrby(k_TermContextKey, weight, prevKey);\n    }\n\n    return key;\n};\n\n// Search will look up the given terms in the graph to find the \n// most relevant documents associated with the given terms using \n// the tfidf scores as a measurement.\nDocumentGraph.prototype.search = function (terms, callback, options) {\n    var self = this;\n    var options = options || {};\n    options.searchLimit = options.searchLimit || this.options.searchLimit;\n    async.map(terms, function (term, cb) {\n        if (term.indexOf(':') < 0) {\n            var prefix = self._fmt(TERM_DOC, \"*\");\n            var k = prefix + self.options.separator + term;\n            if (self._redisCluster) {\n                self._redisCluster.execAll('keys', [k], function (err, clusterResults) {\n                    var reducedTerms = [];\n                    for (var c in clusterResults) {\n                        var keys = clusterResults[c];\n                        for (var i = 0; i < keys.length; i++) {\n                            reducedTerms.push(keys[i].slice(prefix.length - 1, keys[i].length));\n                        }\n                    }\n\n                    cb(null, reducedTerms);\n                });\n            } else {\n                self._client(k).keys(k, function (err, keys) {\n                    var reducedTerms = [];\n                    for (var i = 0; i < keys.length; i++) {\n                        reducedTerms.push(keys[i].slice(prefix.length - 1, keys[i].length));\n                    }\n\n                    cb(null, reducedTerms);\n                });\n            }\n        } else {\n            cb(null, term);\n        }\n    }, function (err, termResults) {\n        terms = _.flatten(termResults);\n        var outKey = self._fmt('search', terms.join('_'));\n        var args = [outKey, terms.length]; \n        for (var i = 0; i < terms.length; i++) {\n            args.push(self._fmt(WEIGHT, terms[i]));\n        }\n        args.push('AGGREGATE');\n        args.push('SUM');\n        var selectResults = function (primaryClient) {\n            primaryClient.zcard(outKey, function (err, cardResult) {\n                if (err) {\n                    callback(err);\n                } else {\n                    primaryClient.zrevrange(outKey, 0, options.searchLimit, 'WITHSCORES', function (err, results) {\n                        if (err) {\n                            callback(err);\n                            return;\n                        }\n\n                        var scores = {};\n                        var ids = [];\n                        while (results.length) {\n                            var docId = results.shift();\n                            var docTermWeight = parseFloat(results.shift(), 10);\n                            if (!scores.hasOwnProperty(docId)) {\n                                scores[docId] = 0;\n                                ids.push(docId);\n                            }\n                            scores[docId] += docTermWeight;\n                        }\n                        ids = ids.sort(function (a, b) { return scores[b] - scores[a] });\n                        callback(null, [ids,scores,cardResult]);\n                    });\n                }\n            });\n        };\n\n        var primaryClient = self._client(args[2]);\n        primaryClient.exists(outKey, function (err, existsResult) {\n            if (err) {\n                callback(err);\n            } else if (existsResult == 1) {\n                selectResults(primaryClient);\n            } else {\n                // handle clustered zunions through the use of copy and migration\n                if (self._redisCluster) {\n                    var clients = [];\n                    for (var i = 2; i < args.length - 2; i++) {\n                        var c = self._client(args[i]);\n                        clients.push([c, args[i], i]);\n                    }\n\n                    // determine the best migration path\n                    async.map(clients, function (item, cb) {\n                        item[0].zcard(item[1], function (err, reply) { \n                            if (reply == 0 || err) {\n                                cb(err, null);\n                            } else {\n                                cb(err, [item[1], reply]); \n                            }\n                        });\n                    }, function (err, results) {\n                        if (err) {\n                            callback(err);\n                        } else {\n                            results = results.filter(function (a) { \n                                if (a == null) {\n                                    return false;\n                                } else {\n                                    return true;\n                                }\n                            });\n\n                            if (results.length == 0) {\n                                callback(false);\n                                return;\n                            }\n\n                            var delKeys = [];\n                            async.every(results, function (item, cb) {\n                                var arg = item[0];\n                                var c = self._client(arg);\n                                if (c != primaryClient) {\n                                    // perform the dump and restore\n                                    c.dump(arg, function (err, reply) {\n                                        if (err) {\n                                            cb(false);\n                                        }\n                                        else if (reply) {\n                                            primaryClient.restore(arg, 0, reply, function (err, result) {\n                                                if (err) {\n                                                    cb(false);\n                                                } else {\n                                                    delKeys.push(arg);\n                                                    cb(true);\n                                                }\n                                            });\n                                        } else {\n                                            cb(true);\n                                        }\n                                    });\n                                } else {\n                                    cb(true);\n                                }\n                            }, function (success) {\n                                if (success) {\n                                    primaryClient.zunionstore(args, function (err, results) {\n                                        if (err) {\n                                            callback(err);\n                                        } else {\n                                            for (var i = 0; i < delKeys.length; i++) {\n                                                primaryClient.del(delKeys[i]);\n                                            }\n                                            selectResults(primaryClient);\n                                        }\n                                    });\n                                } else {\n                                    callback(success);\n                                }\n                            });\n                        }\n                    });\n                }\n                else {\n                    var client = self._client(args[0]);\n                    client.zunionstore(args, function (err, result) {\n                        if (err) {\n                            callback(err);\n                        } else {\n                            selectResults(client);\n                        }\n                    });\n                }\n            }\n        });\n    });\n};\n\nDocumentGraph.prototype.getContents = function (ids, callback) {\n    var formattedIds = [];\n    for (var i = 0; i < ids.length; i++) {\n        formattedIds.push(this._fmt(CONTENT, ids[i]));\n        formattedIds.push(this._fmt(TITLE, ids[i]));\n        formattedIds.push(this._fmt(LINK, ids[i]));\n    }\n\n    if (this._redisCluster) {\n        this._redisCluster.execMany('get', formattedIds, function (err, results) {\n            if (err) {\n                callback(err);\n            } else {\n                var r = [];\n                while (formattedIds.length) {\n                    r.push(results[formattedIds.shift()].toString());\n                    r.push(results[formattedIds.shift()].toString());\n                    r.push(results[formattedIds.shift()].toString());\n                }\n                callback(null, r);\n            }\n        });\n    } else {\n        this._redisClient.mget(formattedIds, callback);\n    }\n};\n\n// readDocument will parse the text, tokenize, and tag it to build a \n// directed and undirected graph of the given document id to various \n// directed nodes that form the structure of the document text. Edge \n// weights are determined by a simple frequency scalar.\nDocumentGraph.prototype.readDocument = function (id, text, title, link, categories, next) {\n    // increment the total number of documents found\n    this._client(this._k_Total).incrby(this._k_Total, 1);\n\n    // set the content of this document in redis so we can get it later\n    var k_ContentId = this._fmt(CONTENT, id);\n    this._client(k_ContentId).set(k_ContentId, text);\n\n    // set the title and link if they exist\n    if (title) {\n        var k_TitleId = this._fmt(TITLE, id);\n        this._client(k_TitleId).set(k_TitleId, title);\n    }\n    if (link) {\n        var k_LinkId = this._fmt(LINK, id);\n        this._client(k_LinkId).set(k_LinkId, link);\n    }\n\n    // parse the text (tokenize, tag..etc)\n    var parseLoop = function (obj, contents, weight, categoryWeights) {\n        obj._gloss.parse(contents);\n        var current = obj._gloss.root;\n        var prev = null;\n        do {\n            var s = current.toJSON();\n            if (!s.children && !s.orig && !s.isFiltered) {\n                // increment total and add/incr bidirectional edges for current node\n                prev = obj._incrEdges(id, obj._fmtNode(s), prev, weight, categoryWeights);\n            } else if (!s.isFiltered) {\n                if (s.orig && s.children) {\n                    // increment total and add bidirectional edge weight for original node\n                    if (!s.orig.isFiltered) {\n                        prev = obj._incrEdges(id, obj._fmtNode(s.orig), prev, weight, categoryWeights);\n                    }\n\n                    for (var i = 0; i < s.children.length; i++) {\n                        // increment total and add/incr bidirectional edges on child\n                        if (!s.children[i].isFiltered) {\n                            prev = obj._incrEdges(id, obj._fmtNode(s.children[i]), prev, weight, categoryWeights);\n                        }\n                    }\n                }\n            }\n\n            current = current.next;\n        } while (current);\n    };\n\n    var categoryWeights = {};\n    if (categories) {\n        var prev = null;\n        for (var i = 0; i < categories.length; i++) {\n            var category = categories[i];\n            if (category && category.length > 0) {\n                category = category.toLowerCase();\n                category = category.replace(/'s/g, \"s\").replace(/\\s+/g, \"_\");\n                categoryWeights[category] = 1;\n            }\n        }\n    }\n    if (title) {\n        parseLoop(this, title, title && text ? 10 : 2, categoryWeights);\n    }\n    if (text) {\n        parseLoop(this, text, 1, categoryWeights);\n    }\n\n    if (this._multi) {\n        this._multi.exec();\n        delete this._multi;\n    }\n};\n\nDocumentGraph.prototype._computeTermWeights = function (ids, term, callback) {\n    var self = this;\n    async.every(ids, function (id, cb) {\n        self._computeWeights(id, [term], cb);\n    }, callback);\n};\n\n// _computeWeights will ensure that all provided terms have their tfidf computed \n// with respect to the given document identifier before returning to callback.\nDocumentGraph.prototype._computeWeights = function (id, terms, callback) {\n    // compute the tfidf for all terms in this document\n    var self = this;\n    var k_WeightId = self._fmt(WEIGHT, id);\n    var rClient = self._client(k_WeightId);\n    async.every(terms, function (term, cb) {\n        self.TFIDF(id, term, function (err, result) {\n            if (err) {\n                cb(false);\n            } else {\n                var k_WeightTerm = self._fmt(WEIGHT, term);\n                var r2Client = self._client(k_WeightTerm);\n                rClient.zadd(k_WeightId, result.tfidf, term, function (err, success) {\n                    r2Client.zadd(k_WeightTerm, result.tfidf, id, function (err, success) {\n                        cb(success);\n                    });\n                });\n            }\n        });\n    }, callback);\n};\n\n// Ensures that all documents in the database are indexed for tfidf weights\nDocumentGraph.prototype.indexAllWeights = function (progress, callback) {\n    var self = this;\n    var k_DocTermStar = this._fmt(DOC_TERM, \"*\");\n    var handleResults = function (err, keys) {\n        var iter = 0;\n        async.eachLimit(keys, 8, function (item, cb) {\n            var id = item.slice((self._fmt(DOC_TERM) + \":\").length, item.length);\n            self.indexWeights(id, function (success) {\n                iter++;\n                progress({ total: keys.length, count: iter, percent: Math.round((iter / keys.length) * 100.0) });\n                cb(null);\n            });\n        }, function () {\n            callback({total: keys.length, count: iter});\n        });\n    }\n\n    if (this._redisCluster) {\n        this._redisCluster.execAll('keys', [k_DocTermStar], function (err, clusterResults) {\n            var keys = [];\n            if (clusterResults) {\n                for (var c in clusterResults) {\n                    var ckeys = clusterResults[c];\n                    for (var i = 0; i < ckeys.length; i++) {\n                        keys.push(ckeys[i]);\n                    }\n                }\n            }\n\n            handleResults(err, keys);\n        });\n    } else {\n        this._client(k_DocTermStar).keys(k_DocTermStar, handleResults);\n    }\n};\n\nDocumentGraph.prototype.indexSimilarDocuments = function (threshold, progress, callback) {\n    var self = this;\n    var k_DocTermStar = this._fmt(DOC_TERM, \"*\");\n    var handleResults = function (err, keys) {\n        var lengthSlice = self._fmt(DOC_TERM).length + 1;\n        var iter = 0;\n        var total = keys.length * keys.length;\n        async.eachLimit(keys, 8, function (idi, cbi) {\n            var docid1 = idi.slice(lengthSlice, idi.length);\n            var docidResults = [self._fmt(DOC_SIM, docid1)];\n            async.eachLimit(keys, 8, function (idk, cbk) {\n                iter++;\n                var docid2 = idk.slice(lengthSlice, idk.length);\n                if (docid1 == docid2) {\n                    cbk();\n                } else {\n                    self.CosineSimilarity(docid1, docid2, function (err, distance) {\n                        progress({ total: total, count: iter, percent: Math.round((iter / total) * 100.0) });\n                        if (distance > threshold) {\n                            self._redisClient.zadd(self._fmt(DOC_SIM, docid1), distance, docid2, function () {\n                                cbk();\n                            });\n                        } else {\n                            cbk();\n                        }\n                    });\n                }\n            }, function (err) {\n                cbi();\n            });\n        }, function (err) {\n            callback({ total: total, count: iter });\n        });\n    };\n\n    if (this._redisCluster) {\n        this._redisCluster.execAll('keys', [k_DocTermStar], function (err, clusterResults) {\n            var keys = [];\n            if (clusterResults) {\n                for (var c in clusterResults) {\n                    var ckeys = clusterResults[c];\n                    for (var i = 0; i < ckeys.length; i++) {\n                        keys.push(ckeys[i]);\n                    }\n                }\n            }\n\n            handleResults(err, keys);\n        });\n    } else {\n        this._client(k_DocTermStar).keys(k_DocTermStar, handleResults);\n    }\n};\n\n// Ensures that all terms in the given document id are indexed and computed for tfidf\nDocumentGraph.prototype.indexWeights = function (id, callback) {\n    var self = this;\n    var k_DocTermId = this._fmt(DOC_TERM, id);\n    this._client(k_DocTermId).zrevrange(k_DocTermId, 0, -1, function (err, keys) {\n        if (!keys) {\n            callback(false);\n            return;\n        }\n\n        self._computeWeights(id, keys, callback);\n    });\n};\n\nDocumentGraph.prototype._isIdFiltered = function (filterPrefixes, id) {\n    var isFiltered = false;\n    if (filterPrefixes && filterPrefixes.length > 0) {\n        isFiltered = true;\n        for (var i = 0; i < filterPrefixes.length; i++) {\n            var prefix = filterPrefixes[i];\n            if (id.indexOf(prefix) >= 0) {\n                isFiltered = false;\n                break;\n            }\n        }\n    }\n    return isFiltered;\n};\n\nDocumentGraph.prototype._cosineFilter = function (prefix, filterPrefixes, id1, id2, callback) {\n    var self = this;\n    var id1_scores = {};\n    var id2_scores = {};\n    var ids = [];\n    var k_PrefixId1 = self._fmt(prefix, id1);\n    var k_PrefixId2 = self._fmt(prefix, id2);\n    self._client(k_PrefixId1).zrevrange(k_PrefixId1, 0, -1, 'WITHSCORES', function (err, id1_results) {\n        if (err) {\n            callback(err, null);\n            return;\n        }\n\n        self._client(k_PrefixId2).zrevrange(k_PrefixId2, 0, -1, 'WITHSCORES', function (err, id2_results) {\n            if (err) {\n                callback(err, null);\n                return;\n            }\n\n            var id1_sum = 0;\n            while (id1_results.length) {\n                var id = id1_results.shift().toString();\n                var score = parseInt(id1_results.shift());\n                if (prefix == TERM_CONTEXT && self._totalTerms) {\n                    score = (score / self._totalTerms);\n                }\n                if (!self._isIdFiltered(filterPrefixes, id)) {\n                    id1_scores[id] = score;\n                    if (ids.indexOf(id) < 0) {\n                        ids.push(id);\n                    }\n\n                    id1_sum += (score * score);\n                }\n            }\n\n            var id2_sum = 0;\n            while (id2_results.length) {\n                var id = id2_results.shift().toString();\n                var score = parseInt(id2_results.shift());\n                if (prefix == TERM_CONTEXT && self._totalTerms) {\n                    score = (score / self._totalTerms);\n                }\n                if (!self._isIdFiltered(filterPrefixes, id)) {\n                    id2_scores[id] = score;\n                    if (ids.indexOf(id) < 0) {\n                        ids.push(id);\n                    }\n\n                    id2_sum += (score * score);\n                }\n            }\n\n            var dotsum = 0;\n            for (var i = 0; i < ids.length; i++) {\n                var id = ids[i];\n                if (id1_scores.hasOwnProperty(id) && id2_scores.hasOwnProperty(id)) {\n                    dotsum += (id1_scores[id] * id2_scores[id]);\n                }\n            }\n\n            var magnitude = (Math.sqrt(id1_sum) * Math.sqrt(id2_sum));\n            var sim = dotsum / magnitude;\n            callback(null, sim);\n            return;\n        });\n    });\n};\n\n// Measures the cosine similarity between two different documents by \n// looking up all the features (concepts only) of a given document id, \n// and for each edge, we will compute them as weighted features.\nDocumentGraph.prototype.CosineConceptSimilarity = function (id1, id2, callback) {\n    this._cosineFilter(WEIGHT, ['noun','adj','adv'], id1, id2, callback);\n};\n\n// Measures the cosine similarity between two different documents by \n// looking up all the features that represent a given document id, \n// and for each edge, we will compute them as weighted features.\nDocumentGraph.prototype.CosineSimilarity = function (id1, id2, callback) {\n    this._cosineFilter(WEIGHT, null, id1, id2, callback);\n};\n\nDocumentGraph.prototype.CosineContextSimilarity = function (term1, term2, callback) {\n    if (!this._totalTerms) {\n        var self = this;\n        this._client(this._k_TotalTerms).get(this._k_TotalTerms, function (err, result) {\n            if (err) {\n                callback(err, null);\n                return;\n            }\n            self._totalTerms = result;\n            self._cosineFilter(TERM_CONTEXT, null, term1, term2, callback);\n        });\n    } else {\n        this._cosineFilter(TERM_CONTEXT, null, term1, term2, callback);\n    }\n};\n\n// TFIDF will return the term frequency - inverse-document frequency of the \n// given arguments. Should the document id be passed in as well, then we will get \n// the tf-idf relative to the given document. Otherwise, we will return the aggregate\n// on the given term itself.\nDocumentGraph.prototype.TFIDF = function (id, key, callback) {\n    // Ensure that we can overload for ('text here', function (err, result)...)\n    if (!callback && typeof text == 'function') {\n        callback = text;\n        text = id;\n        id = undefined;\n    }\n\n    // Get the total frequency of the given term\n    var k_TotalKey = this._fmt(TOTAL, key);\n\n    // Executes and handles results returned from the multi redis command below\n    var execResults = function(err, results) {\n        if (err) {\n            callback(err);\n            return;\n        }\n        \n        var keyFrequency = parseInt(results[0]);\n        var docFrequency = parseInt(results[1]);\n        var keyDocFrequency = results[2];\n\n        // ensure that key frequency is relative to the given document (when appropriate)\n        if (id && results.length == 4) {\n            keyFrequency = parseInt(results[3]);\n        }\n        var tf = 1 + (Math.log(keyFrequency) / Math.LN10);\n        var idf = Math.log(docFrequency / keyDocFrequency) / Math.LN10;\n\n        var result = {};\n        result.key = key;\n        result.rawtf = keyFrequency;\n        result.df = keyDocFrequency;\n        result.n = docFrequency;\n        result.idf = idf;\n        result.tf = tf;\n        result.tfidf = tf * idf;\n        callback(null, result);\n    };\n\n    if (this._redisCluster) {\n        var keys = [this._k_Total, k_TotalKey, this._fmt(TERM_DOC, key)];\n        if (id) {\n            keys.push(this._fmt(DOC_TERM, id));\n        }\n\n        // map the above keys to the various redis functions according to a clustered set of clients\n        var maps = [];\n        for (var i = 0; i < keys.length; i++) {\n            var k = this._client(keys[i]);\n            if (i == 0 || i == 1) {\n                maps[i] = [k, 'get', [keys[i]]];\n            } else if (i == 2) {\n                maps[i] = [k, 'zcard', [keys[i]]];\n            } else {\n                maps[i] = [k, 'zscore', [keys[i], key]];\n            }\n        }\n\n        // Perform the operation as a map-invoke operation set\n        async.map(maps, function (item, clusterCallback) {\n            item[0].send_command(item[1], item[2], clusterCallback);\n        }, execResults);\n    }\n    else if (id) {\n        this._redisClient.multi()\n            .get(this._k_Total).get(k_TotalKey).zcard(this._fmt(TERM_DOC, key)).zscore(this._fmt(DOC_TERM, id), key)\n            .exec(execResults);\n    } else {\n        this._redisClient.multi()\n            .get(this._k_Total).get(k_TotalKey).zcard(this._fmt(TERM_DOC, key))\n            .exec(execResults);\n    }\n};\n\nmodule.exports = DocumentGraph;\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/language/gaddag.js":"/*!\n * gaddag.js\n * Copyright(c) 2012 hillerstorm <progr@mmer.nu>\n */\nvar fs = require('fs');\n\nfunction str (node) {\n    var s, label;\n\n    if (node.$) {\n        s = '1';\n    } else {\n        s = '0';\n    }\n\n    for (label in node.e) {\n        if (node.e.hasOwnProperty(label)) {\n            s += '_' + label + '_' + node.e[label].id;\n        }\n    }\n\n    return s;\n}\n\nmodule.exports = function () {\n    var nextId = 0,\n        previousWord = '';\n    this.root = { id: nextId++, e: {}, $: 0 };\n    uncheckedNodes = [],\n    minimizedNodes = {};\n\n    function minimize (downTo) {\n        var i, tuple,\n            childKey, node;\n\n        for (i = uncheckedNodes.length-1; i > downTo-1; i--) {\n            tuple = uncheckedNodes.pop();\n            childKey = str(tuple.child);\n            node = minimizedNodes[childKey];\n            if (node) {\n                tuple.parent.e[tuple.letter] = node;\n            } else {\n                minimizedNodes[childKey] = tuple.child;\n            }\n        }\n    }\n\n    this.insert = function (word) {\n        var commonPrefix = 0, i,\n            node, slicedWord, nd, ltr;\n\n        for (i = 0; i < Math.min(word.length, previousWord.length); i++) {\n            if (word[i] !== previousWord[i]) {\n                break;\n            }\n            commonPrefix += 1;\n        }\n\n        minimize(commonPrefix);\n        if (uncheckedNodes.length === 0) {\n            node = this.root;\n        } else {\n            node = uncheckedNodes[uncheckedNodes.length-1].child;\n        }\n\n        slicedWord = word.slice(commonPrefix);\n        for (i = 0; i < slicedWord.length; i++) {\n            nd = { id: nextId++, e: {}, $: 0 };\n            ltr = slicedWord[i].toUpperCase();\n            node.e[ltr] = nd;\n            uncheckedNodes.push({\n                parent: node,\n                letter: ltr,\n                child: nd\n            });\n            node = nd;\n        }\n        node.$ = 1;\n        previousWord = word;\n    }\n\n    this.finish = function () {\n        minimize(0);\n        delete uncheckedNodes;\n        delete minimizedNodes;\n        delete previousWord;\n        this.minify(this.root);\n    }\n\n    this.minify = function (node) {\n        delete node.id;\n        if (!node.$) {\n            delete node.$;\n        }\n\n        for (var edge in node.e) {\n            this.minify(node.e[edge]);\n        }\n    };\n\n    // Finds a word, returns 1 if the word was found, 0 otherwise\n    this.find = function (word) {\n        var node = this.root,\n            i, letter;\n\n        for (i = 0; i < word.length; i++) {\n            letter = word[i].toUpperCase();\n            if (!node.e[letter]) {\n                return 0;\n            }\n            node = node.e[letter];\n        }\n        return node.$;\n    };\n\n    // Returns the subtree matching the letter given\n    this.get = function (letter) {\n        return this.root.e[letter.toUpperCase()];\n    };\n};\n","/home/travis/build/npmtest/node-npmtest-salient/node_modules/salient/lib/salient/lda/stream_lda.js":"\nvar events = require('events'),\n    Stream = require('stream');\n\nfunction StreamLDA(k, parseText) {\n    Stream.call(this);\n\n    var self = this;\n    this.readable = true;\n    this.writable = true;\n    this._k = k || 10;\n    this._alpha = 1.0 / this.k;\n    this._eta = 1.0 / this.k;\n    this._tau0 = 1.0;\n    this._kappa = 0.7;\n    this._batchSize = 50;\n    this._batchesSeen = 0;\n    this._docsSeen = 0;\n    this._q = async.queue(function (a, cb) { self._process(q, cb); }, this.options.concurrency || 1);\n    this._q.drain = function () { self.end(); };\n    this._q.resume();\n    this._parseText = parseText;\n}\n\nutil.inherits(StreamLDA, Stream);\n\nStreamLDA.prototype._process = function (queueItem, callback) {\n    if (this.parseText) {\n    }\n};\n\nStreamLDA.prototype.write = function (data) {\n    this._q.push(data);\n};\n\nStreamLDA.prototype.end = function () {\n    this.emit('end');\n};\n\nmodule.exports = StreamLDA;\n"}